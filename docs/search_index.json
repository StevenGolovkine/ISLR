[
["index.html", "An Introduction to Statistical Learning Chapter 1 Introduction", " An Introduction to Statistical Learning Steven Golovkine 2020-04-11 Chapter 1 Introduction This book aims to provide my results to the different exercises of An Introduction to Statistical Learning, with Application in R, by James, Witten, Hastie and Tibshirani (James et al. 2013). The applied exercises will be solved using the packages from the tidyverse (https://www.tidyverse.org) when it is possible. This book is compiled using R Markdown and bookdown (https://github.com/rstudio/bookdown). References "],
["overview.html", "Chapter 2 An Overview of Statistical Learning 2.1 Conceptual Exercises 2.2 Applied Exercises", " Chapter 2 An Overview of Statistical Learning 2.1 Conceptual Exercises 2.1.1 Exercise 1. This exercise is about flexible and inflexible statistical learning methods. First, let’s recall what are the differences between these methods. The aim of statistical learning is to estimate a function \\(f\\) such that \\(f\\) is a link between the input \\(X\\) and the output \\(Y\\). A flexible model means that we do not assume a particular form for \\(f\\). So, an advantage of such models is to generally provide a better fit to the data (however, be careful with the overfitting) but the number of parameters to estimate is usually large. At the contrary, an inflexible model has less parameters but we have to prespecified a particular form for the data (for example linear), even if it poorly fits the data. Question (a) The case of sample size \\(n\\) extremely large and number of predictors \\(p\\) small is the ideal case in statistical learning. A flexible method should perform very well in this case. The flexible method will tend to reduce the bias and won’t be too sensitive to the noise thanks to the large size of the sample. Question (b) The case of sample size \\(n\\) small and number of predictors \\(p\\) very large refers to the high dimensional settings. An inflexible method should show better performance than a flexible one in this case. Here, the trade-off between bias and variance is very important. We allow some bias by using an inflexible model with the hope to reduce a lot the noise in the data. Question (c) In the case of highly non-linear relationship between predictors and reponse, a flexible model will perform better than an inflexible one. In case of inflexible model, we set a particular form for \\(f\\) and we usually can not specified a function for \\(f\\) if \\(f\\) is highly non-linear. Question (d) If the variance of the error terms is extremely high, an inflexible model will perform better than a flexible one. Because, if we set a flexible method, the function \\(f\\) will tend to follow the error and thus will overfit. 2.1.2 Exercise 2. This exercise is about the difference between regression and classification problems and the inference or prediction purposes. Let’s recall what these different terms mean. A regression task is done when we try to infer or predict an output which takes continuous values. A classification task is done when we try to infer or predict an output which takes discrete values. An inference purpose consists in the understanding of how the features have an influence on the response, whereas a prediction purpose is to find a value of the output variable based on a new realisation of the dependant variables and the knowledge of some features and outputs. Question (a) This is a regression problem because the CEO salary is a continuous variable. We aim to do inference here (understanding which factors). \\(n\\) is equal to 500 (top 500 firms in the US), \\(p\\) equals to 3 (profit, number of employees and industry) and the output is the CEO salary. Question (b) This is a classification problem because the output varible is discrete (success or failure). We aim to do prediction (launching a new product and wish to know whether it will be a success or a failure). \\(n\\) is equal to 20, \\(p\\) equals to 13 (price charged for the product, marketing budget, competition price, and ten other variables) and the output variable is success or failure. Question (c) This is a regression problem because the % change in the US dollar is continuous. We aim to do prediction (We are interesting in predicting). \\(n\\) is equal to 52 (number of week in 2012), \\(p\\) equals to 3 (the % change in the US market, the % change in the British market, and the % change in the German market) and the output variable is the % change in the dollar. 2.1.3 Exercise 3. This exercise is about the bias-variance decomposition of the mean square error. Consider the following model: \\(y = f(x) + \\epsilon\\). We denote by \\(\\widehat{y}\\) and \\(\\widehat{f}\\) the estimation of \\(y\\) and \\(f\\). The mean square error is: \\[MSE = \\mathbb{E}\\left[(\\widehat{y} - y)^2\\right].\\] As a more complex model leads to a better estimation of the function \\(f\\), the training error decreases as the model complexity increases. The MSE can be decomposed into three terms: variance, squared bias and irreducible error. “Variance refers to the amount by which \\(\\widehat{f}\\) would change if we estimated it using a different training data set.” So, the variance increases with the model complexity because a complex model is very flexible and gives a different function for each training data set. Conversely, the bias decreases with the model complexity because such a model will fit perfectly the data. The irreducible error is equal to \\(Var(\\epsilon)\\). The test error has a U-shape because it is the sum of the three previous curves. Figure 2.1: Model complexity. 2.1.4 Exercise 4. This exercise is about giving examples of real-life applications of statistical learning. From Armando Arroyo GeekStyle on Linkedin. 2.1.5 Exercise 5. This exercise is about the difference between flexible and non-flexible statistical learning methods. A very flexible method has for main advantage over a less flexible methods the large number of functional forms that it can take. It shows two majors drawbacks: the first one is the number of parameters to fit (usually, way more larger than the non-flexible methods) and the second one, its propension to overfit the data. Moreover, they can exhibit less interpretability. We can prefer a less flexible approach when we want to do inference of the dataset because of the interpretability of such models. However, when the goal is prediction, we may use very flexible methods in order to (hopefully) have better results. The choice of between a very flexible and a less flexible method refers closely with the bias-variance tradeoff. In general, a very flexible one will lead to small bias, large variance and a less flexible one to large bias, small variance. 2.1.6 Exercise 6. This exercise is about the difference between parametric and non-parametric approaches. Consider the following model: \\(Y = f(X) + \\epsilon\\). We aim to estimate the function \\(f\\). For the parametric approaches, we assume a particular form for \\(f\\), linear for example, and then, estimate some parameters. However, if the form we assume is not the right one, our estimate won’t be very accurate. At the opposite, non-parametric approaches do not assume a particular form for \\(f\\). So, the estimate of \\(f\\) will be close to the true functional form of \\(f\\). But, we need a lot of data (compare to parametric approches) to obtain an accurate estimate of \\(f\\). 2.1.7 Exercise 7. This exercise is an application of \\(K\\)-nearest neighbors. Data Obs \\(X_1\\) \\(X_2\\) \\(X_3\\) \\(Y\\) 1 0 3 0 Red 2 2 0 0 Red 3 0 1 3 Red 4 0 1 2 Green 5 -1 0 1 Green 6 1 1 1 Red Question (a) The euclidean distance between to two \\(n\\)-dimensional vectors \\(X\\) and \\(Y\\) is defined by \\[ d(X, Y) = \\sqrt{\\sum_{i = 1}^n (X_i - Y_i)^2}\\] Obs 1 2 3 4 5 6 \\(d(0/i)\\) 3 2 \\(\\sqrt{10}\\) \\(\\sqrt{5}\\) \\(\\sqrt{2}\\) \\(\\sqrt{3}\\) Question (b) For \\(K = 1\\), we classify the test point where the closest observation is. The closest point is the point 5, so the test point will be Green. Question (c) For \\(K = 3\\), we classify the test point where the three closest observation are. The three closest points are the 2, 5 and 6. Two points are red and one is green, so the test point will be Red. Question (d) If the Bayes decision boundary in this problem is highly non-linear, we would expect the best value for K to be small because the smaller \\(K\\) is, the more flexible the model is. So, if the model is very flexible, it will adapt to highly non-linear problem. 2.2 Applied Exercises 2.2.1 Exercise 8. This exercise is about the College dataset. It contains 777 observations of 18 variables about the universities and colleges in the United States. For a description of the variables, please refer to the page 54 of the book or in R by typing help(College) after loading the package ISLR. Question (a) and (b) College &lt;- as_tibble(College, rownames = NA) Question (c) i College %&gt;% summary_df() %&gt;% print_summary_df() Factor variables Private Private Count No 212 Yes 565 Numeric variables Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Apps 0 711 48013.0 3001.64 14978459.53 81.0 329.8 457.6 776.0 1558.0 3624.0 7675.0 11066.2 48094.0 Accept 0 693 26258.0 2018.80 6007959.70 72.0 272.4 361.6 604.0 1110.0 2424.0 4814.2 6979.2 26330.0 Enroll 0 581 6357.0 779.97 863368.39 35.0 118.6 154.0 242.0 434.0 902.0 1903.6 2757.0 6392.0 Top10perc 0 82 95.0 27.56 311.18 1.0 7.0 10.0 15.0 23.0 35.0 50.4 65.2 96.0 Top25perc 0 89 91.0 55.80 392.23 9.0 25.8 30.6 41.0 54.0 69.0 85.0 93.0 100.0 F.Undergrad 0 714 31504.0 3699.91 23526579.33 139.0 509.8 641.0 992.0 1707.0 4005.0 10024.4 14477.8 31643.0 P.Undergrad 0 566 21835.0 855.30 2317798.85 1.0 20.0 35.0 95.0 353.0 967.0 2016.6 3303.6 21836.0 Outstate 0 640 19360.0 10440.67 16184661.63 2340.0 4601.6 5568.8 7320.0 9990.0 12925.0 16552.8 18498.0 21700.0 Room.Board 0 553 6344.0 4357.53 1202743.03 1780.0 2735.8 3051.2 3597.0 4200.0 5050.0 5950.0 6382.0 8124.0 Books 0 122 2244.0 549.38 27259.78 96.0 350.0 400.0 470.0 500.0 600.0 700.0 765.6 2340.0 Personal 0 294 6550.0 1340.64 458425.75 250.0 500.0 600.0 850.0 1200.0 1700.0 2200.0 2488.8 6800.0 PhD 0 78 95.0 72.66 266.61 8.0 43.8 50.6 62.0 75.0 85.0 92.0 95.0 103.0 Terminal 0 65 76.0 79.70 216.75 24.0 52.8 59.0 71.0 82.0 92.0 96.0 98.0 100.0 S.F.Ratio 0 173 37.3 14.09 15.67 2.5 8.3 9.9 11.5 13.6 16.5 19.2 21.0 39.8 perc.alumni 0 61 64.0 22.74 153.56 0.0 6.0 8.0 13.0 21.0 31.0 40.0 46.0 64.0 Expend 0 744 53047.0 9660.17 27266865.64 3186.0 4795.8 5558.2 6751.0 8377.0 10830.0 14841.0 17974.8 56233.0 Grad.Rate 0 81 108.0 65.46 295.07 10.0 37.0 44.6 53.0 65.0 78.0 89.0 94.2 118.0 Question (c) ii Figure 2.2: Pair plots. Question (c) iii Figure 2.3: Boxplots of the variable Outstate by Private. Question (c) iv College &lt;- College %&gt;% mutate(Elite = factor(Top10perc &gt; 50)) College %&gt;% select(Elite) %&gt;% summary_df() %&gt;% print_summary_df() Factor variables Elite Elite Count FALSE 699 TRUE 78 Figure 2.4: Boxplots of the variable Outstate vs Elite. Question (c) v Figure 2.5: Histograms of the variable Apps for different binwidth. Question (c) vi As a brief summary, we found that there is a huge correlation between the number of full-time undergraduates and the number of applications received, accepted and students enrolled. The price to be enrolled in a private university is in mean twice as the price for a public one. But the variance of the price for the private colleges is very important. Moreover, the maximum value of the price for public universities is almost equal to the mean of the private ones (except outliers). Finally, the elite universities (the ones with new students from top 10% of high school class) are usually more expensive than the other ones. 2.2.2 Exercise 9. This exercise is about the Auto dataset. It contains 392 observations of 9 variables about vehicles. For a description of the variables, please refer to R by typing help(Auto) after loading the package ISLR. Auto &lt;- as_tibble(Auto, rownames = NA) Auto &lt;- Auto %&gt;% select(-name) %&gt;% mutate(cylinders = as.factor(cylinders), year = as.factor(year), origin = as.factor(origin)) Question (a), (b) and (c) Auto %&gt;% summary_df() %&gt;% print_summary_df() Factor variables cylinders cylinders Count 3 4 4 199 5 3 6 83 8 103 year year Count 70 29 71 27 72 28 73 40 74 26 75 30 76 34 77 28 78 36 79 29 80 27 81 28 82 30 origin origin Count 1 245 2 68 3 79 Numeric variables Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum mpg 0 127 37.6 23.45 60.92 9 13.000 14 17.000 22.75 29.000 34.19 37.000 46.6 displacement 0 81 387.0 194.41 10950.37 68 85.000 90 105.000 151.00 275.750 350.00 400.000 455.0 horsepower 0 93 184.0 104.47 1481.57 46 60.550 67 75.000 93.50 126.000 157.70 180.000 230.0 weight 0 346 3527.0 2977.58 721484.71 1613 1931.600 1990 2225.250 2803.50 3614.750 4277.60 4464.000 5140.0 acceleration 0 95 16.8 15.54 7.61 8 11.255 12 13.775 15.50 17.025 19.00 20.235 24.8 Question (d) Auto %&gt;% slice(-c(10:85)) %&gt;% summary_df() %&gt;% print_summary_df() Factor variables cylinders cylinders Count 3 3 4 166 5 3 6 71 8 73 year year Count 70 9 71 0 72 0 73 39 74 26 75 30 76 34 77 28 78 36 79 29 80 27 81 28 82 30 origin origin Count 1 194 2 54 3 68 Numeric variables Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum mpg 0 125 35.6 24.40 61.89 11.0 13.0 15.0 18.00 23.95 30.55 35.4 37.775 46.6 displacement 0 70 387.0 187.24 9935.78 68.0 85.0 90.0 100.25 145.50 250.00 350.0 360.000 455.0 horsepower 0 84 184.0 100.72 1275.12 46.0 60.0 65.0 75.00 90.00 115.00 150.0 170.000 230.0 weight 0 279 3348.0 2935.97 658208.03 1649.0 1934.0 1985.0 2213.75 2792.50 3508.00 4177.5 4391.250 4997.0 acceleration 0 92 16.3 15.73 7.26 8.5 11.4 12.5 14.00 15.50 17.30 19.0 20.475 24.8 Question (e) Figure 2.6: Pairs plot Question (f) The variables displacement, weight and origin have a huge correlation with the variable to predict, mpg. So, this ones could particularly useful for the prediction. The relation between these variables does not seem to be linear but instead in \\(\\exp(-x)\\). Moreover, the miles per gallon look very different depending on the origin of the car. 2.2.3 Exercise 10. This exercise is about the Boston dataset. Question (a) Boston &lt;- as_tibble(Boston, rownames = NA) Boston &lt;- Boston %&gt;% mutate(chas = as.logical(chas), rad = as.factor(rad)) It contains 506 observations of 14 variables about housing values in suburbs of Boston. Each of the observation represents a suburb of Boston. For a description of the variables, please refer to R by typing help(Boston) after loading the package MASS. Question (b) Figure 2.7: Pairs plot We can see some interesting correlations in this dataset. For exemple, the mean distances to five Boston employement has a correlation of -0.77 with the nitrogen oxides concentration. Or, the lower status of the population are related to the average number of rooms per dwelling (-0.61 for correlation). The variable that are the most related with the crime rate by town is the full-value property-tax rate per $10,000. Question (c) The variable crim seems to be associated with the variables tax, lstat and nox because they have quite a large correlation with the variable of interest (cf. previous question). Question (d) Figure 2.8: Boxplots of some variables. Half of the suburbs have less than 10% of crime rates, but for three of them, this rate is greater than 70%. The range for this variable is very important. The tax rates do not seem to have outliers but the range is also very important. Indeed, the tax can go from under $200,000 to above $700,000. Finally, there are some suburbs in Boston that have a very low pupil-teacher ratio compare to the others. However, the range is not very wide. Question (e) There are 35 suburbs that bound the Charles river. Question (f) The median pupil-teacher ratio among the towns of Boston is 19.05%. Question (g) Boston[which(Boston$medv == min(Boston$medv)),] %&gt;% print_df() crim zn indus chas nox rm age dis rad tax ptratio black lstat medv 38.3518 0 18.1 FALSE 0.693 5.453 100 1.4896 24 666 20.2 396.90 30.59 5 67.9208 0 18.1 FALSE 0.693 5.683 100 1.4254 24 666 20.2 384.97 22.98 5 Two suburbs share the lowest median value of owner-occupied homes in $1000s. They have pretty the same values for the other predictors (except maybe for the crime rate). Question (h) There are 64 suburbs with an average of number of rooms per dwelling larger than 7 and 13 with more than 8. Boston[which(Boston$rm &gt; 8),] %&gt;% print_df() crim zn indus chas nox rm age dis rad tax ptratio black lstat medv 0.12083 0 2.89 FALSE 0.4450 8.069 76.0 3.4952 2 276 18.0 396.90 4.21 38.7 1.51902 0 19.58 TRUE 0.6050 8.375 93.9 2.1620 5 403 14.7 388.45 3.32 50.0 0.02009 95 2.68 FALSE 0.4161 8.034 31.9 5.1180 4 224 14.7 390.55 2.88 50.0 0.31533 0 6.20 FALSE 0.5040 8.266 78.3 2.8944 8 307 17.4 385.05 4.14 44.8 0.52693 0 6.20 FALSE 0.5040 8.725 83.0 2.8944 8 307 17.4 382.00 4.63 50.0 0.38214 0 6.20 FALSE 0.5040 8.040 86.5 3.2157 8 307 17.4 387.38 3.13 37.6 0.57529 0 6.20 FALSE 0.5070 8.337 73.3 3.8384 8 307 17.4 385.91 2.47 41.7 0.33147 0 6.20 FALSE 0.5070 8.247 70.4 3.6519 8 307 17.4 378.95 3.95 48.3 0.36894 22 5.86 FALSE 0.4310 8.259 8.4 8.9067 7 330 19.1 396.90 3.54 42.8 0.61154 20 3.97 FALSE 0.6470 8.704 86.9 1.8010 5 264 13.0 389.70 5.12 50.0 0.52014 20 3.97 FALSE 0.6470 8.398 91.5 2.2885 5 264 13.0 386.86 5.91 48.8 0.57834 20 3.97 FALSE 0.5750 8.297 67.0 2.4216 5 264 13.0 384.54 7.44 50.0 3.47428 0 18.10 TRUE 0.7180 8.780 82.9 1.9047 24 666 20.2 354.55 5.29 21.9 "],
["linear-regression.html", "Chapter 3 Linear Regression 3.1 Conceptual Exercises 3.2 Applied Exercises", " Chapter 3 Linear Regression 3.1 Conceptual Exercises 3.1.1 Exercise 1. Recall the model for the Advertising data: \\[sales = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times newspaper + \\epsilon\\] After fitting the model, we found out this results: Least square coefficient estimates of the linear model Coefficient Std. error t-statistic p-value Intercept 2.939 0.3119 9.42 &lt; 0.001 TV 0.046 0.0014 32.81 &lt; 0.001 radio 0.189 0.0086 21.89 &lt; 0.001 newspaper -0.001 0.0059 -0.18 0.8599 The null hypotheses to which the p-values given in the table correspond is \\[H_0 : \\beta_0 = \\beta_1 = \\beta_2 = \\beta_3 = 0\\] versus \\[H_1 : \\beta_0 \\neq 0, \\beta_1 \\neq 0, \\beta_2 \\neq 0 ~\\text{and}~ \\beta_3 \\neq 0\\] This p-values means that the two features TV and radio are significant for the Advertising data. However, the feature newspaper is not statistically significant. So, this varible does not have an influence on the sales. 3.1.2 Exercise 2. In both the \\(K\\) nearest neighbors regression and classification, the beginning point is to identifies the \\(K\\) points that are the closest to a given points \\(x_0\\). Let call this set \\(\\mathcal{N}_0\\). But, the quantity of interest is to not same. In the regression setting, we are interested by the estimation of \\(\\hat{f}(x_0)\\). This is done by averaging the values of the \\(K\\) closest points of \\(x_0\\). \\[\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_0} y_i\\] In the classification setting, we first aim to predict the probability of belonging to some class \\(j\\). This is done by counting the proportion of the \\(K\\) closest points of \\(x_0\\) that belong to the class \\(j\\). \\[\\mathbb{P}(Y = j \\mid X = x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_0}\\mathcal{1}(y_i = j)\\] Finally, using the Bayes rules, we classify \\(x_0\\) in the class with the largest probability. 3.1.3 Exercise 3. The linear model is: \\[Y = \\widehat{\\beta}_0 + \\sum_{i = 1}^{5}\\widehat{\\beta}_iX_i.\\] Question (a) For a fixed value of IQ (\\(X_2\\)) and GPA (\\(X_1\\)), females earn more on average than males because the coefficient of Gender (\\(X_3\\)) is positive. However, for a fixed value of IQ (\\(X_2\\)) and GPA (\\(X_1\\)), males earn more on average than females provided that the GPA is high enough (larger than 3.5) because the coefficient of the interaction between GPA and Gender (\\(X_5\\)) is negative. Question (b) The salary of a woman with IQ of 110 and a GPA of 4.0 is (in thousands of dollars): \\[Y = 50 + 20\\times 4.0 + 0.07\\times 110 + 35\\times 1 + 0.01\\times 4.0\\times 110 - 10\\times 4.0\\times 1 = 137.1.\\] Question (c) With only the value of \\(\\widehat{\\beta}_4\\), we can not conclude on the evidence of an GPA/IQ interaction effect on the starting salary after graduation even if this value is very small. To test that, we should compute the F-statistic and the corresponding p-value. If the p-value is very low (usually under 0.05), there is a clear evidence of the relationship between the GPA/IQ interaction and the salary. 3.1.4 Exercise 4. Question (a) and (b) The true relationship between \\(X\\) and \\(Y\\) is linear. So, the cubic regression model will overfit. For the training, we expect the RSS for the cubic model to be lower than the one of the linear model because it will follow the noise more closely. However, on the test set, it should be the opposite. Question (c) and (d) The true relationship between \\(X\\) and \\(Y\\) is not linear. As before, the cubic regression should have a lower train RSS because this model is more flexible than the linear model. However, we can not tell which one will be lower for the test set because it depends on “how far the data is from linear”. If the true relationship is almost linear, the test RSS will be lower for the linear model and conversely, if the true relationship is closer to the cubic model, the test RSS will be lower for the cubic model. 3.1.5 Exercise 5. \\[ \\widehat{y}_i = x_i\\widehat{\\beta} = x_i \\frac{\\sum_{j=1}^n x_jy_j}{\\sum_{k=1}^n x^2_k} = \\sum_{j=1}^n\\frac{x_jx_i}{\\sum_{k=1}^n x^2_k}y_j\\] So, \\(a_j = \\frac{x_jx_i}{\\sum_{k=1}^n x^2_k}\\). We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values. 3.1.6 Exercise 6. The simple linear model is: \\(y_i = \\beta_0 + \\beta_1x_i\\). The estimator of \\(\\beta_0\\) and \\(\\beta_1\\) are: \\[\\widehat{\\beta_0} = \\bar{y} - \\widehat{\\beta_1}\\bar{x} \\quad\\text{and}\\quad \\widehat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\] Let’s predict the value of \\(y_i\\) for \\(x_i = \\bar{x}\\). \\[\\widehat{\\beta_0} + \\widehat{\\beta_1}\\bar{x} = \\bar{y} - \\widehat{\\beta_1}\\bar{x} + \\widehat{\\beta_1}\\bar{x} = \\bar{y}\\] So, the least squares line always passes through the point \\((\\bar{x}, \\bar{y})\\). 3.1.7 Exercise 7. Assume that \\(\\bar{x} = \\bar{y} = 0\\). \\[R^2 = 1 - \\frac{\\sum(y_i - \\widehat{y}_i)^2}{\\sum y_i^2} = \\frac{\\sum y_i^2 - \\sum y_i^2 + 2\\sum y_i\\widehat{y}_i - \\sum\\widehat{y}_i^2}{\\sum y_i^2}\\] We know that \\(\\widehat{y}_i = x_i\\widehat{\\beta}\\). So \\[ R^2 = \\frac{2\\sum y_ix_i\\widehat{\\beta} - \\sum x_i^2\\widehat{\\beta^2}}{\\sum y_i^2} = \\widehat{\\beta}\\frac{2\\sum y_ix_i - \\sum x_i^2\\widehat{\\beta}}{\\sum y_i^2}\\] Replace \\(\\widehat{\\beta}\\) by \\(\\frac{\\sum y_ix_i}{\\sum x_i^2}\\), and \\(\\sum x_i^2\\widehat{\\beta} = \\sum x_iy_i\\). Finally, \\[R^2 = \\frac{2(\\sum x_iy_i)^2 - (\\sum x_iy_i)^2}{\\sum x_i^2\\sum y_i^2} = \\frac{(\\sum x_iy_i)^2}{\\sum x_i^2\\sum y_i^2} = Corr^2\\] In the case of simple linear regression of \\(Y\\) onto \\(X\\), the \\(R^2\\) statistic is equal to the square of the correlation between \\(X\\) and \\(Y\\). 3.2 Applied Exercises 3.2.1 Exercise 8. This exercise is about fitting a simple linear model to the Auto dataset. It contains 392 observations of 9 variables about vehicles. For a description of the variables, please refer to R by typing help(Auto) after loading the package ISLR. Question (a) lm_model &lt;- lm(mpg ~ horsepower, data = auto) lm_model %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the auto dataset. Formula: mpg ~ horsepower Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 333 30.5 0 24 -13.57 -6.99 -5.97 -3.26 -0.34 2.76 7.11 8.77 16.92 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 39.93586 0.71750 55.65984 &lt; 2.22e-16 horsepower -0.15784 0.00645 -24.48914 &lt; 2.22e-16 Residual standard error: 4.906 on 390 degrees of freedom. Multiple \\(R^2\\): 0.606. Adjusted \\(R^2\\): 0.605. F-statistic: 599.718 on 1 and 390, p-value: &lt;2e-16. We test the hypothesis \\(H_0: \\beta_1 = 0\\). We use the F-statistic to determine whether or not we should reject the null hypothesis. As the p-value associated with the the F-statistic is very low, there is a clear evidence of a relationship between mpg and horsepower. There are two measures of model accuracy to characterize how strong is the relationship between the response and the variable, the RSE and the \\(R^2\\). For the linear model, the RSE is 4.91 mpg. The mean value for mpg is 23.45, it indicates a percentage error of roughly 21%. Then, the \\(R^2\\) statistic is equal to 0.61. So, horsepower explains roughly 61% of the variance in mpg. The variable horsepower has a negative relationship with the response mpg because \\(\\beta_1 &lt; 0\\). The predicted value of mpg for horsepower = 98 is 24.47. The 95% confidence interval is (23.97, 24.96) and the 95% prediction interval is (14.81, 34.12). Question (b) Figure 3.1: Least square regression plot for the auto dataset. Question (c) Figure 3.2: Diagnostic plots of the least square regression fit. The plot of the fitted values agains the residuals shows a non-linear pattern in the data. So, non-linear transformation of the data, such as a quadratic transformation, could improve the fit. It does not seem to have another trouble with the fit, maybe one observation with a quite large Cook distance but that’s all. 3.2.2 Exercise 9. This exercise is about fitting a multiple linear model to the Auto dataset. It contains 392 observations of 9 variables about vehicles. For a description of the variables, please refer to R by typing help(Auto) after loading the package ISLR. Question (a) Figure 3.3: Scatterplot matrix. Question (b) Figure 3.4: Correlation plot. Question (c) lm_model &lt;- lm(mpg ~ ., data = auto) lm_model %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the auto dataset. Formula: mpg ~ . Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 392 22.65 0 10.88 -9.59 -5.09 -3.79 -2.16 -0.12 1.87 3.79 5.27 13.06 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.21843 4.64429 -3.70744 0.00024018 cylinders -0.49338 0.32328 -1.52615 0.12779647 displacement 0.01990 0.00752 2.64743 0.00844465 horsepower -0.01695 0.01379 -1.22951 0.21963282 weight -0.00647 0.00065 -9.92879 &lt; 2.22e-16 acceleration 0.08058 0.09884 0.81517 0.41547802 year 0.75077 0.05097 14.72880 &lt; 2.22e-16 origin 1.42614 0.27814 5.12749 4.6657e-07 Residual standard error: 3.328 on 384 degrees of freedom. Multiple \\(R^2\\): 0.821. Adjusted \\(R^2\\): 0.818. F-statistic: 252.428 on 7 and 384, p-value: &lt;2e-16. We have fitted a multiple regression model of mpg onto all the other variables in the auto dataset. In order to say that there is a relationship between the features and the response, we should test the hypothesis of all \\(\\beta_i = 0\\). For the model, the p-value associated with the F-statistic is less than \\(2\\times 10^{-16}\\). So, there is a clear evidence of a relationship between mpg and the features. There are four variables that have a statistically significant relationship with the reponse (because they have a very low p-value). These variables are displacement, weight, year and origin. The coefficient for the year variable suggest that the mpg grows with the year, so recent cars consume less than old cars. Question (d) Figure 3.5: Diagnotistic plots of the multiple least square regression fit. There is a little trend in the residuals, meaning that the linear model may not be the best model to fit on the data. Moreover, the variance of error terms seems to be non-constant. Based on this two observations, some transformation of the data could be a good idea. There is no outliers, but the point 14 appears to be a high leverage point. Question (e) # Recall that cylinders:year stands for the inclusion of the interaction term between # cylinders and years in the regression. cylinders*year adds the cylinders, year and # cylinders:year to the model. .:. adds all the interactions to the model and .*. adds # all the variable and their interaction to the model. lm_model_int &lt;- lm(mpg ~ .*., data = auto) lm_model_int %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the auto dataset. Formula: mpg ~ . * . Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 392 18.77 0 6.74 -7.63 -4.12 -3.2 -1.45 0.06 1.27 2.73 4 11.14 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 35.47889 53.13579 0.66770 0.5047479 cylinders 6.98858 8.24797 0.84731 0.3973815 displacement -0.47854 0.18935 -2.52722 0.0119207 horsepower 0.50343 0.34700 1.45082 0.1476933 weight 0.00413 0.01759 0.23490 0.8144183 acceleration -5.85917 2.17362 -2.69558 0.0073536 year 0.69743 0.60967 1.14395 0.2533996 origin -20.89557 7.09709 -2.94424 0.0034459 cylinders:displacement -0.00338 0.00646 -0.52412 0.6005130 cylinders:horsepower 0.01161 0.02420 0.47993 0.6315682 cylinders:weight 0.00036 0.00090 0.39918 0.6899953 cylinders:acceleration 0.27787 0.16642 1.66969 0.0958433 cylinders:year -0.17413 0.09714 -1.79250 0.0738852 cylinders:origin 0.40217 0.49262 0.81638 0.4148169 displacement:horsepower -0.00008 0.00029 -0.29434 0.7686659 displacement:weight 0.00002 0.00001 1.68205 0.0934193 displacement:acceleration -0.00348 0.00334 -1.04107 0.2985339 displacement:year 0.00593 0.00239 2.48202 0.0135156 displacement:origin 0.02398 0.01947 1.23200 0.2187484 horsepower:weight -0.00002 0.00003 -0.67321 0.5012433 horsepower:acceleration -0.00721 0.00372 -1.93920 0.0532521 horsepower:year -0.00584 0.00394 -1.48218 0.1391606 horsepower:origin 0.00223 0.02930 0.07619 0.9393091 weight:acceleration 0.00023 0.00023 1.02518 0.3059597 weight:year -0.00022 0.00021 -1.05568 0.2918156 weight:origin -0.00058 0.00159 -0.36379 0.7162292 acceleration:year 0.05562 0.02558 2.17427 0.0303306 acceleration:origin 0.45832 0.15666 2.92555 0.0036547 year:origin 0.13926 0.07399 1.88212 0.0606197 Residual standard error: 2.695 on 363 degrees of freedom. Multiple \\(R^2\\): 0.889. Adjusted \\(R^2\\): 0.881. F-statistic: 104.2 on 28 and 363, p-value: &lt;2e-16. Three interactions appears to be statistically significant, displacement:year, acceleration:origin and acceleration:year because they have p-value smaller than 0.05. Question (f) lm_model_squ &lt;- lm(mpg ~ . + I(horsepower^2), data = auto) lm_model_squ %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the auto dataset. Formula: mpg ~ . + I(horsepower^2) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 392 20.55 0 8.82 -8.55 -4.62 -3.6 -1.73 -0.22 1.59 3.22 5.26 12 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.32366 4.62477 0.28621 0.77487184 cylinders 0.34891 0.30483 1.14459 0.25309407 displacement -0.00756 0.00737 -1.02598 0.30554979 horsepower -0.31946 0.03434 -9.30168 &lt; 2.22e-16 weight -0.00327 0.00068 -4.82000 2.0735e-06 acceleration -0.33060 0.09918 -3.33315 0.00094235 year 0.73534 0.04599 15.98852 &lt; 2.22e-16 origin 1.01441 0.25455 3.98505 8.0778e-05 I(horsepower^2) 0.00101 0.00011 9.44884 &lt; 2.22e-16 Residual standard error: 3.001 on 383 degrees of freedom. Multiple \\(R^2\\): 0.855. Adjusted \\(R^2\\): 0.852. F-statistic: 282.813 on 8 and 383, p-value: &lt;2e-16. If we add the square transformation of the horsepower variable to the model, the model seems to be significative (the p-value of the F-statistic is still very low). The RSE is smaller for this model than for the linear model, however, it is still larger than the one of the model with all the interactions. 3.2.3 Exercise 10. This exercise is about fitting a multiple linear model to the Carseats dataset. It contains 400 observations of 11 variables about sales of child car seats at different stores. For a description of the variables, please refer to R by typing help(Carseats) after loading the package ISLR. Question (a) lm_model &lt;- lm(Sales ~ Price + Urban + US, data = carseats) lm_model %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the carseats dataset. Formula: Sales ~ Price + Urban + US Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 400 13.98 0 6.07 -6.92 -3.93 -3.19 -1.62 -0.06 1.58 3.18 4.24 7.06 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 13.04347 0.65101 20.03567 &lt; 2.22e-16 Price -0.05446 0.00524 -10.38923 &lt; 2.22e-16 UrbanYes -0.02192 0.27165 -0.08068 0.93574 USYes 1.20057 0.25904 4.63467 4.8602e-06 Residual standard error: 2.472 on 396 degrees of freedom. Multiple \\(R^2\\): 0.239. Adjusted \\(R^2\\): 0.234. F-statistic: 41.519 on 3 and 396, p-value: &lt;2e-16. Question (b) The variable Price seems to be very significative (p-value very close to 0) and its coefficient is negative (-0.0544588). So, a $100 increase in price company charges for car seats is associated with an average decrease in sales by around 5446 units. The p-value of the coefficient for the dummy variable UrbanYes is very close to 1. So, this indicates that there is no statistical evidence of a difference in unit sales (in thousands) of child car seats between the stores that are in urban or rural location. Finally, the variable USYes is also significative and its coefficient is positive (1.2005727). So, if the store is in the US will increase the sales by around 1201 units. Question (c) The fitted model is: \\[ y_i = \\beta_0 + \\beta_1x_{1, i} + \\beta_2x_{2, i} + \\beta_3x_{3, i} + \\epsilon_i\\] where \\(y_i\\) represents the Sales (Unit sales in thousands at each location); \\(x_{1, i}\\) represents the Price (Price company charges for seats at each site); \\(x_{2, i} = 1\\) if the store is in an urban location, \\(0\\) is the store is in an rural location (UrbanYes); \\(x_{3, i} = 1\\) if the store is in the US, \\(0\\) if not (USYes). Question (d) We can reject the null hypothesis for the variable Price and USYes (cf. question (b)). Question (e) lm_model2 &lt;- lm(Sales ~ Price + US, data = carseats) lm_model2 %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the carseats dataset. Formula: Sales ~ Price + US Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 400 13.98 0 6.07 -6.93 -3.94 -3.17 -1.63 -0.06 1.58 3.17 4.23 7.05 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 13.03079 0.63098 20.65179 &lt; 2.22e-16 Price -0.05448 0.00523 -10.41612 &lt; 2.22e-16 USYes 1.19964 0.25846 4.64148 4.7072e-06 Residual standard error: 2.469 on 397 degrees of freedom. Multiple \\(R^2\\): 0.239. Adjusted \\(R^2\\): 0.235. F-statistic: 62.431 on 2 and 397, p-value: &lt;2e-16. Question (f) The two models fit equally well on the data, they have the same \\(R^2\\) (this is another argument for dropping the variable Urban). However, we can consider that the second model is better because it uses less features. Question (g) Table 3.1: 95% confidence intervals for the coefficients 2.5 % 97.5 % (Intercept) 11.7903202 14.2712653 Price -0.0647598 -0.0441954 USYes 0.6915196 1.7077663 Question (h) Figure 3.6: Diagnotistic plots of the multiple least square regression fit. After looking at the different diagnostic plots, there is no particular evidence of outliers or high leverage observations in the model. However, the linear model may not be the better model for this data. 3.2.4 Exercise 11. This exercise is about the t-statistic on models without intercept. For that, we will generate some data with the following “very” simple model: \\[y = 2x + \\epsilon \\quad\\text{where}\\quad \\epsilon \\sim \\mathcal{N}(0,1).\\] set.seed(42) x &lt;- rnorm(100) y &lt;- 2*x + rnorm(100) Question (a) lm_model_no_inter &lt;- lm(y ~ x + 0) lm_model_no_inter %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x + 0 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 4.75 -0.09 0.82 -1.98 -1.49 -1.3 -0.59 -0.07 0.45 1.13 1.4 2.77 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) x 2.02449 0.0876 23.11114 &lt; 2.22e-16 Residual standard error: 0.908 on 99 degrees of freedom. Multiple \\(R^2\\): 0.844. Adjusted \\(R^2\\): 0.842. F-statistic: 534.125 on 1 and 99, p-value: &lt;2e-16. The p-value associated with the null hypothesis \\(\\beta = 0\\) is very close to 0, so there is a strong evidence that the coefficient is statistically different from 0. The estimation of \\(\\beta\\) is 2.0244855 which is very close to the true value. Question (b) lm_model_no_inter2 &lt;- lm(x ~ y + 0) lm_model_no_inter2 %&gt;% summary() %&gt;% print_summary_lm() Formula: x ~ y + 0 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 2.4 0.04 0.17 -1.57 -0.6 -0.49 -0.21 0.07 0.32 0.55 0.66 0.83 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) y 0.41671 0.01803 23.11114 &lt; 2.22e-16 Residual standard error: 0.412 on 99 degrees of freedom. Multiple \\(R^2\\): 0.844. Adjusted \\(R^2\\): 0.842. F-statistic: 534.125 on 1 and 99, p-value: &lt;2e-16. The value of the t-statistic is the same than for the previous model. So, we can reject the null hypothesis and the estimation of \\(\\beta\\) is, as before, quite close to the true value. Question (c) We can write the model from the question (a) as \\(Y= \\beta_1X + \\epsilon\\) and the one from the question (b) as \\(X = \\beta_2Y + \\epsilon\\). The least square estimation of the parameters leads to \\(\\widehat{\\beta}_1 = (X^\\top X)^{-1}X^\\top Y\\) and \\(\\widehat{\\beta}_2 = (Y^\\top Y)^{-1}Y^\\top X\\). Moreover, we have that \\(X^\\top Y = Y^\\top X\\). So, we have that: \\[\\widehat{\\beta}_1 = (X^\\top X)^{-1}X^\\top Y = Y^\\top X(X^\\top X)^{-1} = (Y^\\top Y)\\widehat{\\beta}_2(X^\\top X)^{-1}.\\] And finally, the relationship between \\(\\widehat{\\beta}_1\\) and \\(\\widehat{\\beta}_2\\) is: \\[\\frac{\\widehat{\\beta}_1}{\\widehat{\\beta}_2} = (Y^\\top Y)(X^\\top X)^{-1},\\] which can be reduce to \\(\\frac{Var(Y)}{Var(X)}\\) in the univariate case. Question (d) t_stat &lt;- (sqrt(100 - 1) * sum(x * y)) / (sqrt(sum(x * x)*sum(y * y) - sum(x * y)**2)) Numerically, we found out that the t-statistic is 23.111142 which is coherent with the result given by the lm funtion. Algebraically, the derivations are pretty straightforward: \\[\\begin{align} \\frac{\\widehat{\\beta}}{SE(\\widehat{\\beta})} &amp;= \\frac{\\sum x_iy_i}{\\sum x_i^2} \\sqrt{\\frac{(n-1)\\sum x_i^2}{\\sum (y_i - x_i\\widehat{\\beta})^2}} \\\\ &amp;= \\frac{\\sqrt{n-1}\\sum x_iy_i}{\\sqrt{x_i^2}\\sqrt{\\sum y_i^2 - 2\\frac{\\sum x_iy_i}{\\sum x_i^2}\\sum x_iy_i + \\left(\\frac{\\sum x_iy_i}{\\sum x_i^2}\\right)^2\\sum x_i^2}} \\\\ &amp;= \\frac{\\sqrt{n-1}\\sum x_iy_i}{\\sqrt{x_i^2}\\sqrt{\\sum y_i^2 - 2\\frac{\\left(\\sum x_iy_i\\right)^2}{\\sum x_i^2} + \\left(\\frac{\\sum x_iy_i}{\\sum x_i^2}\\right)^2\\sum x_i^2}} \\\\ &amp;= \\frac{\\sqrt{n-1}\\sum x_iy_i}{\\sqrt{\\sum x_i^2\\sum y_i^2 - 2\\left(\\sum x_iy_i\\right)^2 + \\left(\\sum x_iy_i\\right)^2}} \\\\ &amp;= \\frac{\\sqrt{n-1}\\sum x_iy_i}{\\sqrt{\\sum x_i^2\\sum y_i^2 - \\left(\\sum x_iy_i\\right)^2}} \\end{align}\\] Question (e) We can exchange \\(X\\) and \\(Y\\) in the formula of the t-statistic given in (d) without changing the results (which is the t-statistic). So, the t-statistic for the regression of \\(Y\\) onto \\(X\\) is the same as the t-statistic for the regression of \\(X\\) onto \\(Y\\). Question (f) lm_model &lt;- lm(y ~ x) lm_model %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 4.75 0 0.82 -1.89 -1.4 -1.21 -0.51 0.01 0.54 1.21 1.49 2.86 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.08837 0.09088 -0.97237 0.33326 x 2.02716 0.08767 23.12391 &lt; 2e-16 Residual standard error: 0.908 on 98 degrees of freedom. Multiple \\(R^2\\): 0.845. Adjusted \\(R^2\\): 0.844. F-statistic: 534.715 on 1 and 98, p-value: &lt;2e-16. lm_model2 &lt;- lm(x ~ y) lm_model2 %&gt;% summary() %&gt;% print_summary_lm() Formula: x ~ y Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 2.4 0 0.17 -1.61 -0.64 -0.53 -0.25 0.03 0.27 0.51 0.62 0.79 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.04188 0.04119 1.01655 0.31187 y 0.41689 0.01803 23.12391 &lt; 2e-16 Residual standard error: 0.412 on 98 degrees of freedom. Multiple \\(R^2\\): 0.845. Adjusted \\(R^2\\): 0.844. F-statistic: 534.715 on 1 and 98, p-value: &lt;2e-16. Considering the two tables, we can argue that the t-statistic for \\(H_0 : \\beta_1 = 0\\) is the same for both regressions. 3.2.5 Exercise 12. This exercise involves simple linear regression without an intercept. Question (a) If we regress \\(Y\\) onto \\(X\\) without an intercept, we found that: \\(\\widehat{\\beta}_1 = \\sum x_iy_i / \\sum x_i^2\\). If we regress \\(X\\) onto \\(Y\\) without an intercept, we found that: \\(\\widehat{\\beta}_2 = \\sum x_iy_i / \\sum y_i^2\\). So, for \\(\\widehat{\\beta}_1\\) to be equal to \\(\\widehat{\\beta}_2\\), we should have \\(\\sum x_i^2 = \\sum y_i^2\\). First, assume that \\(\\mathbb{E}(X) = \\mathbb{E}(Y)\\). So, \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2 \\iff \\text{Var}(X) = \\text{Var}(Y)\\). If there is a linear relationship between \\(X\\) and \\(Y\\), it should exists \\(a \\in \\mathbb{R}\\) such that \\(Y = aX\\). And then \\[\\begin{align} \\text{Var}(X) = \\text{Var}(Y) &amp;\\iff \\text{Var}(X) = \\text{Var}(aX) \\\\ &amp;\\iff \\text{Var}(X) = a^2\\text{Var}(X) \\\\ &amp;\\iff a = 1 \\text{ or } a = -1. \\end{align}\\] Second, remove the assumption. If \\(\\sum x_i^2 = \\sum y_i^2\\), then \\[\\text{Var}(X) + \\mathbb{E}(X)^2 = \\text{Var}(Y) + \\mathbb{E}(Y)^2.\\] Again, if we assume linear relationship between \\(X\\) and \\(Y\\), it should exists \\(a \\in \\mathbb{R}\\) such that \\(Y = aX\\). And then \\[\\begin{align} \\text{Var}(X) + \\mathbb{E}(X)^2 = \\text{Var}(Y) + \\mathbb{E}(Y)^2 &amp;\\iff \\text{Var}(X) + \\mathbb{E}(X)^2 = a^2\\left(\\text{Var}(X) + \\mathbb{E}(X)^2\\right)\\\\ &amp;\\iff a = 1 \\text{ or } a = -1. \\end{align}\\] Finally, if we do not assume a particular relationship between \\(X\\) and \\(Y\\), we can simulate two random variables that appears to be i.i.d. but if we regress one on each other, we found out the same coefficients estimate (see question (c)). Question (b) See exercise 11. Question (c) x &lt;- rnorm(10000, 2, 2) y &lt;- rnorm(10000, sqrt(6), sqrt(2)) lm(y ~ x + 0) %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x + 0 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 10000 16.3 1.25 3.55 -6.02 -1.89 -1.13 -0.01 1.25 2.52 3.64 4.34 10.27 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) x 0.60567 0.008 75.66767 &lt; 2.22e-16 Residual standard error: 2.261 on 9999 degrees of freedom. Multiple \\(R^2\\): 0.364. Adjusted \\(R^2\\): 0.364. F-statistic: 5725.596 on 1 and 9999, p-value: &lt;2e-16. lm(x ~ y + 0) %&gt;% summary() %&gt;% print_summary_lm() Formula: x ~ y + 0 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 10000 17.97 0.51 4.81 -8.95 -3.12 -2.32 -0.96 0.53 1.96 3.3 4.15 9.02 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) y 0.60118 0.00795 75.66767 &lt; 2.22e-16 Residual standard error: 2.252 on 9999 degrees of freedom. Multiple \\(R^2\\): 0.364. Adjusted \\(R^2\\): 0.364. F-statistic: 5725.596 on 1 and 9999, p-value: &lt;2e-16. 3.2.6 Exercise 13. Question (a) set.seed(42) x &lt;- rnorm(n = 100, mean = 0, sd = 1) Question (b) eps &lt;- rnorm(n = 100, mean = 0, sd = sqrt(0.25)) Question (c) y &lt;- -1 + 0.5*x + eps df &lt;- tibble(x, y) The length of the vector y is 100 (same as x and eps). The true value of \\(\\beta_0\\) is -1 and the one of \\(\\beta_1\\) is 0.5. Question (d) Figure 3.7: Scatterplot of y against x. Question (e) lm &lt;- lm(y ~ x) lm %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 2.38 0 0.2 -0.94 -0.7 -0.61 -0.25 0.01 0.27 0.61 0.74 1.43 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.04418 0.04544 -22.97996 &lt; 2.22e-16 x 0.51358 0.04383 11.71686 &lt; 2.22e-16 Residual standard error: 0.454 on 98 degrees of freedom. Multiple \\(R^2\\): 0.583. Adjusted \\(R^2\\): 0.579. F-statistic: 137.285 on 1 and 98, p-value: &lt;2e-16. The estimation of the coefficients are very closed to the true ones, and both are statistically significative. However, the \\(R^2\\) is quite poor (around 0.5), which seems to mean that the model does not explain the data very well. Question (f) Figure 3.8: Results of the linear regression of y against x. Question (g) lm_square &lt;- lm(y ~ x + I(x^2)) lm_square %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x + I(x^2) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 2.36 0 0.2 -0.95 -0.7 -0.62 -0.25 0 0.27 0.6 0.74 1.41 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.04812 0.05627 -18.62702 &lt; 2e-16 x 0.51513 0.04591 11.21951 &lt; 2e-16 I(x^2) 0.00362 0.03020 0.11990 0.90481 Residual standard error: 0.456 on 97 degrees of freedom. Multiple \\(R^2\\): 0.584. Adjusted \\(R^2\\): 0.575. F-statistic: 67.959 on 2 and 97, p-value: &lt;2e-16. Based on the table, there is no evidence that the quadratic term improves the model fit. In particular, the p-value for the coefficient of the quadratic term is very high, and thus this term is not statistically significative. Moreover, the \\(R^2\\) does not increase a lot when we add the quadratic term. Question (h), (i) and (j) Figure 3.9: Different results for the \\(R^2\\), the adjusted \\(R^2\\) and the RSE As we see on the graph, the \\(R^2\\) and the adjusted \\(R^2\\) will decresase when the variance of the noise increase, and conversely for the residual standard errors. This phenomena could be predicted because the more noise we add the less we can recover the true underlying function. Figure 3.10: Confidence intervals for the estimated coefficients The confidence intervals for the two coefficients show the same phenomena than the \\(R^2\\), the adjusted \\(R^2\\) and the residual standard errors. The more the variance of the noise increases, the more difficult it is to estimate the coefficients, and to have a good confidence interval on it. In fact, the confidence interval is larger for large noise and the coefficient can be further from the true one. 3.2.7 Exercise 14. This exercise is about the problem of collinearity in the features. Question (a) set.seed(42) x1 &lt;- runif(100) x2 &lt;- 0.5*x1 + rnorm(100)/10 y &lt;- 2 + 2*x1 + 0.3*x2 + rnorm(100) The linear model is the following: \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon \\text{ where } \\beta_0 = 2,~ \\beta_1 = 2,~ \\beta_2 = 0.3 \\text{ and } \\epsilon \\sim \\mathcal{N}(0, 1).\\] Question (b) ggplot() + geom_point(aes(x = x1, y = x2), size = 6) + annotate(&quot;text&quot;, x = 0.1, y = 0.6, label = paste(&#39;Correlation:&#39;, round(cor(x1, x2), 2)), size = 8, col = &#39;red&#39;) + xlab(&quot;$X_1$&quot;) + ylab(&quot;$X_2$&quot;) + theme_custom() Figure 3.11: Scatterplot of \\(X_1\\) against \\(X_2\\). Question (c) lm &lt;- lm(y ~ x1 + x2) lm %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x1 + x2 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 4.18 0 0.87 -2.1 -1.38 -1.26 -0.66 -0.03 0.65 1.27 1.41 2.08 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.04617 0.19102 10.71198 &lt; 2e-16 x1 1.55979 0.63982 2.43787 0.016595 x2 0.98083 1.02652 0.95549 0.341706 Residual standard error: 0.941 on 97 degrees of freedom. Multiple \\(R^2\\): 0.32. Adjusted \\(R^2\\): 0.306. F-statistic: 22.802 on 2 and 97, p-value: 7.64e-09. The value of \\(\\widehat{\\beta}_0\\) is 2.0461711 which is quite close to the true value of \\(\\beta_0\\). However, both the value of \\(\\widehat{\\beta}_1\\) (1.5597949) and \\(\\widehat{\\beta}_2\\) (0.9808272) appear to be quite far from their true value even if we obviously know that the true relationship between \\(Y\\), \\(X_1\\) and \\(X_2\\) is linear. Thanks to the p-value, we can reject the null hypothesis only for \\(\\beta_1\\), and so \\(\\beta_2\\) does not seem to be statistically significant. Finaly, the \\(R^2\\) is not very good. Question (d) lm_1 &lt;- lm(y ~ x1) lm_1 %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x1 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 4.11 0 0.87 -1.97 -1.42 -1.22 -0.69 -0.02 0.56 1.24 1.59 2.14 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.02128 0.18915 10.68623 &lt; 2.22e-16 x1 2.09295 0.31293 6.68812 1.4022e-09 Residual standard error: 0.94 on 98 degrees of freedom. Multiple \\(R^2\\): 0.313. Adjusted \\(R^2\\): 0.306. F-statistic: 44.731 on 1 and 98, p-value: 1.4e-09. The estimated coefficients are both statistically significant. The residual standard error and the \\(R^2\\) are the same than for the model with \\(X_2\\). We can reject the null hypothesis. Question (e) lm_2 &lt;- lm(y ~ x2) lm_2 %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x2 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 4.72 0 0.92 -2.45 -1.46 -1.23 -0.7 -0.06 0.66 1.23 1.58 2.27 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.29744 0.16483 13.93816 &lt; 2.22e-16 x2 3.16330 0.51481 6.14463 1.7269e-08 Residual standard error: 0.964 on 98 degrees of freedom. Multiple \\(R^2\\): 0.278. Adjusted \\(R^2\\): 0.271. F-statistic: 37.756 on 1 and 98, p-value: 1.73e-08. The estimated coefficients are both statistically significant. The residual standard error and the \\(R^2\\) are a little smaller than for the model with \\(X_1\\). We can reject the null hypothesis. Question (f) At a first sight, the different results can contradict each other, because it result to very different coefficients estimation and statistical significance. But, if we look at the true model, we see that \\(X_1\\) and \\(X_2\\) are very correlated. So, they explained the same variance in the data. In the complete model, they more or less share the explication of the variance but as they can explain the same amount of variance, the model with only \\(X_1\\) or \\(X_2\\) will lead to the same results. Another problem that can arrive in this case is that the matrix \\(X^\\prime X\\) may not be invertible. Question (g) x1 &lt;- c(x1, 0.1) x2 &lt;- c(x2, 0.8) y &lt;- c(y, 6) Question (h) lm &lt;- lm(y ~ x1 + x2) lm %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x1 + x2 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 101 4.45 0 0.91 -2.34 -1.41 -1.26 -0.71 -0.03 0.72 1.27 1.69 2.11 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.14668 0.19109 11.23357 &lt; 2.22e-16 x1 0.61282 0.51998 1.17854 0.2414353 x2 2.57296 0.80975 3.17746 0.0019877 Residual standard error: 0.964 on 98 degrees of freedom. Multiple \\(R^2\\): 0.323. Adjusted \\(R^2\\): 0.309. F-statistic: 23.327 on 2 and 98, p-value: 5.18e-09. The new point changes the significance of the coefficient for this model. Now, \\(\\beta_1\\) is not significant anymore, while \\(\\beta_2\\) is. The other values do not seem to change. We do not have an increasing of the RSE, so the new point is not an outlier. The leverage statistic of the new point is 0.1 while the average leverage is 0.03. So, the new point is a high-leverage point. # Fit the model with only X_1 lm_1 &lt;- lm(y ~ x1) lm_1 %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x1 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 101 5.7 0 1 -2.04 -1.48 -1.26 -0.73 -0.08 0.54 1.25 1.73 3.66 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.14814 0.19968 10.75791 &lt; 2.22e-16 x1 1.92083 0.33196 5.78629 8.4547e-08 Residual standard error: 1.007 on 99 degrees of freedom. Multiple \\(R^2\\): 0.253. Adjusted \\(R^2\\): 0.245. F-statistic: 33.481 on 1 and 99, p-value: 8.45e-08. The new point changes a lot the RSE and the \\(R^2\\). So, it should be consider as an outlier in this case. The leverage statistic of the new point is 0.03 while the average leverage is 0.02. So, the new point is a high-leverage point. # Fit the model with only X_2 lm_2 &lt;- lm(y ~ x2) lm_2 %&gt;% summary() %&gt;% print_summary_lm() Formula: y ~ x2 Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 101 4.68 0 0.92 -2.47 -1.47 -1.2 -0.68 -0.05 0.68 1.23 1.55 2.22 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.26526 0.16278 13.91614 &lt; 2.22e-16 x2 3.32847 0.49570 6.71473 1.1974e-09 Residual standard error: 0.966 on 99 degrees of freedom. Multiple \\(R^2\\): 0.313. Adjusted \\(R^2\\): 0.306. F-statistic: 45.088 on 1 and 99, p-value: 1.2e-09. The new point improve this model. So, this point is not an outlier. The leverage statistic of the new point is 0.02 while the average leverage is 0.02. So, the new point is not a high-leverage point. 3.2.8 Exercise 15. This exercise is about the Boston dataset. We would like to predict per capita crime rate based on the other features. It contains 506 observations of 14 variables of suburbs in Boston. For a description of the variables, please refer to R by typing help(Boston) after loading the pacakge MASS. boston &lt;- as_tibble(Boston) variable &lt;- c(&quot;zn&quot;, &quot;indus&quot;, &quot;chas&quot;, &quot;nox&quot;, &quot;rm&quot;, &quot;age&quot;, &quot;dis&quot;, &quot;rad&quot;, &quot;tax&quot;, &quot;ptratio&quot;, &quot;black&quot;, &quot;lstat&quot;, &quot;medv&quot;) Question (a) lm_list &lt;- vector(&quot;list&quot;, length = length(boston)-1) for(i in seq(1, length(boston)-1)){ name &lt;- variable[i] lm_list[[i]] &lt;- lm(boston$crim ~ boston[[name]]) } names(lm_list) &lt;- variable Variable zn Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 88.95 0 71.01 -4.43 -4.4 -4.37 -4.22 -2.62 1.25 6.3 11.34 84.52 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.45369 0.41722 10.67475 &lt; 2.22e-16 boston[[name]] -0.07393 0.01609 -4.59378 5.5065e-06 Residual standard error: 8.435 on 504 degrees of freedom. Multiple \\(R^2\\): 0.04. Adjusted \\(R^2\\): 0.038. F-statistic: 21.103 on 1 and 504, p-value: 5.51e-06. Variable indus Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 93.78 0 61.76 -11.97 -7.79 -5.54 -2.7 -0.74 0.71 3.59 8.63 81.81 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.06374 0.66723 -3.09301 0.0020913 boston[[name]] 0.50978 0.05102 9.99085 &lt; 2.22e-16 Residual standard error: 7.866 on 504 degrees of freedom. Multiple \\(R^2\\): 0.165. Adjusted \\(R^2\\): 0.164. F-statistic: 99.817 on 1 and 504, p-value: &lt;2e-16. Variable chas Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 88.97 0 73.76 -3.74 -3.72 -3.71 -3.66 -3.44 0.02 7.11 12.04 85.23 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.74445 0.39611 9.45302 &lt; 2e-16 boston[[name]] -1.89278 1.50612 -1.25673 0.20943 Residual standard error: 8.597 on 504 degrees of freedom. Multiple \\(R^2\\): 0.003. Adjusted \\(R^2\\): 0.001. F-statistic: 1.579 on 1 and 504, p-value: 0.209. Variable nox Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 94.1 0 60.87 -12.37 -5.88 -4.96 -2.74 -0.97 0.56 3.41 9.41 81.73 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -13.71988 1.69948 -8.07299 5.0768e-15 boston[[name]] 31.24853 2.99919 10.41899 &lt; 2.22e-16 Residual standard error: 7.81 on 504 degrees of freedom. Multiple \\(R^2\\): 0.177. Adjusted \\(R^2\\): 0.176. F-statistic: 108.555 on 1 and 504, p-value: &lt;2e-16. Variable rm Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 93.8 0 70.43 -6.6 -4.98 -4.64 -3.95 -2.65 0.99 6.92 11.26 87.2 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 20.48180 3.36447 6.08767 2.2720e-09 boston[[name]] -2.68405 0.53204 -5.04482 6.3467e-07 Residual standard error: 8.401 on 504 degrees of freedom. Multiple \\(R^2\\): 0.048. Adjusted \\(R^2\\): 0.046. F-statistic: 25.45 on 1 and 504, p-value: 6.35e-07. Variable age Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 89.64 0 64.78 -6.79 -6.16 -5.6 -4.26 -1.23 1.53 4.65 9.91 82.85 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.77791 0.94398 -4.00208 7.2217e-05 boston[[name]] 0.10779 0.01274 8.46282 2.8549e-16 Residual standard error: 8.057 on 504 degrees of freedom. Multiple \\(R^2\\): 0.124. Adjusted \\(R^2\\): 0.123. F-statistic: 71.619 on 1 and 504, p-value: 2.85e-16. Variable dis Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 88.38 0 63.32 -6.71 -5.9 -5.44 -4.13 -1.53 1.52 4.9 9.32 81.67 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.49926 0.73040 13.00561 &lt; 2.22e-16 boston[[name]] -1.55090 0.16833 -9.21346 &lt; 2.22e-16 Residual standard error: 7.965 on 504 degrees of freedom. Multiple \\(R^2\\): 0.144. Adjusted \\(R^2\\): 0.142. F-statistic: 84.888 on 1 and 504, p-value: &lt;2e-16. Variable rad Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 86.6 0 45.04 -10.16 -7.6 -5.08 -1.38 -0.14 0.66 1.7 3.31 76.43 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.28716 0.44348 -5.15735 3.6058e-07 boston[[name]] 0.61791 0.03433 17.99820 &lt; 2.22e-16 Residual standard error: 6.718 on 504 degrees of freedom. Multiple \\(R^2\\): 0.391. Adjusted \\(R^2\\): 0.39. F-statistic: 323.935 on 1 and 504, p-value: &lt;2e-16. Variable tax Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 90.21 0 48.86 -12.51 -6.59 -4.52 -2.74 -0.19 1.07 2.81 4.51 77.7 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -8.52837 0.81581 -10.45387 &lt; 2.22e-16 boston[[name]] 0.02974 0.00185 16.09939 &lt; 2.22e-16 Residual standard error: 6.997 on 504 degrees of freedom. Multiple \\(R^2\\): 0.34. Adjusted \\(R^2\\): 0.338. F-statistic: 259.19 on 1 and 504, p-value: &lt;2e-16. Variable ptratio Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 91.01 0 67.77 -7.65 -6.22 -5.57 -3.99 -1.91 1.82 5.13 10.17 83.35 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -17.64693 3.14727 -5.60706 3.3953e-08 boston[[name]] 1.15198 0.16937 6.80143 2.9429e-11 Residual standard error: 8.24 on 504 degrees of freedom. Multiple \\(R^2\\): 0.084. Adjusted \\(R^2\\): 0.082. F-statistic: 46.259 on 1 and 504, p-value: 2.94e-11. Variable black Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 100.58 0 63.02 -13.76 -4.87 -3 -2.3 -2.09 -1.3 5.76 11.85 86.82 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 16.55353 1.42590 11.60916 &lt; 2.22e-16 boston[[name]] -0.03628 0.00387 -9.36695 &lt; 2.22e-16 Residual standard error: 7.946 on 504 degrees of freedom. Multiple \\(R^2\\): 0.148. Adjusted \\(R^2\\): 0.147. F-statistic: 87.74 on 1 and 504, p-value: &lt;2e-16. Variable lstat Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 96.79 0 58.63 -13.93 -6.75 -5.38 -2.82 -0.66 1.08 3.91 8.14 82.86 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.33054 0.69376 -4.80072 2.087e-06 boston[[name]] 0.54880 0.04776 11.49065 &lt; 2.22e-16 Residual standard error: 7.664 on 504 degrees of freedom. Multiple \\(R^2\\): 0.208. Adjusted \\(R^2\\): 0.206. F-statistic: 132.035 on 1 and 504, p-value: &lt;2e-16. Variable medv Formula: boston$crim ~ boston[[name]] Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 90.03 0 62.83 -9.07 -5.47 -4.9 -4.02 -2.34 1.3 6.4 11.43 80.96 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.79654 0.93419 12.62757 &lt; 2.22e-16 boston[[name]] -0.36316 0.03839 -9.45971 &lt; 2.22e-16 Residual standard error: 7.934 on 504 degrees of freedom. Multiple \\(R^2\\): 0.151. Adjusted \\(R^2\\): 0.149. F-statistic: 89.486 on 1 and 504, p-value: &lt;2e-16. If we look at the results of the different fitted linear model, we can see that almost every variable is statistically significative (except the variable chas). However, We can split up the other ones into three categories based on the \\(R^2\\). The variable with a large \\(R^2\\) (\\(&gt; 0.3\\)) have a larger contribution to the variance explication (rad and tax). Then, there are variables with a moderate contribution to the variance: indus, nox, age, dis, black, lstat and medv (between \\(0.1\\) and \\(0.2\\)). Finally, some of them have a \\(R^2\\) very close to \\(0\\) (zn, rm and ptratio). Question (b) lm &lt;- lm(crim ~ ., data = boston) lm %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the boston dataset. Formula: crim ~ . Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 84.97 0 40.4 -9.92 -6.37 -4.51 -2.12 -0.35 1.02 2.49 4.66 75.05 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 17.03323 7.23490 2.35431 0.0189491 zn 0.04486 0.01873 2.39431 0.0170249 indus -0.06385 0.08341 -0.76558 0.4442940 chas -0.74913 1.18015 -0.63478 0.5258670 nox -10.31353 5.27554 -1.95497 0.0511520 rm 0.43013 0.61283 0.70188 0.4830888 age 0.00145 0.01793 0.08098 0.9354878 dis -0.98718 0.28182 -3.50289 0.0005022 rad 0.58821 0.08805 6.68045 6.4605e-11 tax -0.00378 0.00516 -0.73319 0.4637927 ptratio -0.27108 0.18645 -1.45390 0.1466113 black -0.00754 0.00367 -2.05196 0.0407023 lstat 0.12621 0.07572 1.66671 0.0962084 medv -0.19889 0.06052 -3.28652 0.0010868 Residual standard error: 6.439 on 492 degrees of freedom. Multiple \\(R^2\\): 0.454. Adjusted \\(R^2\\): 0.44. F-statistic: 31.47 on 13 and 492, p-value: &lt;2e-16. The multiple linear regression appears to be quite good on these data. The model is significative and the \\(R^2\\) is better than the one for every univariate linear model. We can reject the null hypothesis for the features zn, dis, rad, black and medv. The significative variable are not the same than the one for the univariate models. This may be the result of some colinearity problems. Question (c) coefs &lt;- tibble(variable = names(lm$coefficients), beta_multi = lm$coefficients) %&gt;% slice(-1) coefs$beta_uni &lt;- lm_list %&gt;% map(~ .x$coefficients[-1]) %&gt;% unlist() Figure 3.12: Plot of the coefficients from the univariate models against the multivariate model. As we see, the estimated coefficients are very different if we fit a multivariate model or multiple univariate models. Question (d) lm_list &lt;- vector(&quot;list&quot;, length = length(boston)-1) for(i in seq(1, length(boston)-1)){ name &lt;- variable[i] lm_list[[i]] &lt;- lm(boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3)) } names(lm_list) &lt;- variable Variable zn Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 88.95 0 69.68 -4.82 -4.79 -4.76 -4.61 -1.29 0.47 5.91 10.94 84.13 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.84605 0.43298 11.19220 &lt; 2.22e-16 boston[[name]] -0.33219 0.10981 -3.02517 0.0026123 I(boston[[name]]^2) 0.00648 0.00386 1.67912 0.0937505 I(boston[[name]]^3) -0.00004 0.00003 -1.20301 0.2295386 Residual standard error: 8.372 on 502 degrees of freedom. Multiple \\(R^2\\): 0.058. Adjusted \\(R^2\\): 0.053. F-statistic: 10.349 on 3 and 502, p-value: 1.28e-06. Variable indus Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 87.99 0 54.78 -8.28 -7.33 -6.22 -2.51 0.05 0.76 2.43 6.53 79.71 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.66257 1.57398 2.32694 0.020365 boston[[name]] -1.96521 0.48199 -4.07729 5.2971e-05 I(boston[[name]]^2) 0.25194 0.03932 6.40701 3.4202e-10 I(boston[[name]]^3) -0.00698 0.00096 -7.29205 1.1964e-12 Residual standard error: 7.423 on 502 degrees of freedom. Multiple \\(R^2\\): 0.26. Adjusted \\(R^2\\): 0.255. F-statistic: 58.688 on 3 and 502, p-value: &lt;2e-16. Variable chas Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 88.97 0 73.76 -3.74 -3.72 -3.71 -3.66 -3.44 0.02 7.11 12.04 85.23 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.74445 0.39611 9.45302 &lt; 2e-16 boston[[name]] -1.89278 1.50612 -1.25673 0.20943 Residual standard error: 8.597 on 504 degrees of freedom. Multiple \\(R^2\\): 0.003. Adjusted \\(R^2\\): 0.001. F-statistic: 1.579 on 1 and 504, p-value: 0.209. Variable nox Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 87.41 0 52.01 -9.11 -7.35 -6.16 -2.07 -0.25 0.74 1.92 7.88 78.3 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 233.0866 33.6431 6.92821 1.3119e-11 boston[[name]] -1279.3712 170.3975 -7.50816 2.7584e-13 I(boston[[name]]^2) 2248.5440 279.8993 8.03340 6.8113e-15 I(boston[[name]]^3) -1245.7029 149.2816 -8.34465 6.9611e-16 Residual standard error: 7.234 on 502 degrees of freedom. Multiple \\(R^2\\): 0.297. Adjusted \\(R^2\\): 0.293. F-statistic: 70.687 on 3 and 502, p-value: &lt;2e-16. Variable rm Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 105.7 0 68.97 -18.49 -5.13 -4.34 -3.47 -2.22 -0.01 6.82 11.58 87.22 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 112.62460 64.51724 1.74565 0.081483 boston[[name]] -39.15014 31.31149 -1.25034 0.211756 I(boston[[name]]^2) 4.55090 5.00986 0.90839 0.364109 I(boston[[name]]^3) -0.17448 0.26375 -0.66153 0.508575 Residual standard error: 8.33 on 502 degrees of freedom. Multiple \\(R^2\\): 0.068. Adjusted \\(R^2\\): 0.062. F-statistic: 12.168 on 3 and 502, p-value: 1.07e-07. Variable age Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 92.6 0 61.1 -9.76 -7.8 -6.42 -2.67 -0.52 0.02 4.38 8.77 82.84 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.54876 2.76914 -0.92042 0.3577971 boston[[name]] 0.27365 0.18638 1.46826 0.1426608 I(boston[[name]]^2) -0.00723 0.00364 -1.98779 0.0473773 I(boston[[name]]^3) 0.00006 0.00002 2.72373 0.0066799 Residual standard error: 7.84 on 502 degrees of freedom. Multiple \\(R^2\\): 0.174. Adjusted \\(R^2\\): 0.169. F-statistic: 35.306 on 3 and 502, p-value: &lt;2e-16. Variable dis Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 87.14 0 53.43 -10.76 -8.45 -6.51 -2.59 0.03 1.27 3.41 6.72 76.38 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 30.04761 2.44587 12.28504 &lt; 2.22e-16 boston[[name]] -15.55435 1.73597 -8.96005 &lt; 2.22e-16 I(boston[[name]]^2) 2.45207 0.34642 7.07833 4.9412e-12 I(boston[[name]]^3) -0.11860 0.02040 -5.81354 1.0888e-08 Residual standard error: 7.331 on 502 degrees of freedom. Multiple \\(R^2\\): 0.278. Adjusted \\(R^2\\): 0.274. F-statistic: 64.374 on 3 and 502, p-value: &lt;2e-16. Variable rad Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 86.6 0 44.39 -10.38 -7.81 -5.29 -0.41 -0.27 0.18 1.55 3.11 76.22 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.60554 2.05011 -0.29537 0.76783 boston[[name]] 0.51274 1.04360 0.49132 0.62342 I(boston[[name]]^2) -0.07518 0.14854 -0.50610 0.61301 I(boston[[name]]^3) 0.00321 0.00456 0.70311 0.48231 Residual standard error: 6.682 on 502 degrees of freedom. Multiple \\(R^2\\): 0.4. Adjusted \\(R^2\\): 0.396. F-statistic: 111.573 on 3 and 502, p-value: &lt;2e-16. Variable tax Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 90.22 0 46.69 -13.27 -7.34 -5.14 -1.39 0.05 0.54 1.34 3.76 76.95 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 19.18358 11.79555 1.62634 0.10450 boston[[name]] -0.15331 0.09568 -1.60235 0.10971 I(boston[[name]]^2) 0.00036 0.00024 1.48766 0.13747 I(boston[[name]]^3) 0.00000 0.00000 -1.16679 0.24385 Residual standard error: 6.854 on 502 degrees of freedom. Multiple \\(R^2\\): 0.369. Adjusted \\(R^2\\): 0.365. F-statistic: 97.805 on 3 and 502, p-value: &lt;2e-16. Variable ptratio Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 505 89.53 0 65.57 -6.83 -6.3 -5.99 -4.15 -1.65 1.41 4.73 9.51 82.7 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 477.18405 156.79498 3.04336 0.0024621 boston[[name]] -82.36054 27.64394 -2.97933 0.0030287 I(boston[[name]]^2) 4.63535 1.60832 2.88210 0.0041196 I(boston[[name]]^3) -0.08476 0.03090 -2.74328 0.0063005 Residual standard error: 8.122 on 502 degrees of freedom. Multiple \\(R^2\\): 0.114. Adjusted \\(R^2\\): 0.108. F-statistic: 21.484 on 3 and 502, p-value: 4.17e-13. Variable black Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 99.89 0 62.9 -13.1 -4.32 -2.99 -2.34 -2.13 -1.44 5.87 11.82 86.79 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 18.26370 2.30490 7.92385 1.4971e-14 boston[[name]] -0.08356 0.05633 -1.48343 0.13859 I(boston[[name]]^2) 0.00021 0.00030 0.71624 0.47418 I(boston[[name]]^3) 0.00000 0.00000 -0.60777 0.54362 Residual standard error: 7.955 on 502 degrees of freedom. Multiple \\(R^2\\): 0.15. Adjusted \\(R^2\\): 0.145. F-statistic: 29.492 on 3 and 502, p-value: &lt;2e-16. Variable lstat Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 98.59 0 57.86 -15.23 -6.58 -4.93 -2.15 -0.49 0.07 4.03 8.3 83.35 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.20097 2.02865 0.59200 0.554115 boston[[name]] -0.44907 0.46489 -0.96596 0.334530 I(boston[[name]]^2) 0.05578 0.03012 1.85218 0.064587 I(boston[[name]]^3) -0.00086 0.00057 -1.51702 0.129891 Residual standard error: 7.629 on 502 degrees of freedom. Multiple \\(R^2\\): 0.218. Adjusted \\(R^2\\): 0.213. F-statistic: 46.629 on 3 and 502, p-value: &lt;2e-16. Variable medv Formula: boston$crim ~ boston[[name]] + I(boston[[name]]^2) + I(boston[[name]]^3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 506 98.08 0 42.9 -24.43 -6.47 -4.42 -1.98 -0.44 0.44 3.59 7.06 73.65 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 53.16554 3.35631 15.84047 &lt; 2.22e-16 boston[[name]] -5.09483 0.43383 -11.74379 &lt; 2.22e-16 I(boston[[name]]^2) 0.15550 0.01719 9.04552 &lt; 2.22e-16 I(boston[[name]]^3) -0.00149 0.00020 -7.31197 1.0465e-12 Residual standard error: 6.569 on 502 degrees of freedom. Multiple \\(R^2\\): 0.42. Adjusted \\(R^2\\): 0.417. F-statistic: 121.272 on 3 and 502, p-value: &lt;2e-16. Some variable are statistically significative until their cubic version (indus, nox, dis, ptratio and medv). So, we can conclude to some non linear relationship between the response and some of the predictors. However, we should also take a look at the colinearity between the predictors because of them seem to be very correlated, and that can change a lot the results. "],
["classification.html", "Chapter 4 Classification 4.1 Conceptual Exercises 4.2 Applied Exercises", " Chapter 4 Classification 4.1 Conceptual Exercises 4.1.1 Exercise 1. Proof that the logistic function representation and logit representation for the logistic regression model are equivalent. \\[p(x) = \\frac{\\exp(\\beta_0 + \\beta_1x)}{1 - \\exp(\\beta_0 + \\beta_1x)} \\quad\\text{and}\\quad 1 - p(x) = \\frac{1}{1 - \\exp(\\beta_0 + \\beta_1x)}\\] So, \\[\\frac{p(x)}{1 - p(x)} = \\exp(\\beta_0 + \\beta_1x)\\] 4.1.2 Exercise 2. Asume that the observations in the \\(k\\)-th classes are drawn from a \\(\\mathcal{N}(\\mu_k, \\sigma^2)\\)-distribution. Proof that the Bayes classifier assigns an observation to the class for which the discriminant function is maximised. \\[p(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x - \\mu_k\\right)^2\\right)}{\\prod_{l=1}^{K}\\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x - \\mu_l\\right)^2\\right)} = C\\pi_k\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(x - \\mu_k\\right)^2\\right),\\] where \\(C\\) is constant with respect to \\(k\\). By taking the logarithm of \\(p(x)\\), we find: \\[\\log(p(x)) = \\log{C} + \\log{\\pi_k} - \\frac{1}{2\\sigma^2}\\left(x - \\mu_k\\right)^2 = \\log{C} + \\log{\\pi_k} - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} + \\frac{\\mu_k^2}{2\\sigma^2}.\\] So, maximising \\(p(x)\\) with respect to \\(k\\) is equivalent to maximising \\(\\delta_k(x) = \\log{\\pi_k} + \\frac{x\\mu_k}{\\sigma^2} + \\frac{\\mu_k^2}{2\\sigma^2}\\) with respect to \\(k\\). 4.1.3 Exercise 3. Assume there is only one feature which come from an one-dimensional normal distribution. If an observation belongs to the \\(k\\)-th classe, \\(X\\) have a \\(\\mathcal{N}(\\mu_k, \\sigma_k^2)\\) distribution. The probability that \\(X = x\\) belongs to the class \\(k\\) is: \\[ p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\\frac{1}{2\\sigma_k^2}\\left(x - \\mu_k\\right)^2\\right)}{\\prod_{l=1}^{K}\\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma_l^2}}\\exp\\left(-\\frac{1}{2\\sigma_l^2}\\left(x - \\mu_l\\right)^2\\right)}\\] By taking the logarithm of \\(p_k(x)\\), we find: \\[\\log(p(x)) = \\log{C} + \\log{\\pi_k} -\\frac{1}{2}\\log(\\sigma_k) - \\frac{1}{2\\sigma_k^2}\\left(x - \\mu_k\\right)^2.\\] where \\(C\\) is constant with respect to \\(k\\). The Bayes classifier involves assigning \\(X = x\\) to \\(\\arg \\max_k p_k(x)\\) which is equal to \\(\\arg \\max_k \\left(\\log{\\pi_k} -\\frac{1}{2}\\log(\\sigma_k) - \\frac{1}{2\\sigma_k^2}\\left(x - \\mu_k\\right)^2\\right)\\). So, the Bayes classifier is quadratic is this case. 4.1.4 Exercise 4. This exercise is about the curse of dimensionality. Question (a) We aim to predict the response of a new observation \\(X\\) using only observations that are within 10% of the range of \\(X\\) closest to that test observation. As the range of \\(X\\) is \\([0, 1]\\), we only consider observation points that are in the interval \\([X-0.5, X+0.5]\\). On average, 10% of the available observations will be used to make the predictions. Question (b) We aim to do the same but in dimension 2. On average, only 1% of the observations will be used to make the predictions. It corresponds to 10% on hte first axis and 10% on the second axis (\\(1\\% = 10\\% \\times 10\\%\\)). Question (c) Now, we want to do the same but in dimension 100. On average, there are few points within 10% of the range of each direction of \\(X\\). Indeed, there are \\(10\\%^{100}\\) points in this area. Question (d) When we want to consider 10% of the range of each of the \\(p\\) features, the proportion of available data that is used is \\(10\\%^p\\). When \\(p\\) grows, \\(10\\%^p\\) becomes smaller and smaller. So, when \\(p\\) is large, there are very few observations in the \\(10\\%\\) range of the observation. Question (e) We want \\(10\\%\\) observations that are in an hypercube around the new observations. Note \\(l\\) the length of each side of the hypercube. In order to have 10% of the data to predict the response of the new observation, we should have \\(l^p = 0.1\\). So, the length of each side of the hypercube is \\(l = (0.1)^{1/p}\\). If \\(p = 1\\), the length is \\(0.1\\), if \\(p = 2, l = 0.31\\) and if \\(p = 100, l = 0.98\\). Given that the maximum size of the hypercube is \\(1\\), having an hypercube of length \\(0.98\\) to get 10% of the data when \\(p = 100\\) is something very annoying. See The Elements of Statistical Learning, by Hastie, Tibshirani and Friedman for more information. 4.1.5 Exercise 5. This exercise is about the differences between LDA and QDA. Question (a) Assume that the Bayes decision boundary is linear. QDA may be better on the training set in case of overfitting. But, LDA should be better on the test set. Question (b) QDA should be better than LDA on both the training and test sets because it allows more flexibility. Question (c) The test prediction accuracy should increase with the sample size for QDA if the Bayes decision boundary is non-linear. However, usually, more observations mean better knowledge on the true decision boundary. Question (d) QDA is not flexible enough to model linear decision as good as LDA. So, the test arror rate of QDA will not be better than the one of LDA if the Bayes decision boundary is linear. Mathematically speaking, this is because the coefficient in front of the quadratic term in QDA can not null. As exemple, you can take a look at the figure 4.11). 4.1.6 Exercise 6. Question (a) \\[ \\begin{align} \\mathbb{P}(Y = A \\mid X_1 = 40, X_2 = 3.5) &amp;= \\frac{\\exp(-6+0.05\\times 40 + 1 \\times 3.5)}{1 + \\exp(-6+0.05\\times 40 + 1 \\times 3.5)} \\\\&amp;= 0.377 \\end{align} \\] The probability that a student who studies 40 hours and has an undergrad GPA of 3.5 gets an A is 38%. Question (b) \\[ \\begin{align} \\mathbb{P}(Y = A \\mid X_1 = x, X_2 = 3.5) &gt; 0.5 &amp;\\Longleftrightarrow \\frac{\\exp(-6+0.05\\times x + 1 \\times 3.5)}{1 + \\exp(-6+0.05\\times x + 1 \\times 3.5)} &gt; 0.5 \\\\ &amp;\\Longleftrightarrow \\exp(-6+0.05\\times x + 1 \\times 3.5) &gt; 1 \\\\ &amp;\\Longleftrightarrow -6+0.05\\times x + 1 \\times 3.5 &gt; 0 \\\\ &amp;\\Longleftrightarrow x &gt; 50 \\\\ \\end{align} \\] A student with an undergrad GPA of 3.5 needs to studies 50 hours to have 50% chance of getting an A in the class. 4.1.7 Exercise 7. \\[ \\begin{align} \\mathbb{P}(Y = &quot;yes&quot; \\mid X = 4) &amp;= \\frac{0.8f_1(4)}{0.8f_1(4) + 0.2f_2(4)} \\quad\\text{where} f_1 \\sim \\mathcal{N}(10, 36) \\text{ and } f_2 \\sim \\mathcal{N}(0, 36) \\\\&amp;= 0.75 \\end{align} \\] A company with a percentage profit of 4% last year have 75% chance to issue dividend this year. 4.1.8 Exercise 8. The error rates for the logistic regression is quite high. So, it might imply that the Bayes decision boundary is (higly) non-linear. Thus, 1-NN should be prefered for new observations. However, 1-NN usually overfits. It is possible that there is an error of 0% on the training set and 36% on the test sets. So, we can not conclude on which method is better. 4.1.9 Exercise 9. This exercise is about odds. Note \\(p_{def}\\) the default probability. Question (a) \\[ \\frac{p_{def}}{1 - p_{def}} = 0.37 \\Longleftrightarrow 1.37p_{def} = 0.37 \\Longleftrightarrow p_{def} = 0.27%%\\] On average, 27% of people with an odds of 0.37 of defaulting on their credit cards payment will default. Question (b) \\[ odd = \\frac{p_{def}}{1 - p_{def}} = \\frac{0.16}{1 - 0.16} = 0.19 \\] A person with a percentage of changce to default of 16% on their credit card payment has an odd of 0.19. 4.2 Applied Exercises 4.2.1 Exercise 10. This exercise is about fitting a simple logistic model to the Weekly dataset. It contains 1089 observations of 9 variables about the weekly percentage returns for the S&amp;P 500 stock index between 1990 and 2010. For a description of the variables, please refer to R by typing help(Weekly) after loading the package ISLR. weekly &lt;- as_tibble(Weekly, rownames = NA) Question (a) weekly %&gt;% summary_df() %&gt;% print_summary_df() Factor variables Direction Direction Count Down 484 Up 605 Numeric variables Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Year 0 21 20.000000 2000.05 36.40 1990.000000 1991.000000 1992.00000 1995.000000 2000.00000 2005.000000 2008.000000 2009.000000 2010.000000 Lag1 0 1004 30.221000 0.15 5.56 -18.195000 -3.629600 -2.43040 -1.154000 0.24100 1.405000 2.807000 3.737600 12.026000 Lag2 0 1005 30.221000 0.15 5.56 -18.195000 -3.629600 -2.43040 -1.154000 0.24100 1.409000 2.807000 3.737600 12.026000 Lag3 0 1005 30.221000 0.15 5.57 -18.195000 -3.667600 -2.44100 -1.158000 0.24100 1.409000 2.807000 3.737600 12.026000 Lag4 0 1005 30.221000 0.15 5.57 -18.195000 -3.667600 -2.44100 -1.158000 0.23800 1.409000 2.807000 3.737600 12.026000 Lag5 0 1005 30.221000 0.14 5.58 -18.195000 -3.667600 -2.44520 -1.166000 0.23400 1.405000 2.798200 3.737600 12.026000 Volume 0 1089 9.240749 1.57 2.84 0.087465 0.167552 0.19555 0.332022 1.00268 2.053727 4.365844 5.310663 9.328214 Today 0 1003 30.221000 0.15 5.56 -18.195000 -3.629600 -2.43040 -1.154000 0.24100 1.405000 2.807000 3.737600 12.026000 Figure 4.1: Pair plots. We see a strong exponential pattern between the Year and Volume features. Question (b) logit_model &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = weekly, family = &#39;binomial&#39;) logit_model %&gt;% summary() %&gt;% print_summary_glm() Results of the model on the weekly dataset. Formula: Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 1089 3.15 0.04 1.36 -1.69 -1.35 -1.32 -1.26 0.99 1.08 1.14 1.17 1.46 Coefficients Variable Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.26686 0.08593 3.10561 0.0018988 Lag1 -0.04127 0.02641 -1.56261 0.1181444 Lag2 0.05844 0.02686 2.17538 0.0296014 Lag3 -0.01606 0.02666 -0.60238 0.5469239 Lag4 -0.02779 0.02646 -1.05014 0.2936533 Lag5 -0.01447 0.02638 -0.54850 0.5833482 Volume -0.02274 0.03690 -0.61633 0.5376748 Null deviance: 1496.202 on 1088 degrees of freedom. Residual deviance: 1486.357 on 1082 degrees of freedom. AIC: 1500.357 Only one predictor appears to be statistically signicant in this model, the Lag2 feature. Question (c) The function predict.glm gives the probability of the market going up between two weeks. This can be verified using the command contrasts(weekly$Direction). logit_prob &lt;- predict.glm(logit_model, type = &#39;response&#39;) logit_pred &lt;- if_else(logit_prob &gt; .5, &#39;Up&#39;, &#39;Down&#39;) Figure 4.2: Confusion matrix for the complete logistic model. The percentage of correct prediction of the movement of the market is 56.1%. However, if we set Up for avery prediction of the Direction variable, we will have a percentage of correct prediction of 55.6. So, the logistic regression improves very slightly the prediction accuracy. And, as we perform the prediction on the training set, it tends to overestimate the errors rate. The results on unseen data might (and should) be worse. Question (d) mask &lt;- (weekly$Year &gt;= 1990 &amp; weekly$Year &lt;= 2008) train &lt;- weekly[mask,] test &lt;- weekly[!mask,] logit_model &lt;- glm(Direction ~ Lag2, data = train, family = &#39;binomial&#39;) logit_prob &lt;- predict(logit_model, newdata = test, type = &#39;response&#39;) logit_pred &lt;- if_else(logit_prob &gt; .5, &#39;Up&#39;, &#39;Down&#39;) Figure 4.3: Confusion matrix for the logistic model on the test set. The percentage of correct prediction of the movement of the market on the test set is 62.5% for the logistic model. Question (e) lda_model &lt;- lda(Direction ~ Lag2, data = train) lda_prob &lt;- predict(lda_model, test)$posterior[, &#39;Up&#39;] lda_pred &lt;- if_else(lda_prob &gt; .5, &#39;Up&#39;, &#39;Down&#39;) Figure 4.4: Confusion matrix for the LDA model on the test set. The percentage of correct prediction of the movement of the market on the test set is 62.5% for the LDA model. Question (f) qda_model &lt;- qda(Direction ~ Lag2, data = train) qda_prob &lt;- predict(qda_model, test)$posterior[, &#39;Up&#39;] qda_pred &lt;- if_else(qda_prob &gt; .5, &#39;Up&#39;, &#39;Down&#39;) Figure 4.5: Confusion matrix for the QDA model on the test set. The percentage of correct prediction of the movement of the market on the test set is 58.7% for the QDA model. Question (h) knn_model &lt;- knn(train[, &#39;Lag2&#39;], test[, &#39;Lag2&#39;], train$Direction, k = 1) Figure 4.6: Confusion matrix for the \\(K\\)-NN model on the test set. The percentage of correct prediction of the movement of the market on the test set is 50% for the \\(K\\)-NN model with \\(K = 1\\). Question (g) Depending on what we want to emphasize, different methods provide the best results. If we are interested by the total accuracy (the number of good prediction), we should use the logistic model or the LDA model. But, if we want to just be good on the prediction of the Up, we must consider the QDA model (but the results on the Down prediction will be awful…). The \\(K\\)-NN does not provide good results, it really seems to overfit the training data. Question (i) As none of the features (except Lag2) appear to be statistically significant in the logistic model, it seems unlikely that adding a combinaison of these features in the models will lead to better results. However, we can try to change the \\(K\\) in the \\(K\\)-NN algorithm to reduce the overfitting. set.seed(42) errors_rate &lt;- c() k &lt;- seq(1, 20, by = 1) for(i in k){ knn_model &lt;- knn(train[, &#39;Lag2&#39;], test[, &#39;Lag2&#39;], train$Direction, k = i) errors_rate &lt;- c(errors_rate, round(100*mean(knn_model == test$Direction), 1)) } df &lt;- tibble(k = seq(1, 20, by = 1), errors_rate) Figure 4.7: Evolution of percentage of correct predictions compared to \\(K\\). For the \\(K\\)-NN model, we obtain the best results for \\(K\\) between 10 and 17. But, even here, the results are not as good as the ones of the logistic model and the LDA model. 4.2.2 Exercise 11. auto &lt;- as_tibble(Auto) Question (a) auto &lt;- auto %&gt;% mutate(mpg01 = if_else(mpg &gt; median(mpg), 1, 0)) %&gt;% mutate(mpg01 = as_factor(mpg01)) %&gt;% select(-c(mpg, name)) Question (b) Figure 4.8: Pair plots. Based on the pairs plot, four variables seem to be associated with the variable mpg01 and could be used to predict its value. These variables are cylinders, displacement, horsepower and weight. However, some of them appears to be particularly correlated (in particular, displacement with horsepower and weight). So, we have to be careful to not had redundant information in the model. Question (c) set.seed(42) idx &lt;- auto$mpg01 %&gt;% createDataPartition(p = 0.7, list = FALSE, times = 1) train &lt;- auto[idx[,1], ] test &lt;- auto[-idx[,1], ] Question (d) lda_model &lt;- lda(mpg01 ~ cylinders + displacement + horsepower + weight, data = train) lda_prob &lt;- predict(lda_model, test)$posterior[, &#39;1&#39;] lda_pred &lt;- if_else(lda_prob &gt; .5, 1, 0) Figure 4.9: Confusion matrix for the LDA model on the test set. The test error of the LDA model is 0.09. Question (e) qda_model &lt;- qda(mpg01 ~ cylinders + displacement + horsepower + weight, data = train) qda_prob &lt;- predict(qda_model, test)$posterior[, &#39;1&#39;] qda_pred &lt;- if_else(qda_prob &gt; .5, 1, 0) Figure 4.10: Confusion matrix for the QDA model on the test set. The test error of the QDA model is 0.11. Question (f) logit_model &lt;- glm(mpg01 ~ cylinders + displacement + horsepower + weight, data = train, family = &#39;binomial&#39;) logit_prob &lt;- predict(logit_model, newdata = test, type = &#39;response&#39;) logit_pred &lt;- if_else(logit_prob &gt; .5, 1, 0) Figure 4.11: Confusion matrix for the logistic model on the test set. The test error of the logistic model is 0.13. Question (g) set.seed(42) errors_rate &lt;- c() k &lt;- seq(1, 100, by = 1) for(i in k){ knn_model &lt;- knn(train[, c(&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;)], test[, c(&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;)], train$mpg01, k = i) errors_rate &lt;- c(errors_rate, 1 - mean(knn_model == test$mpg01)) } df &lt;- tibble(k = seq(1, 100, by = 1), errors_rate) Figure 4.12: Evolution of percentage of incorrect predictions compared to \\(K\\). The \\(K\\) which give the best results for the \\(K\\)-NN model is \\(K = 3\\). 4.2.3 Exercise 12. Consider to look here. 4.2.4 Exercise 13. boston &lt;- as_tibble(Boston) boston &lt;- boston %&gt;% mutate(crim01 = if_else(crim &gt; median(crim), 1, 0)) %&gt;% mutate(crim01 = as_factor(crim01)) %&gt;% select(-c(crim)) set.seed(42) idx &lt;- boston$crim01 %&gt;% createDataPartition(p = 0.7, list = FALSE, times = 1) train &lt;- boston[idx[,1], ] test &lt;- boston[-idx[,1], ] lda_model &lt;- lda(crim01 ~ ., data = train) lda_prob &lt;- predict(lda_model, test)$posterior[, &#39;1&#39;] lda_pred &lt;- if_else(lda_prob &gt; .5, 1, 0) Figure 3.7: Confusion matrix for the LDA model on the test set. The test error of the LDA model is 0.13. qda_model &lt;- qda(crim01 ~ ., data = train) qda_prob &lt;- predict(qda_model, test)$posterior[, &#39;1&#39;] qda_pred &lt;- if_else(qda_prob &gt; .5, 1, 0) Figure 3.8: Confusion matrix for the QDA model on the test set. The test error of the QDA model is 0.12. logit_model &lt;- glm(crim01 ~ ., data = train, family = &#39;binomial&#39;) logit_prob &lt;- predict(logit_model, newdata = test, type = &#39;response&#39;) logit_pred &lt;- if_else(logit_prob &gt; .5, 1, 0) Figure 4.13: Confusion matrix for the logistic model on the test set. The test error of the logistic model is 0.09. set.seed(42) errors_rate &lt;- c() k &lt;- seq(1, 100, by = 1) for(i in k){ knn_model &lt;- knn(select(train, -c(&#39;crim01&#39;)), select(test, -c(&#39;crim01&#39;)), train$crim01, k = i) errors_rate &lt;- c(errors_rate, 1 - mean(knn_model == test$crim01)) } df &lt;- tibble(k = seq(1, 100, by = 1), errors_rate) Figure 3.10: Evolution of percentage of incorrect predictions compared to \\(K\\). The \\(K\\) which give the best results for the \\(K\\)-NN model is \\(K = 4\\) or \\(K = 5\\). "],
["resampling-methods.html", "Chapter 5 Resampling Methods 5.1 Conceptual Exercises 5.2 Applied Exercises", " Chapter 5 Resampling Methods 5.1 Conceptual Exercises 5.1.1 Exercise 1. Proof that \\(\\alpha = \\frac{\\sigma_Y^2 - \\sigma_{X,Y}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{X,Y}}\\) minimize \\(\\text{Var}(\\alpha X + (1 - \\alpha)Y)\\). Using properties of the variance, we found that: \\[\\text{Var}(\\alpha X + (1 - \\alpha)Y) = \\alpha^2\\sigma_X^2 + (1 - \\alpha)^2\\sigma_Y^2 + 2\\alpha(1 - \\alpha)\\sigma_{X,Y}.\\] As we seek to minimize this quantity with respect to \\(\\alpha\\), we have to set the derivative with respect to \\(\\alpha\\) equal to \\(0\\). \\[\\begin{align} \\frac{d\\text{Var}(\\alpha X + (1 - \\alpha)Y)}{d\\alpha} = 0 &amp;\\Longleftrightarrow 2\\alpha\\sigma_X^2 + 2(1 - \\alpha)\\sigma_Y^2 + 2(1 - 2\\alpha)\\sigma_{X,Y} = 0 \\\\ &amp;\\Longleftrightarrow \\left(\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{X,Y}\\right)\\alpha = \\sigma_Y^2 - \\sigma_{X,Y} \\\\ &amp;\\Longleftrightarrow \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{X,Y}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{X,Y}} \\end{align}\\] 5.1.2 Exercise 2. Suppose that we obtain a bootstrap sample from a set of \\(n\\) observations. Question (a) We draw one observation from a set of \\(n\\) observations. So, the probability of getting a particular observation \\(j\\) is \\(1/n\\). \\[\\mathbb{P}(X_j \\neq S^{(1)}) = 1 - \\mathbb{P}(X_j = S^{(1)}) = 1 - \\frac{1}{n}, \\quad\\text{where}~ S^{(1)} ~\\text{is the first element of the sample.}\\] Question (b) The bootstrap is performed wih replacement. So, the probability of getting a ârticular observation is \\(1/n\\) at each drawing. \\[\\mathbb{P}(X_j \\neq S^{(2)}) = 1 - \\mathbb{P}(X_j = S^{(2)}) = 1 - \\frac{1}{n}, \\quad\\text{where}~ S^{(2)} ~\\text{is the first element of the sample.}\\] Question (c) \\[\\begin{align} \\mathbb{P}(X_j \\notin S) &amp;= \\mathbb{P}(X_j \\neq S^{(1)}, \\dots, X_j \\neq S^{(n)}) \\\\ &amp;= \\mathbb{P}(X_j \\neq S^{(1)}) \\times \\dots \\times \\mathbb{P}(X_j \\neq S^{(n)}) \\\\ &amp;= \\left(1 - \\mathbb{P}(X_j = S^{(1)})\\right) \\times \\dots \\times \\left(1 - \\mathbb{P}(X_j = S^{(n)})\\right) \\\\ &amp;= \\left(1 - \\frac{1}{n}\\right)^n \\end{align}\\] Question (d) \\[\\begin{align} \\mathbb{P}(X_j \\in S) &amp;= \\mathbb{P}(X_j \\notin S)\\\\ &amp;= 1 - \\left(1 - \\frac{1}{5}\\right)^5 \\\\ &amp;= 0.672 \\end{align}\\] Question (e) \\[\\begin{align} \\mathbb{P}(X_j \\in S) &amp;= \\mathbb{P}(X_j \\notin S)\\\\ &amp;= 1 - \\left(1 - \\frac{1}{100}\\right)^{100} \\\\ &amp;= 0.634 \\end{align}\\] Question (f) \\[\\begin{align} \\mathbb{P}(X_j \\in S) &amp;= \\mathbb{P}(X_j \\notin S)\\\\ &amp;= 1 - \\left(1 - \\frac{1}{10000}\\right)^{10000} \\\\ &amp;= 0.632 \\end{align}\\] Question (g) prob &lt;- function(n) return(1 - (1 - (1/n))**n) df &lt;- tibble(n = seq(1, 1000, by = 1), p = prob(n)) Figure 5.1: Probability that a particular observation belongs to the bootstrap sample. We observe that the probability that the \\(j\\)th observation is in the bootstrap samples seems to convergence until \\(0.63\\). Question (h) store &lt;- rep(NA, 1000) for(i in 1:1000) store[i] &lt;- sum(sample(1:100, replace = TRUE) == 4) &gt; 0 This piece of code repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample. In mean, we found that the fourth observation is contain in 66.5% of the samples. So, we retrieve the observation that gave at the previous question. Mathematically, we can be prove like that: \\[\\begin{align} 1 - \\left(1 - \\frac{1}{n}\\right)^n &amp;= 1 - \\exp\\left(n\\ln\\left(1 - \\frac{1}{n}\\right)\\right) \\\\ &amp;\\underset{n \\rightarrow \\infty}{=} 1 - \\exp\\left(n\\left(-\\frac{1}{n} + o\\left(\\frac{1}{n}\\right)\\right)\\right) \\\\ &amp;\\underset{n \\rightarrow \\infty}{\\longrightarrow} 1 - \\exp(-1)) \\approx 0.63 \\end{align}\\] 5.1.3 Exercise 3. Question (a) From page 181 of Introduction to Statistical learning with R: Step 1: Randomly divide the set of observation in \\(k\\) groups; Step 2: Fit the method on \\(k-1\\) groups; Step 3: Compute the MSE on the remaining group; Step 4: Repeat the process \\(k\\) times, considering another test group; Step 5: Estimate the test error by averaging the test errors of each fold. Question (b) On the validation set approach: Advantages: More accurate test error estimation (bias and variance). Disadvantages: Computationnaly less efficient. On the LOOCV: Advantages: Computationnaly more efficient and more accurate test error estimation (variance). Disadvantages: Less accurate test error estimation (bias). 5.1.4 Exercise 4. We can estimate \\(\\widehat{Y}\\) by bootstrap samples. Assume \\(R\\) bootstrap samples. So, we obtain \\(R\\) values for \\(\\widehat{Y}\\): \\(\\widehat{Y}_1, \\dots, \\widehat{Y}_R\\). And \\[ \\bar{\\widehat{Y}} = \\frac{1}{R}\\sum_{i = 1}^R \\widehat{Y}_i \\quad\\text{and}\\quad sd(\\widehat{Y}) = \\left(\\frac{1}{R - 1}\\sum_{i=1}^R\\left(\\widehat{Y}_i - \\bar{\\widehat{Y}}\\right)^2\\right)^{1/2}.\\] 5.2 Applied Exercises 5.2.1 Exercise 5. df &lt;- as_tibble(Default) Question (a) logit_model &lt;- glm(default ~ income + balance, data = df, family = &#39;binomial&#39;) logit_model %&gt;% summary() %&gt;% print_summary_glm() Results of the model on the df dataset. Formula: default ~ income + balance Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 10000 6.2 -0.07 0.15 -2.47 -0.49 -0.31 -0.14 -0.06 -0.02 -0.01 -0.01 3.72 Coefficients Variable Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -11.54047 0.43476 -26.54468 &lt; 2.22e-16 income 0.00002 0.00000 4.17418 2.9906e-05 balance 0.00565 0.00023 24.83628 &lt; 2.22e-16 Null deviance: 2920.65 on 9999 degrees of freedom. Residual deviance: 1578.966 on 9997 degrees of freedom. AIC: 1584.966 Question (b) Question (b.i) Split the sample. set.seed(42) idx &lt;- df$default %&gt;% createDataPartition(p = 0.7, list = FALSE, times = 1) train &lt;- df[idx[,1],] test &lt;- df[-idx[,1],] Question (b.ii) Fit a logistic regression on the train set. logit_model &lt;- glm(default ~ income + balance, data = train, family = &#39;binomial&#39;) Question (b.iii) Prediction on the test set. pred_prob &lt;- predict(logit_model, test, type= &#39;response&#39;) pred &lt;- if_else(pred_prob &gt; 0.5, &#39;Yes&#39;, &#39;No&#39;) Question (b.iv) Compute the validation set error. misclassif &lt;- 1 - mean(test$default == pred) So, there are 2.73% of misclassified on the validation set, which correponds to the validation set error. Question (c) misclassif &lt;- vector(&#39;double&#39;, 3) for(i in 1:3){ set.seed(i) idx &lt;- df$default %&gt;% createDataPartition(p = 0.7, list = FALSE, times = 1) train &lt;- df[idx[,1],] test &lt;- df[-idx[,1],] logit_model &lt;- glm(default ~ income + balance, data = train, family = &#39;binomial&#39;) pred_prob &lt;- predict(logit_model, test, type= &#39;response&#39;) pred &lt;- if_else(pred_prob &gt; 0.5, &#39;Yes&#39;, &#39;No&#39;) misclassif[i] &lt;- 1 - mean(test$default == pred) } The three misclassification error are respectively 2.6%, 2.77% and 2.53% on each on the validation set. These results are quite close to each others. Question (d) set.seed(42) idx &lt;- df$default %&gt;% createDataPartition(p = 0.7, list = FALSE, times = 1) train &lt;- df[idx[,1],] test &lt;- df[-idx[,1],] logit_model &lt;- glm(default ~ income + balance + student, data = train, family = &#39;binomial&#39;) pred_prob &lt;- predict(logit_model, test, type= &#39;response&#39;) pred &lt;- if_else(pred_prob &gt; 0.5, &#39;Yes&#39;, &#39;No&#39;) misclassif &lt;- 1 - mean(test$default == pred) So, there are 2.83% of misclassified on the validation set, which correponds to the validation set error. This result is almost the same as the one of the model without the student feature. So, the inclusion of this feature does not leads to a significant reduction in the test error rate. 5.2.2 Exercise 6. df &lt;- as_tibble(Default) Question (a) logit_model &lt;- glm(default ~ income + balance, data = df, family = &#39;binomial&#39;) logit_model_summary &lt;- logit_model %&gt;% summary() The estimation of the coefficient for the ìncome feature is \\(2.0808976\\times 10^{-5}\\) with a standard error of \\(4.9851672\\times 10^{-6}\\). And the estimation of the coefficient for the balance feature is \\(0.0056471\\) with a standard error of \\(2.2737314\\times 10^{-4}\\). Question (b) boot.fn &lt;- function(df, index){ return(coef(glm(default ~ income + balance, data = df, subset = index, family = &#39;binomial&#39;))) } Question (c) set.seed(42) bootstrap_summary &lt;- boot::boot(data = df, statistic = boot.fn, R = 1000) Using the bootstrap method, the estimation of the coefficient for the income feature is \\(2.0808976\\times 10^{-5}\\) with a standard error of \\(5.0734443\\times 10^{-6}\\). And the estimation of the coefficient for the balance feature is \\(0.0056471\\) with a standard error of \\(2.2991334\\times 10^{-4}\\). Question (d) The estimated standard errors obtained with the glm function are slightly smaller than the ones obtained using the bootstrap method. This is due to the fact that the formula for the standard errors rely on some assumptions, and more particularly on an estimation of the noise variance. As we estimate \\(\\sigma^2\\) using the RSS, we use the linearaty assumption in our model. So, there is probably a non-linear relationship in the data, and so, the residuals from a linear fit will be inflated and so will \\(\\widehat{\\sigma}^2\\). Secondly, the standard formulas assume that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). However, the bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of the coefficients than the summary function. 5.2.3 Exercise 7. weekly &lt;- as_tibble(Weekly) Question (a) logit_model &lt;- glm(Direction ~ Lag1 + Lag2, data = weekly, family = &#39;binomial&#39;) Question (b) logit_model_1 &lt;- glm(Direction ~ Lag1 + Lag2, data = weekly, subset = 2:nrow(weekly), family = &#39;binomial&#39;) Question (c) pred &lt;- predict(logit_model_1, newdata = weekly[1, c(&#39;Lag1&#39;, &#39;Lag2&#39;)], type = &#39;response&#39;) So, the prediction of the direction of the first observation is Up (because \\(\\mathbb{P}(Direction = UP | Lag1, Lag2) = 0.5713923\\)). This observation is incorrectly classify. Question (d) errors &lt;- vector(&#39;logical&#39;, nrow(weekly)) for(i in 1:nrow(weekly)){ model &lt;- glm(Direction ~ Lag1 + Lag2, data = weekly[-i,], family = &#39;binomial&#39;) pred_prob &lt;- predict(model, newdata = weekly[i, c(&#39;Lag1&#39;, &#39;Lag2&#39;)], type = &#39;response&#39;) pred &lt;- ifelse(pred_prob &gt; .5, &#39;Up&#39;, &#39;Down&#39;) errors[i] &lt;- (weekly[i, &#39;Direction&#39;] != pred) } Question (e) loocv_error &lt;- mean(errors) The LOOCV estimate for the test error is 0.4499541. The value of the LOOCV estimate is quite small, a bit under \\(0.5\\). It does indicate that the linear model is not very suitable for this dataset. 5.2.4 Exercise 8. Question (a) set.seed(42) x &lt;- rnorm(100) y &lt;- x - 2*x**2 + rnorm(100) In this dataset, we have \\(n = 100\\) and \\(p = 2\\). The model used to generate the data is: \\[ Y = X - 2X^2 + \\epsilon, \\quad\\text{where}\\quad \\epsilon \\sim \\mathcal{N}(0, 1)\\] Question (b) We see that there is a clear relationship between \\(Y\\) and \\(X\\) which is not linear. Question (c) set.seed(42) cv_errors &lt;- vector(&#39;numeric&#39;, 4) for(i in 1:4){ model &lt;- glm(y ~ poly(x, i), data = df) cv_errors[i] &lt;- cv.glm(df, model)$delta[1] } Model Linear Quadratic Cubic Quadric LOOCV Errors 10.9659305 0.8858076 0.9087144 0.9644779 Question (d) set.seed(1) cv_errors &lt;- vector(&#39;numeric&#39;, 4) for(i in 1:4){ model &lt;- glm(y ~ poly(x, i), data = df) cv_errors[i] &lt;- cv.glm(df, model)$delta[1] } Model Linear Quadratic Cubic Quadric LOOCV Errors 10.9659305 0.8858076 0.9087144 0.9644779 The LOOCV errors are exactly the same when we use another seed because we predict every observation using all the other ones which in fact involve no randomness. Question (e) The model which the smallest LOOCV error is the quadratic model which makes sense because data are generated using this model. Question (f) model &lt;- lm(y ~ poly(x, 4), data = df) model %&gt;% summary() %&gt;% print_summary_lm() Results of the linear model on the df dataset. Formula: y ~ poly(x, 4) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 100 4.54 0 0.8 -1.68 -1.44 -1.13 -0.59 -0.05 0.57 1.2 1.54 2.86 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.20424 0.09144 -24.10521 &lt; 2e-16 poly(x, 4)1 19.51809 0.91443 21.34464 &lt; 2e-16 poly(x, 4)2 -30.12292 0.91443 -32.94189 &lt; 2e-16 poly(x, 4)3 0.27121 0.91443 0.29659 0.76743 poly(x, 4)4 1.15503 0.91443 1.26312 0.20964 Residual standard error: 0.914 on 95 degrees of freedom. Multiple \\(R^2\\): 0.942. Adjusted \\(R^2\\): 0.94. F-statistic: 385.611 on 4 and 95, p-value: &lt;2e-16. The coefficients that are statistically significant are the ones for \\(X\\) and \\(X^2\\) which correspond to the results given by the cross-validation. 5.2.5 Exercise 9. boston &lt;- as_tibble(Boston) Question (a) mu_hat &lt;- mean(boston$medv) The estimate of the mean of medv is 22.5328063. Question (b) se_mu_hat &lt;- sd(boston$medv) / sqrt(nrow(boston)) So, a \\(95\\%\\) confidence interval fot the mean of medv is \\(22.5328063 \\pm 2\\times0.4088611\\). Question (c) mean.fn &lt;- function(data, index) return(mean(data[index])) mu_boot &lt;- boot(boston$medv, statistic = mean.fn, R = 1000) The standard error of \\(\\widehat{\\mu}\\) using the bootstrap is \\(0.4097547\\) which is very close to \\(0.4088611\\) (the one found in question (b)). Question (d) Based on the bootstrap estimate, a \\(95\\%\\) confidence interval for the mean of medv is \\([21.713297, 23.3523157]\\). The one found with t.test(boston$medv) is \\([21.729528, 23.3360846]\\). Both of the intervals are very similar. Question (e) med_hat &lt;- median(boston$medv) The estimate of the median of medv is 21.2. Question (f) median.fn &lt;- function(data, index) return(median(data[index])) median_boot &lt;- boot(boston$medv, statistic = median.fn, R = 1000) The standard error of \\(\\widehat{\\mu}_{med}\\) using the bootstrap is \\(0.3770327\\) . Question (g) quant_hat &lt;- quantile(boston$medv, probs = 0.1) The estimate of the tenth percentile of medv is 12.75. Question (h) quant.fn &lt;- function(data, index) return(quantile(data[index], probs = 0.1)) quant_boot &lt;- boot(boston$medv, statistic = quant.fn, R = 1000) The standard error of \\(\\widehat{\\mu}_{0.1}\\) using the bootstrap is \\(0.4923\\) . "],
["linear-model-selection-and-regularization.html", "Chapter 6 Linear Model Selection and Regularization 6.1 Conceptual Exercises 6.2 Applied Exercises", " Chapter 6 Linear Model Selection and Regularization 6.1 Conceptual Exercises 6.1.1 Exercise 1. Question (a) The model with \\(k\\) predictors which has the smallest training RSS should be the best subset model because this model contains the \\(k\\) predictors which give the smallest training RSS after trying all of the models with \\(k\\) predictors. However, by luck, the model with \\(k\\) predictors selected with forward stepwise or backward stepwise procdedure can be the same than the one from the best subset selection. Question (b) We can not say which model with \\(k\\) predictors has the smallest test RSS because the best subset selection can overfit (it looks god on training data) and forward and backaward selection might not lead to the same model with \\(k\\) predictors. Question (c) The predictors in the \\(k\\)-variable model identified by forward and backward stepwise are a subset of the predictors in the \\((k+1)\\)-variable model identified by forward and backward stepwise selection by definition of the forward and backward stepwise selection procedure (it is the step 2.a of both algorithms). The predictors in the \\(k\\)-variable model identified by backward (resp. forward) stepwise are not a subset of the predictors in the \\((k+1)\\)-variable model identified by forward (resp. backward) stepwise selection because these two procdedure are different. The predictors in the \\(k\\)-variable model identified by best subset are not necessarily a subset of the predictors in the \\((k+1)\\)-variable model identified by best subset selection because for any particular \\(k\\), the best subset selection considers all the models with \\(k\\) predictors, so these model can be very different for two differents \\(k\\). 6.1.2 Exercise 2. Question (a) The lasso, relative to least squares, is less flexible (because we add a constraint) and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance bacause the variance of the lasso estimator will substantially decrease and the bias will slightly increase has the \\(\\lambda\\) increase. Question (b) The ridge regression has the same properties relative to least squares than the lasso (but different constraints). Question (c) Non-linear methods, relative to least squares, are more flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias because non-linear methods tends to have large variance and low bias. 6.1.3 Exercise 3. Suppose we estimate the regression coefficients in a linear regression model by minimizing \\[ \\mid\\mid Y - X\\beta \\mid\\mid^2 \\quad\\text{subject to}\\quad \\mid\\mid \\beta \\mid\\mid_1 \\leq s\\] for a particular value of \\(s\\). This minimization corresponds to the Lasso regression. Question (a) When \\(s\\) increases from \\(0\\), we increase the flexibility of the model. So, there is less constraint with large \\(s\\). And, the training RSS steadily decreases with an increasing of \\(s\\). Question (b) The test RSS will decrease at the beginning when \\(s\\) starts to increase from 0. And then, it will reach a minimum point and finally, restart to increase when the model overfits. Question (c) The fewer the constraints the larger the variance. The variance steadily increases with \\(s\\). Question (d) As the opposite, the fewer the constraints the smaller the bias. The bias steadily decreases with \\(s\\). Question (e) By definition, the irreducible error is irreducible. So, it will remain constant along the change of \\(s\\). 6.1.4 Exercise 4. Suppose we estimate the regression coefficients in a linear regression model by minimizing \\[ \\mid\\mid Y - X\\beta \\mid\\mid^2 + \\lambda\\mid\\mid \\beta \\mid\\mid_2^2\\] for a particular value of \\(s\\). This minimization corresponds to the Ridge regression. Question (a) When \\(\\lambda\\) increases from \\(0\\), we decrease the flexibility of the model. So, there is more constraint with large \\(\\lambda\\). And, the training RSS steadily increases with an increasing of \\(\\lambda\\). Question (b) The test RSS will decrease at the beginning when \\(\\lambda\\) starts to increase from 0. And then, it will reach a minimum point and finally, restart to increase when the model overfits. Question (c) The harder the constraints the smaller the variance. The variance steadily decreases with \\(\\lambda\\). Question (d) As the opposite, the harder the constraints the larger the bias. The bias steadily increases with \\(\\lambda\\). Question (e) By definition, the irreducible error is irreducible. So, it will remain constant along the change of \\(\\lambda\\). 6.1.5 Exercise 5. It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting. Suppose that \\(n = 2\\), \\(p = 2\\), \\(x_{11} = x_{12}\\), \\(x_{21} = x_{22}\\). Furthermore, suppose that \\(y_1 + y_2 = 0\\) and \\(x_{11} + x_{21} = 0\\) and \\(x_{12} + x_{22} = 0\\), so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: \\(\\widehat{β}_0 = 0\\). Question (a) The ridge regression optimization problem is written: \\[ \\arg \\min_{\\beta_1, \\beta_2} \\mathcal{L}(\\beta_1, \\beta_2) = \\arg \\min_{\\beta_1, \\beta_2} \\left(y_1 - \\beta_1x_{11} - \\beta_2x_{12}\\right)^2 + \\left(y_2 - \\beta_1x_{21} - \\beta_2x_{22}\\right)^2 + \\lambda\\left(\\beta_1^2 + \\beta_2^2\\right)\\] Question (b) By taking the partial derivatives to \\(\\beta_1\\) and \\(\\beta_2\\) and setting to \\(0\\). \\[\\frac{\\partial \\mathcal{L}(\\beta_1, \\beta_2)}{\\partial\\beta_1} = 0 \\Leftrightarrow -2x_{11}\\left(y_1 - \\beta_1x_{11} - \\beta_2x_{12}\\right) - 2x_{21}\\left(y_2 - \\beta_1x_{21} - \\beta_2x_{22}\\right) + 2\\lambda\\beta_1 = 0\\] \\[\\frac{\\partial \\mathcal{L}(\\beta_1, \\beta_2)}{\\partial\\beta_2} = 0 \\Leftrightarrow -2x_{12}\\left(y_1 - \\beta_1x_{11} - \\beta_2x_{12}\\right) - 2x_{22}\\left(y_2 - \\beta_1x_{21} - \\beta_2x_{22}\\right) + 2\\lambda\\beta_2 = 0\\] Using \\(x_{11} = x_{12}\\) and \\(x_{22} = x_{21}\\), we found \\[(\\beta_1 + \\beta_2)(x_{11} + x_{21}) + \\lambda\\beta_1 = x_{11}y_1 + x_{21}y_2\\] and \\[(\\beta_1 + \\beta_2)(x_{12} + x_{22}) + \\lambda\\beta_2 = x_{11}y_1 + x_{21}y_2\\] Then, using \\(x_{11} + x_{21} = 0\\) and \\(x_{12} + x_{22} = 0\\), we arrive to \\[\\left\\{ \\begin{array}{r c l} \\lambda\\beta_1 &amp;=&amp; x_{11}y_1 + x_{21}y_2\\\\ \\lambda\\beta_2 &amp;=&amp; x_{11}y_1 + x_{21}y_2 \\end{array} \\right. \\Leftrightarrow \\beta_1 = \\beta_2\\] So, the ridge coefficient estimates satisfy \\(\\widehat{\\beta_1} = \\widehat{\\beta_2}\\). Question (c) The lasso regression optimization problem is written: \\[ \\arg \\min_{\\beta_1, \\beta_2} \\mathcal{L}(\\beta_1, \\beta_2) = \\arg \\min_{\\beta_1, \\beta_2} \\left(y_1 - \\beta_1x_{11} - \\beta_2x_{12}\\right)^2 + \\left(y_2 - \\beta_1x_{21} - \\beta_2x_{22}\\right)^2 + \\lambda\\left(\\mid\\beta_1\\mid + \\mid\\beta_2\\mid\\right)\\] Question (d) By taking the partial derivatives to \\(\\beta_1\\) and \\(\\beta_2\\) and setting to \\(0\\). \\[\\frac{\\partial \\mathcal{L}(\\beta_1, \\beta_2)}{\\partial\\beta_1} = 0 \\Leftrightarrow -2x_{11}\\left(y_1 - \\beta_1x_{11} - \\beta_2x_{12}\\right) - 2x_{21}\\left(y_2 - \\beta_1x_{21} - \\beta_2x_{22}\\right) + \\lambda\\frac{\\mid\\beta_1\\mid}{\\beta_1} = 0\\] \\[\\frac{\\partial \\mathcal{L}(\\beta_1, \\beta_2)}{\\partial\\beta_2} = 0 \\Leftrightarrow -2x_{12}\\left(y_1 - \\beta_1x_{11} - \\beta_2x_{12}\\right) - 2x_{22}\\left(y_2 - \\beta_1x_{21} - \\beta_2x_{22}\\right) + \\lambda\\frac{\\mid\\beta_2\\mid}{\\beta_2} = 0\\] Using \\(x_{11} = x_{12}\\) and \\(x_{22} = x_{21}\\), we found \\[(\\beta_1 + \\beta_2)(x_{11} + x_{21}) + \\lambda\\frac{\\mid\\beta_1\\mid}{2\\beta_1} = x_{11}y_1 + x_{21}y_2\\] and \\[(\\beta_1 + \\beta_2)(x_{12} + x_{22}) + \\lambda\\frac{\\mid\\beta_2\\mid}{2\\beta_2} = x_{11}y_1 + x_{21}y_2\\] Then, using \\(x_{11} + x_{21} = 0\\) and \\(x_{12} + x_{22} = 0\\), we arrive to \\[\\left\\{ \\begin{array}{r c l} \\lambda\\frac{\\mid\\beta_1\\mid}{2\\beta_1} &amp;=&amp; x_{11}y_1 + x_{21}y_2\\\\ \\lambda\\frac{\\mid\\beta_2\\mid}{2\\beta_2} &amp;=&amp; x_{11}y_1 + x_{21}y_2 \\end{array} \\right. \\Leftrightarrow \\frac{\\mid\\beta_1\\mid}{\\beta_1} = \\frac{\\mid\\beta_2\\mid}{\\beta_2}\\] So, the ridge coefficient estimates satisfy \\(\\frac{\\mid\\beta_1\\mid}{\\beta_1} = \\frac{\\mid\\beta_2\\mid}{\\beta_2}\\). So, there are an infinite number of solutions for this equation. The only constraint is that \\(\\beta_1\\) and \\(\\beta_2\\) must have the same sign (and different of \\(0\\)). 6.1.6 Exercise 6. Question (a) The equation (6.12) with \\(p = 1\\) is written: \\[L = \\left(y_1 - \\beta_1\\right)^2 + \\lambda\\beta_1^2\\] y &lt;- 2; lambda &lt;- 10 beta &lt;- seq(-10, 10, 0.1) ridge &lt;- function(beta, y, lambda) return((y - beta)**2 + lambda*beta**2) df &lt;- tibble(beta = beta, L = ridge(beta, y, lambda)) Figure 6.1: The ridge regression optimization problem is solved by (6.14) Question (b) The equation (6.13) with \\(p = 1\\) is written: \\[L = \\left(y_1 - \\beta_1\\right)^2 + \\lambda\\mid\\beta_1\\mid\\] y &lt;- 2; lambda &lt;- 10 beta &lt;- seq(-10, 10, 0.1) ridge &lt;- function(beta, y, lambda) return((y - beta)**2 + lambda*abs(beta)) df &lt;- tibble(beta = beta, L = ridge(beta, y, lambda)) Figure 6.2: The lasso regression optimization problem is solved by (6.15) 6.1.7 Exercise 7. We will now derive the Bayesian connection to the lasso and ridge regression. Question (a) Suppose the regression model: \\(Y = X\\beta + \\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I_n)\\). The likelihood of the data can be written as: \\[\\begin{align*} f(Y \\mid X, \\beta) &amp;= \\prod_{i = 1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(y_i - x_i\\beta\\right)^2\\right)\\\\ &amp;= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n / 2}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2\\right) \\end{align*}\\] Question (b) We have the following prior on \\(\\beta\\): \\(p(\\beta) = \\frac{1}{2b}\\exp(-\\mid\\beta\\mid / b)\\). So, the posterior distribution for \\(\\beta\\) is: \\[\\begin{align*} p(\\beta \\mid Y, X) &amp;\\propto f(Y \\mid X, \\beta)p(\\beta) \\\\ &amp;\\propto \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n / 2}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2\\right)\\frac{1}{2b}\\exp(-\\mid\\beta\\mid / b) \\\\ &amp;\\propto \\frac{1}{2b}\\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n / 2}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2 - \\frac{\\mid\\beta\\mid}{b} \\right) \\end{align*}\\] Question (c) The mode for \\(\\beta\\) under this posterior distribution is the maximum of \\(p(\\beta \\mid, Y, X)\\): \\[\\begin{align*} \\max_\\beta p(\\beta \\mid Y, X) &amp;= \\min_\\beta \\frac{1}{2\\sigma^2}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2 + \\frac{\\mid\\beta\\mid}{b} \\\\ &amp;= \\min_\\beta \\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2 + \\frac{2\\sigma^2}{b}\\mid\\beta\\mid \\end{align*}\\] which is equivalent to the lasso regression optimization function. So, the lasso estimate if the mode for \\(\\beta\\) under this posterior distribution. Question (d) We have the following prior on \\(\\beta\\): \\(p(\\beta) = \\prod_{i = 1}^p \\frac{1}{\\sqrt{2\\pi c}}\\exp(- \\beta_i / 2c)\\). So, the posterior distriubtion for \\(\\beta\\) is: \\[\\begin{align*} p(\\beta \\mid Y, X) &amp;\\propto f(Y \\mid X, \\beta)p(\\beta) \\\\ &amp;\\propto \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n / 2}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2\\right)\\left(\\frac{1}{2\\pi c}\\right)^{p/2}\\exp\\left(- \\frac{1}{2c}\\sum_{i = 1}^p \\beta_i^2\\right) \\\\ &amp;\\propto \\left(\\frac{1}{2\\pi c}\\right)^{p/2}\\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n / 2}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2 - \\frac{1}{2c}\\sum_{i = 1}^p \\beta_i^2\\right) \\end{align*}\\] Question (e) The mode for \\(\\beta\\) under this posterior distribution is the maximum of \\(p(\\beta \\mid, Y, X)\\): \\[\\begin{align*} \\max_\\beta p(\\beta \\mid Y, X) &amp;= \\min_\\beta \\frac{1}{2\\sigma^2}\\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2 + \\frac{1}{2c}\\sum_{i = 1}^p \\beta_i^2 \\\\ &amp;= \\min_\\beta \\sum_{i = 1}^n \\left(y_i - x_i\\beta\\right)^2 + \\frac{\\sigma^2}{b}\\sum_{i = 1}^p \\beta_i^2 \\end{align*}\\] which is equivalent to the ridge regression optimization function. As the posterior distribution is also gaussian, we know that the mode is also the mean. 6.2 Applied Exercises 6.2.1 Exercise 8. Question (a) set.seed(42) n &lt;- 100 X &lt;- rnorm(n, 0, 1) eps &lt;- rnorm(n, 0, 1) Question (b) Y &lt;- 5 + 3 * X + 0.4 * X**2 + X**3 + eps Question (c) X &lt;- as_tibble(poly(X, degree = 10, raw = TRUE)) colnames(X) &lt;- paste0(&quot;$X_{&quot;, colnames(X), &quot;}$&quot;) df &lt;- X %&gt;% add_column(Y, .before = 1) First, we perform best subset selection in order to choose the best model containing the predictors \\(X\\), \\(X^2, \\dots, X^{10}\\). reg_subset &lt;- regsubsets(Y ~ ., data = df, nvmax = 10) reg_subset$xnames &lt;- str_sub(reg_subset$xnames, 2, -2) Figure 2.2: Selected variables for each criteria for best subset selection. Figure 2.3: Best models according to \\(C_p\\), \\(BIC\\) and adjusted \\(R^2\\) for best subset selection. Table 6.1: Coefficients for the best model according to \\(BIC\\). Intercept \\(X_{1}\\) \\(X_{2}\\) \\(X_{5}\\) \\(X_{7}\\) \\(X_{9}\\) 5.00352 3.05965 0.25633 0.81479 -0.17204 0.01039 Table 6.2: Coefficients for the best model according to \\(C_p\\). Intercept \\(X_{1}\\) \\(X_{2}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{7}\\) \\(X_{8}\\) \\(X_{9}\\) 4.86923 3.01089 0.65315 0.89162 -0.09081 -0.21299 0.01459 0.01568 Table 6.3: Coefficients for the best model according to Adjusted \\(R^2\\). Intercept \\(X_{1}\\) \\(X_{2}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{7}\\) \\(X_{8}\\) \\(X_{9}\\) 4.86923 3.01089 0.65315 0.89162 -0.09081 -0.21299 0.01459 0.01568 Question (d) reg_subset &lt;- regsubsets(Y ~ ., data = df, nvmax = 10, method = &#39;forward&#39;) reg_subset$xnames &lt;- str_sub(reg_subset$xnames, 2, -2) Figure 6.3: Selected variables for each criteria for forward selection. Figure 6.4: Best models according to \\(C_p\\), \\(BIC\\) and adjusted \\(R^2\\) for forward selection. Table 6.4: Coefficients for the best model according to \\(BIC\\). Intercept \\(X_{1}\\) \\(X_{2}\\) \\(X_{3}\\) \\(X_{5}\\) \\(X_{7}\\) \\(X_{9}\\) 5.00245 2.95906 0.25966 0.19189 0.72191 -0.15637 0.00955 Table 6.5: Coefficients for the best model according to \\(C_p\\). Intercept \\(X_{1}\\) \\(X_{2}\\) \\(X_{3}\\) \\(X_{5}\\) \\(X_{7}\\) \\(X_{9}\\) 5.00245 2.95906 0.25966 0.19189 0.72191 -0.15637 0.00955 Table 6.6: Coefficients for the best model according to Adjusted \\(R^2\\). Intercept \\(X_{1}\\) \\(X_{2}\\) \\(X_{3}\\) \\(X_{4}\\) \\(X_{5}\\) \\(X_{7}\\) \\(X_{9}\\) \\(X_{10}\\) 4.85097 3.02558 0.81393 -0.04738 -0.21319 0.92544 -0.2203 0.01611 0.00076 reg_subset &lt;- regsubsets(Y ~ ., data = df, nvmax = 10, method = &#39;backward&#39;) reg_subset$xnames &lt;- str_sub(reg_subset$xnames, 2, -2) Figure 6.5: Selected variables for each criteria for backward selection. Figure 6.6: Best models according to \\(C_p\\), \\(BIC\\) and adjusted \\(R^2\\) for backward selection. Table 6.7: Coefficients for the best model according to \\(BIC\\). Intercept \\(X_{1}\\) \\(X_{4}\\) \\(X_{5}\\) \\(X_{7}\\) \\(X_{9}\\) 5.13379 3.07276 0.04595 0.8142 -0.17212 0.01048 Table 6.8: Coefficients for the best model according to \\(C_p\\). Intercept \\(X_{1}\\) \\(X_{4}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{7}\\) \\(X_{8}\\) \\(X_{9}\\) 4.98423 2.96772 0.5976 0.96214 -0.2685 -0.24729 0.03118 0.01974 Table 6.9: Coefficients for the best model according to Adjusted \\(R^2\\). Intercept \\(X_{1}\\) \\(X_{4}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{7}\\) \\(X_{8}\\) \\(X_{9}\\) 4.98423 2.96772 0.5976 0.96214 -0.2685 -0.24729 0.03118 0.01974 Question (e) set.seed(42) lasso_cv &lt;- cv.glmnet(as.matrix(df[-1]), df$Y, alpha = 1) Figure 6.7: Cross validation error as a function of \\(\\lambda\\). lasso_model &lt;- glmnet(as.matrix(df[-1]), df$Y, alpha = 1) lasso_coef &lt;- predict(lasso_model, type = &#39;coefficients&#39;, s = lasso_cv$lambda.min) Table 6.10: Coefficients for the model fitted by lasso. (Intercept) \\(X_{1}\\) \\(X_{2}\\) \\(X_{3}\\) \\(X_{4}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{7}\\) \\(X_{8}\\) \\(X_{9}\\) \\(X_{10}\\) 5.10693 3.06612 0.18405 0.91642 0 0 0 0 0 0 0 The lasso gives good coefficients estimation (and set to 0 all the coefficient that are not significant in the model). We can do model selection using the lasso. Question (f) set.seed(42) Y &lt;- 5 + 0.2 * pull(X, 7) + eps df &lt;- X %&gt;% add_column(Y, .before = 1) First, we perform best subset selection for the \\(Y\\) vector with the \\(X\\) matrix as features. reg_subset &lt;- regsubsets(Y ~ ., data = df, nvmax = 10) reg_subset$xnames &lt;- str_sub(reg_subset$xnames, 2, -2) Figure 6.8: Selected variables for each criteria for best subset selection. Figure 6.9: Best models according to \\(C_p\\), \\(BIC\\) and adjusted \\(R^2\\) for best subset selection. Table 6.11: Coefficients for the best model according to \\(BIC\\). Intercept \\(X_{3}\\) \\(X_{4}\\) \\(X_{5}\\) \\(X_{9}\\) 4.95872 -1.1237 -0.03994 0.9405 0.01203 Table 6.12: Coefficients for the best model according to \\(C_p\\). Intercept \\(X_{3}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{9}\\) \\(X_{10}\\) 4.96071 -0.90561 0.8318 -0.02629 0.0146 0.00078 Table 6.13: Coefficients for the best model according to Adjusted \\(R^2\\). Intercept \\(X_{2}\\) \\(X_{3}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{8}\\) \\(X_{9}\\) 4.87656 0.22826 -0.91874 0.83297 -0.085 0.01372 0.01476 And now, we fit the lasso model on the data. set.seed(42) lasso_cv &lt;- cv.glmnet(as.matrix(df[-1]), df$Y, alpha = 1) Figure 6.10: Cross validation error as a function of \\(\\lambda\\). lasso_model &lt;- glmnet(as.matrix(df[-1]), df$Y, alpha = 1) lasso_coef &lt;- predict(lasso_model, type = &#39;coefficients&#39;, s = lasso_cv$lambda.min) Table 6.14: Coefficients for the model fitted by lasso. (Intercept) \\(X_{1}\\) \\(X_{2}\\) \\(X_{3}\\) \\(X_{4}\\) \\(X_{5}\\) \\(X_{6}\\) \\(X_{7}\\) \\(X_{8}\\) \\(X_{9}\\) \\(X_{10}\\) 4.73572 0 0 0 0 0.00427 0 0.19456 0 0 0 The results obtained with the best subset selection are very poor compared to the ones obtained with the lasso. Indeed, we can not recover the true underlying model using the best subset selection. At the opposite, the model fit with the lasso is almost perfect. A large part of the coefficients is set to \\(0\\) except the one of interest \\(\\beta_7\\) (and another \\(\\beta_5\\), but very close to \\(0\\)). 6.2.2 Exercise 9. In this exercise, we will predict the number of applications received (Apps variable) using the other variables in the College dataset. Question (a) set.seed(42) df &lt;- as_tibble(College[-1]) idx &lt;- df$Apps %&gt;% createDataPartition(p = 0.7, list = FALSE, times = 1) train &lt;- slice(df, idx[,1]); Y_train &lt;- as.vector(train$Apps); X_train &lt;- as.matrix(select(train, -Apps)) test &lt;- slice(df, -idx[,1]); Y_test &lt;- as.vector(test$Apps); X_test &lt;- as.matrix(select(test, -Apps)) Question (b) lm_model &lt;- lm(Apps ~ ., data = train) pred_lm &lt;- predict.lm(lm_model, newdata = test) The mean squared error obtained using the linear model is \\(1.4251116\\times 10^{6}\\). Question (c) cv.out &lt;- cv.glmnet(X_train, Y_train, alpha = 0) ridge_mod &lt;- glmnet(X_train, Y_train, alpha = 0, lambda = cv.out$lambda.min) pred_ridge &lt;- predict(ridge_mod, newx = X_test) The mean squared error obtained using the ridge regression model is \\(1.3541858\\times 10^{6}\\). Question (d) cv.out &lt;- cv.glmnet(X_train, Y_train, alpha = 1) lasso_mod &lt;- glmnet(X_train, Y_train, alpha = 1, lambda = cv.out$lambda.min) pred_lasso &lt;- predict(lasso_mod, newx = X_test) The mean squared error obtained using the ridge regression model is \\(1.4228908\\times 10^{6}\\). The number of non-zero coefficients is 14. Question (e) pcr_mod &lt;- pcr(Apps ~ ., data = train, scale = TRUE, validation = &#39;CV&#39;) pred_pcr &lt;- predict(pcr_mod, test, ncomp = which.min(pcr_mod$validation$adj)) The mean squared error obtained using the PCR model is \\(1.4251116\\times 10^{6}\\). The number of components \\(M\\) selected with cross-validation is 16. Question (f) pls_mod &lt;- plsr(Apps ~ ., data = train, scale = TRUE, validation = &#39;CV&#39;) pred_pls &lt;- predict(pls_mod, test, ncomp = which.min(pls_mod$validation$adj)) The mean squared error obtained using the PCR model is \\(1.4230893\\times 10^{6}\\). The number of components \\(M\\) selected with cross-validation is 13. Question (g) Let’s compute the \\(R^2\\) for each model in order to compare them. \\(R^2\\) for all the models Model \\(R^2\\) Linear 0.8984327 Ridge 0.9034876 Lasso 0.898591 PCR 0.8984327 PLS 0.8985768 So, all the model are quite comparable with a \\(R^2\\) around \\(0.9\\). None of them performs really better than the others. And as the \\(R^2\\) is quite high, all of them are quite accurate in this case and fit the data pretty well. All the tests arrors are very similar. 6.2.3 Exercise 10. We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated dataset. Question (a) set.seed(42) n &lt;- 1000; p &lt;- 20 X &lt;- as.data.frame(matrix(rnorm(n * p), nrow = n)) beta &lt;- runif(20, min = 0.5, max = 1) beta[sample(1:length(beta), 5)] &lt;- 0 Y &lt;- as.matrix(X) %*% beta + rnorm(n) df &lt;- data.frame(Y, X) Question (b) idx &lt;- sample(1:length(Y), size = 100) train &lt;- df[idx,] test &lt;- df[-idx,] Question (c) Let’s perform best subset selection on the train set. regfit_subset &lt;- regsubsets(Y ~ ., data = train, nvmax = 20) model_matrix &lt;- model.matrix(Y ~ ., data = train) val_errors_train &lt;- rep(NA, 20) for (i in 1:20) { coefi &lt;- coef(regfit_subset, id = i) pred &lt;- model_matrix[, names(coefi)] %*% coefi val_errors_train[i] &lt;- mean((train$Y - pred)**2) } Figure 6.11: Train MSE associated with the best model of each size. Question (d) Let’s do the same thing on the test set. regfit_subset &lt;- regsubsets(Y ~ ., data = train, nvmax = 20) model_matrix &lt;- model.matrix(Y ~ ., data = test) val_errors_test &lt;- rep(NA, 20) for (i in 1:20) { coefi &lt;- coef(regfit_subset, id = i) pred &lt;- model_matrix[, names(coefi)] %*% coefi val_errors_test[i] &lt;- mean((test$Y - pred)**2) } Figure 6.12: Test MSE associated with the best model of each size. Question (e) The minimum value of the test MSE is 1.2052158. This minimum is reach for a model of size 15. Question (f) The model at which the test set MSE is minimized as the same number of coefficients than the true model. Moreover, all the coefficients that are set to \\(0\\) in the generating process are equal to \\(0\\) in the model with the minimum test MSE. But, the coefficients are not to close to true ones (but this is probably due to the generating process). Question (g) beta &lt;- c(0, beta) names(beta) &lt;- names(coef(regfit_subset, id = 20)) beta_errors &lt;- rep(NA, 20) for (i in 1:20) { coefi &lt;- coef(regfit_subset, id = i) b &lt;- merge(data.frame(beta=names(beta), beta), data.frame(beta=names(coefi),coefi), all.x = TRUE) b[is.na(b)] &lt;- 0 beta_errors[i] &lt;- sqrt(sum((b[,2] - b[,3])**2)) } Figure 6.13: Errors on the coefficients compared to the size of the model. The \\(\\sqrt{\\sum_{j=1}^p (\\beta_j - \\hat{\\beta}_j^r)^2}\\) curve looks quite the same than the test MSE. However, the minimum is not reach to the same model size. 6.2.4 Exercise 11. We will now try to predict per capita crime rate in the Bostondata set. # Load data df &lt;- as_tibble(Boston) # Split intot train and test set set.seed(42) idx &lt;- sample(1:nrow(df), size = 100) train &lt;- df[idx,]; model_train &lt;- model.matrix(crim ~ ., data = train) test &lt;- df[-idx,]; model_test &lt;- model.matrix(crim ~ ., data = test) Question (a) Let’s start by best subset selection. regfit_subset &lt;- regsubsets(crim ~ ., data = train, nvmax = ncol(train) - 1) Figure 6.14: Best models according to \\(BIC\\) for best subset selection. So, it appears that the best model according to the best subset selection model has two variables. best_coeff &lt;- coef(regfit_subset, id = 2) pred_best &lt;- model_test[, names(best_coeff)] %*% best_coeff Then, let’s perform lasso regression. set.seed(42) lasso_cv &lt;- cv.glmnet(x = as.matrix(train[,2:ncol(train)]), y = train[[&#39;crim&#39;]], alpha = 1) lasso_mod &lt;- glmnet(as.matrix(train[,2:ncol(train)]), train[[&#39;crim&#39;]], alpha = 1, lambda = lasso_cv$lambda.min) pred_lasso &lt;- predict(lasso_mod, newx = as.matrix(test[,2:ncol(test)])) Another one is the ridge regression. set.seed(42) ridge_cv &lt;- cv.glmnet(x = as.matrix(train[,2:ncol(train)]), y = train[[&#39;crim&#39;]], alpha = 0) ridge_mod &lt;- glmnet(as.matrix(train[,2:ncol(train)]), train[[&#39;crim&#39;]], alpha = 0, lambda = ridge_cv$lambda.min) pred_ridge &lt;- predict(ridge_mod, newx = as.matrix(test[,2:ncol(test)])) And finally, the PCR model. set.seed(42) pcr_mod &lt;- pcr(crim ~ ., data = train, scale = TRUE, validation = &#39;CV&#39;) pred_pcr &lt;- predict(pcr_mod, test[,2:ncol(test)], ncomp = 7) Question (b) We are going to compare the different models using the MSE on the test set. mse_best &lt;- mean((pred_best - test$crim)**2) mse_lasso &lt;- mean((pred_lasso - test$crim)**2) mse_ridge &lt;- mean((pred_ridge - test$crim)**2) mse_pcr &lt;- mean((pred_pcr - test$crim)**2) MSE for all the models Model MSE Best subset 41.8357193 Ridge 42.4799642 Lasso 40.5281645 PCR 45.7516391 So, it appears that the lasso has the smallest MSE on the test set. Thus, we choose the lasso model as the final model. Question (c) Our model does not involve all the features because the lasso model shrinks some coefficients towards 0. predict(lasso_mod, type = &#39;coefficient&#39;) ## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) -2.57705466 ## zn . ## indus . ## chas . ## nox . ## rm . ## age . ## dis . ## rad 0.46978606 ## tax . ## ptratio . ## black . ## lstat 0.25150257 ## medv -0.06290058 "],
["moving-beyond-linearity.html", "Chapter 7 Moving Beyond Linearity 7.1 Conceptual exercises 7.2 Applied exercises", " Chapter 7 Moving Beyond Linearity 7.1 Conceptual exercises 7.1.1 Exercise 1. A cubic regression spline with one knot at \\(\\xi\\) can be obtained using a basis of the form \\(x, x^2, x^3, (x - \\xi)_+^3\\), where \\((x - \\xi)_+^3 = (x - \\xi)^3\\) if \\(x &gt; \\xi\\) and equals \\(0\\) otherwise. We will now show that a function of the form \\[ f(x) = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\beta_4(x - \\xi)_+^3 \\] is indeed a cubic regression spline, regardless of the values of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\) and \\(\\beta_4\\). Question (a) Let’s define a function \\[ f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3, \\quad\\text{for some real numbers}\\quad a_1, b_1, c_1, d_1. \\] For all \\(x \\leq \\xi\\), \\((x - \\xi)_+^3 = 0\\), and so we have \\(f(x) = f_1(x)\\) for all \\(x \\leq \\xi\\). And \\(a_1 = \\beta_0\\), \\(b_1 = \\beta_1\\), \\(c_1 = \\beta_2\\) and \\(d_1 = \\beta_3\\). Question (b) Let’s define a function \\[ f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3, \\quad\\text{for some real numbers}\\quad a_2, b_2, c_2, d_2. \\] For all \\(x &gt; \\xi\\), \\((x - \\xi)_+^3 = (x - \\xi)^3 = x^3 - 3\\xi x^2 + 3\\xi^2 x - \\xi^3\\). Thus, we can rewrite \\(f(x)\\) as: \\[ f(x) = (\\beta_0 - \\beta_4\\xi^3) + (\\beta_1 + 3\\beta_4\\xi^2)x + (\\beta_2 - 3\\beta_4\\xi)x^2 + (\\beta_3 + \\beta_4)x^3.\\] Then, by identification, we found that \\(a_2 = \\beta_0 - \\beta_4\\xi^3\\), \\(b_2 = \\beta_1 + 3\\beta_4\\xi^2\\), \\(c_2 = \\beta_2 - 3\\beta_4\\xi\\) and \\(d_2 = \\beta_3 + \\beta_4\\). Question (c) We have that: \\[\\begin{align*} f_2(\\xi) &amp;= (\\beta_0 - \\beta_4\\xi^3) + (\\beta_1 + 3\\beta_4\\xi^2)\\xi + (\\beta_2 - 3\\beta_4\\xi)\\xi^2 + (\\beta_3 + \\beta_4)\\xi^3 \\\\ &amp;= \\beta_0 + \\beta_1\\xi + \\beta_2\\xi^2 + \\beta_3\\xi^3 + (-\\beta_4\\xi^3 + 3\\beta_4\\xi^3 - 3\\beta_4\\xi^3 + \\beta_4\\xi^3) \\\\ &amp;= f_1(\\xi) \\end{align*}\\] So, \\(f(x)\\) is continous at \\(\\xi\\). Question (d) By taking the first derivatives: \\[f_1^\\prime(x) = \\beta_1 + 2\\beta_2x + 3\\beta_3x^2 \\quad\\text{and}\\quad f_2^\\prime(x) = (\\beta_1 + 3\\beta_4\\xi^2) + 2(\\beta_2 - 3\\beta_4\\xi)x + 3(\\beta_3 + \\beta_4)x^2.\\] And so, \\[\\begin{align*} f_2^\\prime(\\xi) &amp;= (\\beta_1 + 3\\beta_4\\xi^2) + 2(\\beta_2 - 3\\beta_4\\xi)\\xi + 3(\\beta_3 + \\beta_4)\\xi^2 \\\\ &amp;= \\beta_1 + 2\\beta_2\\xi + 3\\beta_3\\xi^2 + (3\\beta_4\\xi^2 - 6\\beta_4\\xi^2 + 3\\beta_4\\xi^2) \\\\ &amp;= f_1^\\prime(\\xi) \\end{align*}\\] So, \\(f^\\prime(x)\\) is continuous at \\(\\xi\\). Question (e) By taking the second derivatives: \\[f_1^{\\prime\\prime}(x) = 2\\beta_2 + 6\\beta_3x \\quad\\text{and}\\quad f_2^{\\prime\\prime}(x) = 2(\\beta_2 - 3\\beta_4\\xi) + 6(\\beta_3 + \\beta_4)x.\\] And so, \\[\\begin{align*} f_2^{\\prime\\prime}(\\xi) &amp;= 2(\\beta_2 - 3\\beta_4\\xi) + 6(\\beta_3 + \\beta_4)\\xi \\\\ &amp;= 2\\beta_2 + 6\\beta_3\\xi - 6\\beta_4\\xi + 6\\beta_4\\xi \\\\ &amp;= f_1^{\\prime\\prime}(\\xi) \\end{align*}\\] So, \\(f^{\\prime\\prime}(x)\\) is continuous at \\(\\xi\\). Therefore, \\(f(x)\\) is indeed a cubic sline. 7.1.2 Exercise 2. Suppose that a curve \\(\\widehat{g}\\) is computed to smoothly fit a set of \\(n\\) points using the following formula: \\[ \\widehat{g} = \\arg\\min_g \\left(\\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2 + \\lambda\\int \\left[g^{(m)}(x)\\right]^2dx\\right),\\] where \\(g^{(m)}\\) represents the \\(m\\)th derivatives of \\(g\\) (and \\(g^{(0)} = g\\)). Question (a) When \\(\\lambda = \\infty\\) and \\(m = 0\\), \\(\\widehat{g}\\) will be perfectly smooth and so \\(\\int \\left[g^{(0)}(x)\\right]^2dx\\) should be very close to \\(0\\). Thus, we find that \\(\\widehat{g}\\) will be equal to \\(0\\). Question (b) When \\(\\lambda = \\infty\\) and \\(m = 1\\), \\(\\widehat{g}\\) will be perfectly smooth and so \\(\\int \\left[g^{(1)}(x)\\right]^2dx\\) should be very close to \\(0\\). Thus, we find that \\(\\widehat{g}\\) will be a constant function (of the form \\(\\widehat{g}(x) = k\\)). Question (c) When \\(\\lambda = \\infty\\) and \\(m = 2\\), \\(\\widehat{g}\\) will be perfectly smooth and so \\(\\int \\left[g^{(2)}(x)\\right]^2dx\\) should be very close to \\(0\\). Thus, we find that \\(\\widehat{g}\\) will be a linear function (of the form \\(\\widehat{g}(x) = ax + b\\)). Question (d) When \\(\\lambda = \\infty\\) and \\(m = 3\\), \\(\\widehat{g}\\) will be perfectly smooth and so \\(\\int \\left[g^{(3)}(x)\\right]^2dx\\) should be very close to \\(0\\). Thus, we find that \\(\\widehat{g}\\) will be equal a quadratic function (of the form \\(\\widehat{g}(x) = ax^2 + bx + c\\)). Question (e) When \\(\\lambda = 0\\) and \\(m = 3\\), we do not put any constraints on \\(g(x_i)\\), then \\(\\widehat{g}\\) will be such that it interpolates all of the \\(y_i\\). 7.1.3 Exercise 3. Suppose we fit a curve with basis functions: \\[\\begin{align*} b_1(X) &amp;= X \\\\ b_2(X) &amp;= (X - 1)^2\\mathbf{1}(X \\geq 1) \\end{align*}\\] We fit the linear regression model \\[ Y = \\beta_0 + \\beta_1b_1(X) + \\beta_2b_2(X) + \\epsilon,\\] and obtain coefficient estimates \\(\\widehat{\\beta}_0 = 1, \\widehat{\\beta}_1 = 1\\) and \\(\\widehat{\\beta}_2 = -2\\). X &lt;- seq(-2, 2, 0.1) Y &lt;- 1 + X - 2 * (X - 1)**2 * (X &gt; 1) Figure 7.1: Sketch of the estimated curve between \\(X = -2\\) and \\(X = 2\\) 7.1.4 Exercise 4. Suppose we fit a curve with basis functions \\[\\begin{align*} b_1(X) &amp;= \\mathbf{1}(0 \\leq X \\leq 2) - (X - 1)\\mathbf{1}(1 \\leq 2) \\\\ b_2(X) &amp;= (X - 3)\\mathbf{1}(3 \\leq X \\leq 4) + \\mathbf{1}(4 &lt; X \\leq 5) \\end{align*}\\] We fit the linear regression model \\[ Y = \\beta_0 + \\beta_1b_1(X) + \\beta_2b_2(X) + \\epsilon,\\] and obtain coefficient estimates \\(\\widehat{\\beta}_0 = 1, \\widehat{\\beta}_1 = 1\\) and \\(\\widehat{\\beta}_2 = 3\\). b1 &lt;- function(x) (x &gt;= 0 &amp; x &lt;= 2) - (x - 1) * (x &gt;= 1 &amp; x &lt;= 2) b2 &lt;- function(x) (x - 3) * (x &gt;= 3 &amp; x &lt;= 4) + (x &gt; 4 &amp; x &lt;= 5) X &lt;- seq(-5, 5, 0.1) Y &lt;- 1 + b1(X) + 3 * b2(X) Figure 7.2: Sketch of the estimated curve between \\(X = -5\\) and \\(X = 5\\) 7.1.5 Exercise 5. Consider two curves, \\(\\widehat{g}_1\\) and \\(\\widehat{g}_2\\), defined by \\[\\widehat{g}_1 = \\arg\\min_g \\left(\\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2 + \\lambda\\int \\left[g^{(3)}(x)\\right]^2dx\\right),\\] \\[\\widehat{g}_2 = \\arg\\min_g \\left(\\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2 + \\lambda\\int \\left[g^{(4)}(x)\\right]^2dx\\right)\\] where \\(g^{(m)}\\) represents the \\(m\\)th derivative of \\(g\\). Question (a) As \\(\\lambda \\rightarrow \\infty\\), \\(\\widehat{g}_2\\) will have the smaller training RSS because \\(\\widehat{g}_2\\) should be more flexible than \\(\\widehat{g}_1\\) (less constraints), so it should better fit the training data and lead to a smaller training RSS. Question (b) As \\(\\lambda \\rightarrow \\infty\\), we can not say which function will have the smaller test RSS because it depends on the true underlying function \\(g\\). If the true function \\(g\\) is a polynomial function with degree at most 2, then \\(\\widehat{g}_1\\) will likely have the smaller test RSS (because \\(\\widehat{g}_2\\) will overfit). Once, the true function is a polynomial function with degree larger than 3, it should be the other way round. Question (c) For \\(\\lambda = 0\\), \\(\\widehat{g}_1\\) and \\(\\widehat{g}_2\\) become the same function, so they will have the same training and test RSS. 7.2 Applied exercises 7.2.1 Exercise 6. wage &lt;- as_tibble(Wage) X &lt;- wage$age; Y &lt;- wage$wage Question (a) We aim to predict wage using age with polynomial regression of degree \\(d\\). The degree will be chosen by cross-validation. fits &lt;- list() cv_error &lt;- vector(length = 10) set.seed(42) for(d in 1:10){ fits[d] &lt;- list(lm(wage ~ poly(age, d), data = wage)) fit &lt;- glm(wage ~ poly(age, d), data = wage) cv_error[d] &lt;- cv.glm(wage, fit, K = 10)$delta[2] } Figure 7.3: Cross-validation errors according to the considered degree. So, according to cross-validation, the best degree to fit the wage is 9. Now, we will compare the chosen degree with the one obtained by hypothesis testing using ANOVA. ANOVA tests the null hypothesis that a model \\(\\mathcal{M}_1\\) is sufficient to explain the data against the alternative hypothesis that a more complex model \\(\\mathcal{M}_2\\) is required. Careful, \\(\\mathcal{M}_1\\) and \\(\\mathcal{M}_2\\) must be nested. do.call(anova, fits) ## Analysis of Variance Table ## ## Model 1: wage ~ poly(age, d) ## Model 2: wage ~ poly(age, d) ## Model 3: wage ~ poly(age, d) ## Model 4: wage ~ poly(age, d) ## Model 5: wage ~ poly(age, d) ## Model 6: wage ~ poly(age, d) ## Model 7: wage ~ poly(age, d) ## Model 8: wage ~ poly(age, d) ## Model 9: wage ~ poly(age, d) ## Model 10: wage ~ poly(age, d) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 2998 5022216 ## 2 2997 4793430 1 228786 143.7638 &lt; 2.2e-16 *** ## 3 2996 4777674 1 15756 9.9005 0.001669 ** ## 4 2995 4771604 1 6070 3.8143 0.050909 . ## 5 2994 4770322 1 1283 0.8059 0.369398 ## 6 2993 4766389 1 3932 2.4709 0.116074 ## 7 2992 4763834 1 2555 1.6057 0.205199 ## 8 2991 4763707 1 127 0.0796 0.777865 ## 9 2990 4756703 1 7004 4.4014 0.035994 * ## 10 2989 4756701 1 3 0.0017 0.967529 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Then, the results are the same than the ones from the book (p. 270). The p-value comparing the linear model to the quadratic model is essentially zero (\\(&lt; 10^{-15}\\)), indicating that a linear fit is not sufficient. Similarly the p-value comparing the quadratic model to the cubic model is very low, so the quadratic fit is also insufficient. The p-value comparing the cubic and the quadric models is approximately \\(5\\%\\) while the degree-5 polynomial seems unnecessary because its high p-value. Hence, either a cubic or a quartic polynomial appear to provide a reasonable fit to the data, but lower- or higher-order models are not justified. These results are quite similar to the cross-validation ones. Even if the cubic or quartic polynomial have not the smallest CV errors, they are quite close to the best. Finally, let’s fit the data with a degree-4 polynomial and plot it. fit_final &lt;- glm(wage ~ poly(age, 4), data = wage) pred &lt;- predict(fit_final, newdata = list(age = X), se.fit = TRUE) Figure 7.4: Quadric polynomial model. Question (b) Let’s fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. cv_error &lt;- vector() set.seed(42) for(d in 2:15){ wage$age_cut &lt;- cut(wage$age, d) fit &lt;- glm(wage ~ age_cut, data = wage) cv_error[d-1] &lt;- cv.glm(wage, fit, K = 10)$delta[2] } Figure 7.5: Cross-validation errors according to the considered number of cuts. So, according to cross-validation, the best number of cuts to fit the wage is 11. Finally, let’s fit the data with 11 cuts for the age and plot it. wage$age_cut &lt;- cut(wage$age, which.min(cv_error) + 1) fit_final &lt;- glm(wage ~ age_cut, data = wage) pred &lt;- predict(fit_final, newdata = list(age_cut = cut(X, which.min(cv_error) + 1)), se.fit = TRUE) Figure 7.6: Step model. 7.2.2 Exercise 7. wage &lt;- as_tibble(Wage) X &lt;- wage$age; Y &lt;- wage$wage Let’s explore relationships between the different variables in the wage dataset. For that, first, we are going to compute the correlation between the features and the ouput wage. Figure 7.7: Correlation plot. fit_gam &lt;- gam(wage ~ ns(age, 5) + maritl + race + education + jobclass + health, data = wage) Figure 7.8: Results of the GAM. The results are pretty intuitive. With other variables being fixed, wage tends to be highest for intermediate values of age, and lowest for the very young and the very old. With other variables being fixed, wage tends to increase with education. The more educated a person is, the higher their salary, on average. Same, people with very good health have a higher wage than the one with less good health. Moreover, information jobs are more paid than industrial ones. Finally, white and married people seems to earn more money. 7.2.3 Exercise 8. auto &lt;- as_tibble(Auto) Let’s explore relationship between the different variables in the wage dataset. For that, first, we are going to compute the correlation between the variables. Figure 7.9: Correlation plot. We are going to use the mpg variable as a output. Most of the other variables seem to have a non-linear relationship with the variable mpg. Let’s use the displacement in order to predict the mpg variable. fits &lt;- list() cv_error &lt;- vector(length = 10) set.seed(42) for(d in 1:10){ fits[d] &lt;- list(lm(mpg ~ bs(displacement, df = d), data = auto)) fit &lt;- glm(mpg ~ bs(displacement, df = d), data = auto) cv_error[d] &lt;- cv.glm(auto, fit, K = 10)$delta[2] } Figure 7.10: Cross-validation errors according to the degree of freedom of the B-splines. fit_final &lt;- glm(mpg ~ bs(displacement, df = 7), data = auto) pred &lt;- predict(fit_final, newdata = list(displacement = auto$displacement), se.fit = TRUE) Figure 7.11: B-spline model with 7 degree of freedom. 7.2.4 Exercise 9. boston &lt;- as_tibble(Boston) We uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response. Question (a) model_poly &lt;- lm(nox ~ poly(dis, 3), data = boston) Results of the linear model on the boston dataset. Formula: nox ~ poly(dis, 3) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 418 0.32 0 0 -0.12 -0.08 -0.07 -0.04 -0.01 0.02 0.09 0.13 0.19 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.55470 0.00276 201.02089 &lt; 2.22e-16 poly(dis, 3)1 -2.00310 0.06207 -32.27107 &lt; 2.22e-16 poly(dis, 3)2 0.85633 0.06207 13.79599 &lt; 2.22e-16 poly(dis, 3)3 -0.31805 0.06207 -5.12396 4.275e-07 Residual standard error: 0.062 on 502 degrees of freedom. Multiple \\(R^2\\): 0.715. Adjusted \\(R^2\\): 0.713. F-statistic: 419.335 on 3 and 502, p-value: &lt;2e-16. pred &lt;- predict(model_poly, newdata = list(dis = boston$dis), se.fit = TRUE) Figure 7.12: Cubic polynomial regression model Question (b) res &lt;- list() set.seed(42) for(d in 2:10){ fit &lt;- lm(nox ~ poly(dis, d), data = boston) res[[d-1]] &lt;- list(d = d, pred = predict(fit, newdata = list(dis = boston$dis), se.fit = TRUE)) } ggfit &lt;- function(d, pred){ ggplot(aes(x = dis, y = nox), data = boston) + geom_point(alpha = 0.5) + geom_ribbon(aes(ymin = pred$fit - 2*pred$se.fit, ymax = pred$fit + 2*pred$se.fit), alpha = 0.7, fill = &#39;blue&#39;) + geom_line(aes(x = boston$dis, y = pred$fit), col = &#39;red&#39;, lwd = 2) + ggtitle(paste(&#39;Degree: &#39;, d, &#39;(RSS = &#39;, round(sum((boston$nox - pred$fit)**2), 2), &#39;)&#39;)) + xlab(&#39;Weighted mean of distances to five Boston employment centres&#39;) + ylab(&#39;Nitrogen oxides concentration&#39;) + theme_custom() } plot_list &lt;- res %&gt;% map(~ ggfit(.x$d, .x$pred)) Figure 7.13: Results of polynomial fits. Question (c) fits &lt;- list() cv_error &lt;- vector(length = 10) set.seed(42) for(d in 1:10){ fits[d] &lt;- list(lm(nox ~ poly(dis, d), data = boston)) fit &lt;- glm(nox ~ poly(dis, d), data = boston) cv_error[d] &lt;- cv.glm(boston, fit, K = 10)$delta[2] } Figure 7.14: Cross-validation errors according to the considered degree. Question (d) We fit a regression spline using four degrees of freedom to predict nox using dis. We choose the knots to be the quantile of dis. model_spline &lt;- lm(nox ~ bs(dis, df = 4), data = boston) Results of the linear model on the boston dataset. Formula: nox ~ bs(dis, df = 4) Residuals Name NA_num Unique Range Mean Variance Minimum Q05 Q10 Q25 Q50 Q75 Q90 Q95 Maximum Residuals 0 420 0.32 0 0 -0.12 -0.08 -0.07 -0.04 -0.01 0.02 0.09 0.13 0.19 Coefficients Variable Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.73447 0.01460 50.30552 &lt; 2.22e-16 bs(dis, df = 4)1 -0.05810 0.02186 -2.65782 0.008116 bs(dis, df = 4)2 -0.46356 0.02366 -19.59614 &lt; 2.22e-16 bs(dis, df = 4)3 -0.19979 0.04311 -4.63395 4.5805e-06 bs(dis, df = 4)4 -0.38881 0.04551 -8.54394 &lt; 2.22e-16 Residual standard error: 0.062 on 501 degrees of freedom. Multiple \\(R^2\\): 0.716. Adjusted \\(R^2\\): 0.714. F-statistic: 316.463 on 4 and 501, p-value: &lt;2e-16. pred &lt;- predict(model_spline, newdata = list(dis = boston$dis), se.fit = TRUE) Figure 7.15: Four degrees of freedom spline regression model Question (e) res &lt;- list() set.seed(42) for(d in 2:10){ fit &lt;- lm(nox ~ bs(dis, df = d), data = boston) res[[d-1]] &lt;- list(d = d, pred = predict(fit, newdata = list(dis = boston$dis), se.fit = TRUE)) } plot_list &lt;- res %&gt;% map(~ ggfit(.x$d, .x$pred)) Figure 7.16: Results of splines fits. Question (f) fits &lt;- list() cv_error &lt;- vector(length = 10) set.seed(42) for(d in 1:10){ fits[d] &lt;- list(lm(nox ~ bs(dis, df = d), data = boston)) fit &lt;- glm(nox ~ bs(dis, df = d), data = boston) cv_error[d] &lt;- cv.glm(boston, fit, K = 10)$delta[2] } Figure 7.17: Cross-validation errors according to the considered degree. 7.2.5 Exercise 10. We are going to analize the College data set. college &lt;- as_tibble(College) Question (a) Let’s split the data set into training set and test set. idx &lt;- sample(1:nrow(college), size = round(0.5 * nrow(college))) train &lt;- college %&gt;% slice(idx) test &lt;- college %&gt;% slice(-idx) reg_subset &lt;- regsubsets(Outstate ~ ., data = train, nvmax = 10, method = &#39;forward&#39;) Figure 4.1: Selected variables for each criteria for forward selection. Figure 7.18: Best models according to \\(C_p\\), \\(BIC\\) and adjusted \\(R^2\\) for forward selection. Table 7.1: Coefficients for the best model according to \\(BIC\\). (Intercept) PrivateYes Top10perc Room.Board PhD perc.alumni Expend Grad.Rate -3295.056 2554.948 21.75788 1.18555 28.6945 41.16226 0.16548 21.72396 Table 7.2: Coefficients for the best model according to \\(C_p\\). (Intercept) PrivateYes Apps Accept Top10perc F.Undergrad Room.Board PhD perc.alumni Expend Grad.Rate -2959.954 2481.64 -0.18079 0.59595 26.94416 -0.1486 1.1412 26.30114 45.48378 0.16667 17.13799 Table 7.3: Coefficients for the best model according to Adjusted \\(R^2\\). (Intercept) PrivateYes Apps Accept Top10perc F.Undergrad Room.Board PhD perc.alumni Expend Grad.Rate -2959.954 2481.64 -0.18079 0.59595 26.94416 -0.1486 1.1412 26.30114 45.48378 0.16667 17.13799 According to BIC, a satisfactory model that uses just a subset of the predictors contatins the variables Private, Top10perc, Room.Board, PhD, perc.alumni, Expend and Grad.Rate. Question (b) Let’s a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous question as the predictors. gam_model &lt;- gam(Outstate ~ Private + ns(Top10perc, 4) + ns(Room.Board, 4) + ns(PhD, 4) + ns(perc.alumni, 4) + ns(Expend, 4) + ns(Grad.Rate, 4), data = train) Figure 4.8: Results of the GAM. We found the following results: Private schools have higher of state tuition. Universities that enroll the most students from top 10% of high school class appear to have slighly less tuition than the other ones. The higher the room and board costs the higher the tuition. The higher the number of PhD the higher the tuition. The higher the percentage of alumni the higher the tuition. The tuition is the highest for universities with not to low and not to high instructional expenditure per student. The tuition appears to be almost linear with the graduation rate. The more we paid to enter the university the more likely we obtain the degree. Question (c) pred &lt;- predict(gam_model, newdata = test, se.fit = TRUE) We found out the GAM model have a MSE of 4.0899017^{6}. Let’s compare the results with a linear model. lm_model &lt;- lm(Outstate ~ ., data = train) pred_lm &lt;- predict(lm_model, newdata = test, se.fit = TRUE) The MSE for the linear model is 4.2630381^{6}. So, the GAM model is significally better than the linear model. Question (d) According to the plots, most of the variables seems to have non-linear relationship with the response. Expend appears to be the variable with the most evidence of non-linear relationship with the response. 7.2.6 Exercise 11. In Section 7.7, it was mentioned that GAMs are generally fit using a backfitting approach. The idea behind backfitting is actually quite simple. We will now explore backfitting in the context of multiple linear regression. Suppose that we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. Therefore, we take the following iterative approach: we repeatedly hold all but one coefficient esti- mate fixed at its current value, and update only that coefficient estimate using a simple linear regression. The process is continued until convergence — that is, until the coefficient estimates stop changing. We now try this out on a toy example. Question (a) n &lt;- 100 X1 &lt;- rnorm(n) X2 &lt;- rnorm(n) betas &lt;- c(0.2, 0.3, -0.8) Y &lt;- betas[1] + betas[2] * X1 + betas[3] * X2 + rnorm(n, 0, 0.5) Question (b) Initialize \\(\\widehat{\\beta}_1\\) has a random value. beta1_hat &lt;- runif(1, 0, 1) Question (c) Assume \\(\\widehat{\\beta}_1\\) fixed, fit the model \\[ Y - \\widehat{\\beta}_1X_1 = \\beta_0 + \\beta_2X_2 + \\epsilon.\\] a &lt;- Y - beta1_hat * X1 beta2_hat &lt;- lm(a ~ X2)$coef[2] Question (d) Assume \\(\\widehat{\\beta}_2\\) fixed, fit the model \\[ Y - \\widehat{\\beta}_2X_2 = \\beta_0 + \\beta_1X_1 + \\epsilon.\\] a &lt;- Y - beta2_hat * X2 beta1_hat &lt;- lm(a ~ X1)$coef[2] Question (e) betas0 &lt;- c(0) betas1 &lt;- c(beta1_hat) betas2 &lt;- c(beta2_hat) for(i in 1:50){ a &lt;- Y - betas1[length(betas1)] * X1 betas2 &lt;- c(betas2, lm(a ~ X2)$coef[2]) a &lt;- Y - betas2[length(betas2)] * X2 betas1 &lt;- c(betas1, lm(a ~ X1)$coef[2]) betas0 &lt;- c(betas0, lm(a ~ X1)$coef[1]) } Figure 7.19: Estimation of the coefficients Question (f) Let’s perform a simple multiple linear regression and we’ll compare the results with the ones found in the previous answer. lm_model &lt;- lm(Y ~ X1 + X2) Figure 7.20: Estimation of the coefficients by lm Question (g) Two iterations seem to provide a rather good estimation of the coefficients. Moreover, after four iterations the algorithm appears to have converged. 7.2.7 Exercise 12. This problem is a continuation of the previous exercise. In a toy example with \\(p = 100\\), show that one can approximate the multiple linear regression coefficient estimates by repeatedly performing simple linear regression in a backfitting procedure. n &lt;- 1000 p &lt;- 100 X &lt;- matrix(rnorm(n * p), ncol = p, nrow = n) betas &lt;- rnorm(p) Y &lt;- X %*% betas + rnorm(n, 0, 0.2) d &lt;- 50 # Number of iterations betas_hat &lt;- matrix(0, ncol = p, nrow = d) MSE &lt;- rep(0, d) for(i in 1:d){ for(j in 1:p){ a &lt;- Y - (X[,-j] %*% betas_hat[i,-j]) betas_hat[i:d, j] = lm(a ~ X[, j])$coef[2] } MSE[i] &lt;- mean((Y - (X %*% betas_hat[i, ]))^2) } lm_model &lt;- lm(Y ~ -1 + X) betas_lm &lt;- coef(lm_model) MSE_lm &lt;- mean((Y - (X %*% betas_lm))**2) Figure 7.21: MSE using the backfitting approach compare to the lm model After four iterations it appears that the backfiting approach has the same MSE than the linear model. "],
["tree-based-methods.html", "Chapter 8 Tree-based methods 8.1 Conceptual exercises 8.2 Applied exercises", " Chapter 8 Tree-based methods 8.1 Conceptual exercises 8.1.1 Exercise 1. Figure 8.1: Example of results from binary splitting. 8.1.2 Exercise 2. Boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form \\[ f(x) = \\sum_{j = 1}^p f_j(X_j).\\] Consider only two predictive variables (\\(x = (X_1~ X_2)^\\top\\)) and let’s go through the algorithm 8.2. Set \\(\\widehat{f}(x) = 0\\) and \\(r_i = y_i\\) for all \\(i\\) in the training set. As we want depth-one trees (the number of split is one) and use all the variable, \\(B\\) (the number of trees) will going to be equal to \\(2\\). The first tree leads to \\[ \\widehat{f}^1(x) = a_1\\mathbb{1}(X_1 &lt; c_1) + b_1.\\] So, \\[ \\widehat{f}(x) = 0 + \\lambda\\widehat{f}^1(x) \\] and \\[ r_i = y_i - \\lambda\\widehat{f}^1(x_i). \\] We can do the same things with the other variable, and we found out \\[ \\widehat{f}^2(x) = a_2\\mathbb{1}(X_2 &lt; c_2) + b_2.\\] So, \\[ \\widehat{f}(x) = \\lambda\\left(\\widehat{f}^1(x) + \\widehat{f}^2(x)\\right) \\] and \\[ r_i = y_i - \\lambda\\left(\\widehat{f}^1(x_i) + \\widehat{f}^2(x)\\right). \\] Finally, by induction, we can extend this results to model with \\(p\\) features, and so, leads to an additive model. 8.1.3 Exercise 3. Recall the following definition. Denote by \\(K\\) the number of classes. Gini index: \\[ G = \\sum_{k = 1}^K \\widehat{p}_{mk}(1 - \\widehat{p}_{mk})\\] Classification error: \\[ E = 1 - \\max_k \\widehat{p}_{mk}\\] Cross-entropy: \\[ D = -\\sum_{k = 1}^K \\widehat{p}_{mk}\\log\\widehat{p}_{mk}\\] Figure 2.1: Error measures 8.1.4 Exercise 4. Question (a) Figure 8.2: Tree Question (b) Figure 8.3: Tree 8.1.5 Exercise 5. Suppose we produce ten bootstrapped samples from a dataset containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of \\(X\\), produce \\(10\\) estimates of \\(\\mathbb{P}(Red | X)\\): \\[ 0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75.\\] By majority vote, we found that the class is Red (four out six bootstrapped samples have \\(\\mathbb{P}(Red | X) &gt; 0.5\\)). By average probability, we found that the class is Green (because the mean probability among the bootstrapped samples is 0.45. 8.1.6 Exercise 6. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. In order to perform recursive binary splitting, we consider all predictors \\(X_1, \\dots, X_p\\), and all possible values of the cutpoint \\(s\\) for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS. For any \\(j\\) and \\(s\\), we define the pair of half-planes \\[ R_1(j, s) = \\{X | X_j &lt; s\\} \\quad\\text{and}\\quad R_2(j,s) = \\{X | X_j \\geq s\\},\\] and we seek the value of \\(j\\) and \\(s\\) that minimize the equation \\[ \\sum_{i : x_i \\in R_1(j, s)}(y_i - \\widehat{y}_{R_1})^2 + \\sum_{i : x_i \\in R_2(j, s)}(y_i - \\widehat{y}_{R_2})^2\\], where \\(\\widehat{y}_{R_1}\\) is the mean response for the training observations in \\(R_1(j, s)\\), and \\(\\widehat{y}_{R_2}\\) is the mean response for the training observations in \\(R_2(j, s)\\). Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to mnimize the RSS within each of the resulting regions. However, instead of splitting the entire predictor space, we split one of the two previously identified regions. The process continues until a stopping criterion is reached (e.g no region contains more than five observations). Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\). Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\) there corresponds a subtree \\(T \\subset T_0\\) such that \\[ \\sum_{m = 1}^{|T|}\\sum_{i: x_i \\in R_m} (y_i - \\widehat{y}_{R_m})^2 + \\alpha|T|\\] is as small as possible. \\(|T|\\) indicates the number of terminal nodes of the tree \\(T\\), \\(R_m\\) is the rectangle (i.e the subset of predictor space) corresponding to the \\(m\\)th terminal node, and \\(\\widehat{y}_{R_m}\\) is the predicted response associated with \\(R_m\\). The tuning parameter \\(\\alpha\\) controls a trade-off between the subtree’s complexity and its fit to the training data. Use \\(K\\)-fold cross-validation to choose \\(\\alpha\\). That is, diivide the training observations into \\(K\\) folds. For each \\(k = 1, \\dots, K\\): Repeat steps 1 and 2 on all but the \\(k\\)th fold of the training data. Evaluate the mean squared prediction error on the data in the left-out \\(k\\)th fold, as a function of \\(\\alpha\\). Average the results for each value of \\(\\alpha\\), and pick \\(\\alpha\\) to minimize the average error. Return the subtree from step 2 that corresponds to the chosen value of \\(\\alpha\\). 8.2 Applied exercises 8.2.1 Exercise 7. set.seed(42) boston &lt;- as_tibble(Boston) train &lt;- sample(1:nrow(boston), size = nrow(boston)/2) p &lt;- ncol(boston) - 1 mtry &lt;- c(p, p/3, round(sqrt(p), 0)) MSE &lt;- matrix(rep(0, length(mtry) * 500), ncol = length(mtry)) for(m in 1:length(mtry)){ t_boston &lt;- randomForest(medv ~ .,data = boston, subset = train, xtest = select(slice(boston, -train), -medv), ytest = pull(slice(boston, -train), medv), mtry = mtry[m], ntree = 500) MSE[, m] &lt;- t_boston$test$mse } Figure 8.4: Test error resulting from random forests The MSE decreaeses quickly with the number of trees. Then, the three values for mtry give quite the same results and have a MSE around 10 after 100 trees. 8.2.2 Exercise 8. We will seek to predict Sales, from Carseats dataset, using regression trees and related approaches, treating the response a quantitative variable. carseats &lt;- as_tibble(Carseats) Question (a) idx &lt;- sample(1:nrow(carseats), size = nrow(carseats)/3) train &lt;- carseats %&gt;% slice(-idx) test &lt;- carseats %&gt;% slice(idx) Question (b) tree_carseat &lt;- tree(Sales ~ ., data = train) sales_hat &lt;- predict(tree_carseat, newdata = test) MSE_test &lt;- mean((test$Sales - sales_hat)**2) Figure 8.5: Regression tree fit on the training data The MSE on the test set is 5.369 with regression tree. Question (c) We use cross-validation in order to determine the optimal level of tree complexity. cv_carseat &lt;- cv.tree(tree_carseat, FUN = prune.tree) tree_pruned_carseat &lt;- prune.tree(tree_carseat, best = cv_carseat$size[which.min(cv_carseat$dev)]) sales_hat_pruned &lt;- predict(tree_pruned_carseat, newdata = test) MSE_pruned_test &lt;- mean((test$Sales - sales_hat_pruned)**2) Figure 8.6: Pruned regression tree fit on the training data The MSE on the test set is 5.539 with pruned regression tree. It does not improve the test MSE. Question (d) Then, we use the bagging approach in order to analyze this data. Recall that bagging is simply a special case of a random forest with \\(m = p\\). set.seed(42) bagging_carseat &lt;- randomForest(Sales ~ ., data = train, mtry = ncol(carseats)-1, importance = TRUE) sales_hat_bagging &lt;- predict(bagging_carseat, newdata = test) MSE_bagging_test &lt;- mean((test$Sales - sales_hat_bagging)**2) The MSE on the test set is 2.832 with bagging. Figure 6.3: Importance plot Question (e) Finally, we use the random forest to analyze the data. set.seed(42) m_try &lt;- seq(1, ncol(carseats)-1, by = 1) rf_carseat &lt;- m_try %&gt;% map(function(m) randomForest(Sales ~ ., data = train, mtry = m, importance = TRUE)) sales_hat_rf &lt;- rf_carseat %&gt;% map(function(rf) predict(rf, newdata = test)) MSE_rf_test &lt;- sales_hat_rf %&gt;% map_dbl(function(predict) mean((test$Sales - predict)**2)) Figure 6.7: MSE with respect to \\(m\\) The best \\(m\\) is 6 for the random forest. It leads to a MSE of 2.6424074. 8.2.3 Exercise 9. We will seek to predict Purchase, from OJ dataset, using regression trees and related approaches, treating the response a qualitative variable. OJ &lt;- as_tibble(OJ) Question (a) idx &lt;- sample(1:nrow(OJ), size = 800) train &lt;- OJ %&gt;% slice(idx) test &lt;- OJ %&gt;% slice(-idx) Question (b) set.seed(42) tree_OJ &lt;- tree(Purchase ~ ., data = train) The training error rate is 16%. The tree has 9 terminal nodes. The used variables to grown the tree are LoyalCH, PriceDiff, SpecialCH, ListPriceDiff. Question (c) tree_OJ ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 800 1078.00 CH ( 0.59750 0.40250 ) ## 2) LoyalCH &lt; 0.48285 304 320.70 MM ( 0.22039 0.77961 ) ## 4) LoyalCH &lt; 0.0356415 55 0.00 MM ( 0.00000 1.00000 ) * ## 5) LoyalCH &gt; 0.0356415 249 290.00 MM ( 0.26908 0.73092 ) ## 10) PriceDiff &lt; 0.235 143 132.50 MM ( 0.17483 0.82517 ) ## 20) SpecialCH &lt; 0.5 127 96.18 MM ( 0.12598 0.87402 ) * ## 21) SpecialCH &gt; 0.5 16 21.93 CH ( 0.56250 0.43750 ) * ## 11) PriceDiff &gt; 0.235 106 142.30 MM ( 0.39623 0.60377 ) * ## 3) LoyalCH &gt; 0.48285 496 454.40 CH ( 0.82863 0.17137 ) ## 6) LoyalCH &lt; 0.764572 246 299.20 CH ( 0.70325 0.29675 ) ## 12) PriceDiff &lt; 0.085 82 111.90 MM ( 0.42683 0.57317 ) ## 24) ListPriceDiff &lt; 0.235 56 68.75 MM ( 0.30357 0.69643 ) * ## 25) ListPriceDiff &gt; 0.235 26 32.10 CH ( 0.69231 0.30769 ) * ## 13) PriceDiff &gt; 0.085 164 143.40 CH ( 0.84146 0.15854 ) ## 26) PriceDiff &lt; 0.265 68 80.57 CH ( 0.72059 0.27941 ) * ## 27) PriceDiff &gt; 0.265 96 50.13 CH ( 0.92708 0.07292 ) * ## 7) LoyalCH &gt; 0.764572 250 96.29 CH ( 0.95200 0.04800 ) * Consider the final node \\(27\\). The splitting variable at this node is PriceDiff. The splitting value at this node is \\(0.265\\). There are \\(96\\) points in the subtree below this node. The deviance for all points contained in region below this node is \\(50.13\\). The prediction at this node is Purchase = CH. About \\(93\\%\\) of the points in this node have Sales = CH and the remaining points have Purchase = MM. Question (d) Figure 3.5: Classification tree fit on the training data LoyalCH is the most importante variable of the tree. If LoyalCH &lt; 0.03, the tree predicts MM. If LoyalCH &gt; 0.76, the tree predict CH. If LoyalCH lies between \\(0.03\\) and \\(0.76\\), the result depends on PriceDiff, SpecialCh and ListPriceDiff. Question (e) purchase_hat &lt;- predict(tree_OJ, newdata = test, type = &#39;class&#39;) MSE_test &lt;- mean(test$Purchase != purchase_hat) The test error rate is 18.15%. Figure 7.16: Confusion matrix for the tree model on the test set. Question (f) set.seed(42) cv_OJ &lt;- cv.tree(tree_OJ, FUN = prune.tree) Question (g) Figure 8.7: Cross-validation error rate with respect to the tree size Question (h) The tree that corresponds to the lowest cross-validated classification error rate is of the size 9. Question (i) tree_pruned_OJ &lt;- prune.tree(tree_OJ, best = 5) Question (j) The training error rate is 17.5% of the pruned tree. Question (k) purchase_hat_pruned &lt;- predict(tree_pruned_OJ, newdata = test, type = &#39;class&#39;) MSE_pruned_test &lt;- mean(test$Purchase != purchase_hat_pruned) The test error rate is 21.48% on the pruned tree. 8.2.4 Exercise 10. We now use boosting to predict Salary in the Hitters data set. Question (a) hitters &lt;- as_tibble(Hitters) hitters &lt;- hitters %&gt;% filter(!is.na(hitters$Salary)) %&gt;% mutate(log_Salary = log(Salary)) Question (b) train &lt;- hitters %&gt;% slice(1:200) test &lt;- hitters %&gt;% slice(201:263) Question (c) and Question (d) set.seed(42) lambda &lt;- 10**seq(-5, 0, by = 0.1) MSE_train &lt;- rep(0, length(lambda)) MSE_test &lt;- rep(0, length(lambda)) for(i in 1:length(lambda)){ boost_hitters &lt;- gbm(log_Salary ~ ., data = select(train, -Salary), distribution = &#39;gaussian&#39;, n.trees = 1000, shrinkage = lambda[i]) train_pred &lt;- predict(boost_hitters, train, n.tree = 1000) MSE_train[i] &lt;- mean((train$log_Salary - train_pred)**2) test_pred &lt;- predict(boost_hitters, test, n.tree = 1000) MSE_test[i] &lt;- mean((test$log_Salary - test_pred)**2) } Figure 2.8: Mean Square Error The minimum test error which is 0.2564687 is obtained at \\(\\lambda = 0.0398107\\). Question (e) lm_hitters &lt;- lm(log_Salary ~ ., data = select(train, -Salary)) lm_pred &lt;- predict(lm_hitters, data = test) MSE_lm &lt;- mean((test$log_Salary - lm_pred)**2) The test MSE of the linear regression model is 1.1697807. X &lt;- model.matrix(log_Salary ~ ., data = select(train, -Salary)) Y &lt;- train$log_Salary X_test &lt;- model.matrix(log_Salary ~ ., data = select(test, -Salary)) cv_out &lt;- cv.glmnet(X, Y, alpha = 0) ridge_hitters &lt;- glmnet(X, Y, alpha = 0, lambda = cv_out$lambda.min) ridge_pred &lt;- predict(ridge_hitters, newx = X_test) MSE_ridge &lt;- mean((test$log_Salary - ridge_pred)**2) The test MSE of the ridge regression model is 0.4568975. The boosting model is way better than linear and ridge regression models. Question (f) Figure 8.8: Importance plot Question (g) set.seed(42) bagging_hitters &lt;- randomForest(log_Salary ~ ., data = select(train, -Salary), mtry = ncol(hitters)-2, importance = TRUE) salary_hat_bagging &lt;- predict(bagging_hitters, newdata = test) MSE_bagging_test &lt;- mean((test$log_Salary - salary_hat_bagging)**2) The MSE on the test set is 0.234 with bagging. 8.2.5 Exercise 11. Question (a) caravan &lt;- as_tibble(Caravan) caravan$Purchase &lt;- ifelse(caravan$Purchase == &#39;Yes&#39;, 1, 0) train &lt;- caravan %&gt;% slice(1:1000) test &lt;- caravan %&gt;% slice(1001:5822) Question (b) Figure 8.9: Importance plot Question (c) test_pred &lt;- predict(boost_caravan, test, n.tree = 1000, type = &#39;response&#39;) test_pred &lt;- ifelse(test_pred &gt; 0.2, 1, 0) Figure 8.10: Confusion matrix for the boosting model on the test set. 21.83% of people who are predicted to make a purchase actually end up making one with the boosting model. knn_model &lt;- knn(select(train, -Purchase), select(test, -Purchase), train$Purchase, k = 5) Figure 8.11: Confusion matrix for the \\(k\\)NN model on the test set. 11.76% of people who are predicted to make a purchase actually end up making one with the \\(k\\)NN model. 8.2.6 Exercise 12. Check out this Kaggle kernel for a comparison of boosting, bagging and random forests with logistic regression on a diabete dataset. "],
["support-vector-machines.html", "Chapter 9 Support Vector Machines 9.1 Conceptual exercises 9.2 Applied exercises", " Chapter 9 Support Vector Machines 9.1 Conceptual exercises 9.1.1 Exercise 1. This problem involves hyperplanes in two dimensions. The blue line correspond to the hyperplane \\(1 + 3X_1 - X_2 = 0\\) and the purple line correspond to the hyperplane \\(-2 + X_1 + 2X_2 = 0\\). The blue points are in the space where \\(1 + 3X_1 - X_2 &gt; 0\\) and \\(-2 + X_1 + 2X_2 &gt; 0\\). The red points are in the space where \\(1 + 3X_1 - X_2 &gt; 0\\) and \\(-2 + X_1 + 2X_2 &lt; 0\\). The green points are in the space where \\(1 + 3X_1 - X_2 &lt; 0\\) and \\(-2 + X_1 + 2X_2 &lt; 0\\). The yellow points are in the space where \\(1 + 3X_1 - X_2 &lt; 0\\) and \\(-2 + X_1 + 2X_2 &gt; 0\\). Figure 8.1: Example of hyperplanes in two dimensions. 9.1.2 Exercise 2. This problem involves non-linear decision boundary in two dimensions. Question (a) and Question (b) Let’s plot the curve \\((1 + X_1)^2 + (2 - X_2)^2 = 4\\). The blue points refer to the space where \\((1 + X_1)^2 + (2 - X_2)^2 &gt; 4\\) and the red points to \\((1 + X_1)^2 + (2 - X_2)^2 \\leq 4\\). Figure 9.1: Example of non-linear decision boundary Question (c) The observation \\((0, 0)\\) will be blue, \\((-1, 1)\\) red, \\((2, 2)\\) blue and \\((3, 8)\\) also blue. Question (d) Let’s expand the formula \\((1 + X_1)^2 + (2 - X_2)^2\\). \\[(1 + X_1)^2 + (2 - X_2)^2 = 5 + 2X_1 + X_1^2 + 4 - 4X_2 + X_2^2\\] This expression is linear in terms of \\(X_1\\), \\(X_2\\), \\(X_1^2\\) and \\(X_2^2\\). 9.1.3 Exercise 3. We explore the maximal margin classifier on a toy data set. Question (a) and Question (b) The optimal separating hyperplane aims to separate the two classes by maximising the distance between the closest points of the different classes. So, it has to pass though the middle of the observations \\(2\\) and \\(5\\) which is the point \\((2, 1.5)\\) and \\(3\\) and \\(6\\) which is the point \\((4, 3.5)\\). Thus, it leads to the equation \\(y: x \\mapsto x - 0.5\\) df &lt;- tibble(X1 = c(3, 2, 4, 1, 2, 4, 4), X2 = c(4, 2, 4, 4, 1, 3, 1), Y = c(&#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;R&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;)) Figure 9.2: Example of toy dataset with a separating hyperplane Question (c) We rewrite the equation found in the previous question as: \\[0.5 - X_1 + X_2 = 0\\] Then, classify to Red if \\(0.5 - X_1 + X_2 &gt; 0\\), and classify to Blue otherwise. Question (d) Figure 9.3: Example of the margins Question (e) Figure 9.4: Example of the support vectors Question (f) The seventh observation, which is the point \\((4, 1)\\) do not affect the maximal margin hyperplane because it does not belong to the margins. Question (g) A non-optimal separating hyperplane would be \\[0.1 - 0.8X_1 + X_2 = 0.\\] Figure 9.5: Example of the non-optimal hyperplane Question (h) Figure 9.6: Example of the non-separable points 9.2 Applied exercises 9.2.1 Exercise 4. We simulate a two-class data set with \\(100\\) observations and two features in which there is a visible but non-linear separation between the two classes. set.seed(42) t &lt;- seq(0, 1, length.out = 50) X_1 &lt;- sin(2*pi*t) + rnorm(50, 0, 0.25) X_2 &lt;- sin(2*pi*t) + 1 + rnorm(50, 0, 0.25) df &lt;- tibble(t = c(t, t), X = c(X_1, X_2), cl = as.factor(c(rep(-1, 50), rep(1, 50)))) idx &lt;- sample(1:nrow(df), 50) train &lt;- df %&gt;% slice(idx) test &lt;- df %&gt;% slice(-idx) Figure 9.7: Simulated data The boundary between the two classes is clearly non-linear. Linear SVM svm_linear &lt;- tune(&#39;svm&#39;, cl ~ ., data = train, kernel = &#39;linear&#39;, ranges = list(gamma = 2^(-1:1), cost = 2^(2:4))) svm_linear_best &lt;- svm_linear$best.model preds_train &lt;- predict(svm_linear_best, train) preds_test &lt;- predict(svm_linear_best, test) Around 16% of the train observations are misclassified and 18% of the test observations. Figure 9.8: Results of linear SVM Polynomial SVM svm_poly &lt;- tune(&#39;svm&#39;, cl ~ ., data = train, kernel = &#39;polynomial&#39;, ranges = list(gamma = 2^(-1:1), cost = 2^(2:4), degree = 2:5)) svm_poly_best &lt;- svm_poly$best.model preds_train &lt;- predict(svm_poly_best, train) preds_test &lt;- predict(svm_poly_best, test) Around 22 of the train observations are misclassified and 22% of the test observations. Figure 9.9: Results of polynomial SVM Radial SVM svm_radial &lt;- tune(&#39;svm&#39;, cl ~ ., data = train, kernel = &#39;radial&#39;, ranges = list(gamma = 2^(-1:1), cost = 2^(2:4))) svm_radial_best &lt;- svm_radial$best.model preds_train &lt;- predict(svm_radial_best, train) preds_test &lt;- predict(svm_radial_best, test) Around 4 of the train observations are misclassified and 10% of the test observations. Figure 9.10: Results of radial SVM Conclusion Here, the radial kernel shows the best results in term of misclassification error rate. Of course, it was expected because the generating process was a sinus. 9.2.2 Exercise 5. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features. Question (a) X1 &lt;- runif(500) - 0.5 X2 &lt;- runif(500) - 0.5 Y &lt;- as.factor(1 * (X1**2 - X2**2 &gt; 0)) df &lt;- tibble(X1, X2, Y) Question (b) Figure 9.11: Plot of the observations with true classes Question (c) lm_model &lt;- glm(Y ~ ., data = df, family = &#39;binomial&#39;) Question (d) pred_lm &lt;- predict(lm_model, df, type = &#39;response&#39;) Figure 9.12: Plot of the observations with predicted classes for LM model The decision boundary is linear and do not fit to the true regression line. Question (e) glm_model &lt;- glm(Y ~ poly(X1, 2) + poly(X2, 2) + I(X1 * X2), data = df, family = &#39;binomial&#39;) Question (f) pred_glm &lt;- predict(glm_model, df, type = &#39;response&#39;) Figure 9.13: Plot of the observations with predicted classes for GLM model The decision boundary is not linear and looks like the true decision boundary. Question (g) svm_poly &lt;- tune(&#39;svm&#39;, Y ~ ., data = df, kernel = &#39;polynomial&#39;, ranges = list(cost = 2^(2:4))) svm_poly_best &lt;- svm_poly$best.model preds_svm &lt;- predict(svm_poly_best, df) Figure 9.14: Results of polynomial SVM A linear kernel fails to find non linear boundary. Question (h) svm_radial &lt;- tune(&#39;svm&#39;, Y ~ ., data = df, kernel = &#39;radial&#39;, ranges = list(cost = 2^(2:4))) svm_radial_best &lt;- svm_radial$best.model preds_svm &lt;- predict(svm_radial_best, df) Figure 9.15: Results of kernel SVM A radial kernel performs way better on this data. The prediction boundary seems to be quite close to the true boundary. Question (i) So, the support vector machine, with radial kernel, appears to be very good to find non-linear decision boundary. However, even if logistic regression may also found out this kind of boundary, it requires to add non linear transformation of the features to find it. 9.2.3 Exercise 6. Question (a) set.seed(42) X1 &lt;- runif(500, 0, 1) X2 &lt;- c(runif(250, X1[1:250] + 0.05), runif(250, 0, X1[251:500] - 0.05)) noise_X1 &lt;- runif(100, 0.1, 0.9) noise_X2 &lt;- 0.8 * noise_X1 + 0.1 Y &lt;- as.factor(1 * (X1 - X2 &gt; 0)) noise_Y &lt;- as.factor(sample(c(0, 1), size = 100, replace = TRUE)) df &lt;- tibble(X1, X2, Y) %&gt;% bind_rows(tibble(X1 = sort(noise_X1), X2 = sort(noise_X2), Y = noise_Y)) %&gt;% filter(!is.na(Y)) Figure 7.3: Plot of the observations with true classes Question (b) svm_poly &lt;- tune(&#39;svm&#39;, Y ~ ., data = df, kernel = &#39;linear&#39;, ranges = list(cost = 10^(-2:4))) Figure 7.5: Plot of the errors from cross-validation Question (c) set.seed(43) X1 &lt;- runif(500, 0, 1) X2 &lt;- runif(500, 0, 1) Y &lt;- as.factor(1 * (X1 - X2 &gt; 0)) df_test &lt;- tibble(X1, X2, Y) costs = 10**(-2:4) errors_test &lt;- rep(NA, length(costs)) for(i in 1:length(costs)){ model_svm &lt;- svm(Y ~ ., data = df, kernel = &#39;linear&#39;, cost = costs[i]) pred &lt;- predict(model_svm, df_test) errors_test[i] &lt;- mean(df_test$Y != pred) } Figure 9.16: Plot of the errors on the test set Question (d) Here, we see that a smaller cost performs better on the test dataset. But, we do not point out the overfitting phenomenon of a high cost on the train dataset. 9.2.4 Exercise 7. We will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto dataset. auto &lt;- as_tibble(Auto) %&gt;% select(-c(&#39;name&#39;, &#39;year&#39;, &#39;origin&#39;, &#39;weight&#39;, &#39;cylinders&#39;)) Question (a) We create a binary variable that takes on a \\(1\\) for cars with gas mileage above the median, and a \\(0\\) for cars with gas mileage below the median. Y &lt;- 1 * (auto$mpg &gt; median(auto$mpg)) auto &lt;- auto %&gt;% add_column(Y) %&gt;% select(-c(&#39;mpg&#39;)) %&gt;% mutate_at(vars(Y), funs(as.factor(.))) Question (b) svm_linear &lt;- tune(&#39;svm&#39;, Y ~ ., data = auto, kernel = &#39;linear&#39;, ranges = list(cost = 10^(-2:4))) Figure 9.17: Plot of the errors from cross-validation The lowest cross-validation error is obtained for cost = 0.1. Question (c) svm_radial &lt;- tune(&#39;svm&#39;, Y ~ ., data = auto, kernel = &#39;radial&#39;, ranges = list(cost = 10^(-2:4), gamma = 10^(-2:4))) Figure 9.18: Plot of the errors from cross-validation The lowest cross-validation error is obtained for cost = 10000 and gamma = 0.01. svm_poly &lt;- tune(&#39;svm&#39;, Y ~ ., data = auto, kernel = &#39;polynomial&#39;, ranges = list(cost = 10^(-2:4), degree = 1:4)) Figure 9.19: Plot of the errors from cross-validation The lowest cross-validation error is obtained for cost = 0.1 and degree = 1. Question (d) Figure 9.20: Results based on displacement x horsepower 9.2.5 Exercise 8. The problem involves the OJ dataset. df &lt;- as_tibble(OJ) Question (a) set.seed(42) idx &lt;- sample(1:nrow(df), 800) train &lt;- df %&gt;% slice(idx) test &lt;- df %&gt;% slice(-idx) Question (b) Let’s fit a support vector classifier to the training data using cost = 0.01 with Purchase as the response and the other variables as predictors. svm_linear &lt;- svm(Purchase ~ ., data = train, kernel = &#39;linear&#39;, cost = 0.01) Its prodoces 432 supports vectors out of \\(800\\) training points. Out of these, 215 belong to level CH and 217 belong to level MM. Question (c) train_error &lt;- mean(train$Purchase != predict(svm_linear, train)) test_error &lt;- mean(test$Purchase != predict(svm_linear, test)) The training error rate is 17.12% and the test error rate is 16.3%. Question (d) svm_linear &lt;- tune(&#39;svm&#39;, Purchase ~ ., data = train, kernel = &#39;linear&#39;, ranges = list(cost = 10^(seq(-2, 1, by = 0.1)))) The optimal cost found is 1. Question (e) train_error &lt;- mean(train$Purchase != predict(svm_linear$best.model, train)) test_error &lt;- mean(test$Purchase != predict(svm_linear$best.model, test)) The training error rate is 16.75% and the test error rate is 16.3%. Question (f) We do the same process using support vector machine with radial kernel. svm_radial &lt;- tune(&#39;svm&#39;, Purchase ~ ., data = train, kernel = &#39;radial&#39;, ranges = list(cost = 10^(seq(-2, 1, by = 0.1)))) The optimal cost found is 0.6309573. train_error &lt;- mean(train$Purchase != predict(svm_radial$best.model, train)) test_error &lt;- mean(test$Purchase != predict(svm_radial$best.model, test)) The training error rate is 15.25% and the test error rate is 15.56%. Question (f) We do the same process using support vector machine with radial kernel. svm_poly &lt;- tune(&#39;svm&#39;, Purchase ~ ., data = train, kernel = &#39;polynomial&#39;, ranges = list(cost = 10^(seq(-2, 1, by = 0.1)), degree = 2)) The optimal cost found is 5.0118723. train_error &lt;- mean(train$Purchase != predict(svm_poly$best.model, train)) test_error &lt;- mean(test$Purchase != predict(svm_poly$best.model, test)) The training error rate is 14.75% and the test error rate is 16.67%. Question (h) In this case, it appears that the support vector classifier with radial kernel give the best results in terms of percentage error on the test set for this dataset. However, all the results are pretty close. "],
["unsupervised-learning.html", "Chapter 10 Unsupervised Learning 10.1 Conceptual exercises 10.2 Applied exercises", " Chapter 10 Unsupervised Learning 10.1 Conceptual exercises 10.1.1 Exercise 1. This problem involves the \\(K\\)-means clustering algorithm. Question (a) We want to prove that \\[\\frac{1}{\\lvert C_k \\rvert}\\sum_{i, i^\\prime \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i^\\prime j})^2 = 2 \\sum_{i \\in C_k}\\sum_{j = 1}^p(x_{ij} - \\bar{x}_{kj})^2 \\quad\\text{where}\\quad \\bar{x}_{kj} = \\frac{1}{\\lvert C_k \\rvert}\\sum_{i \\in C_k} x_{ij}.\\] \\[\\begin{align*} \\frac{1}{\\lvert C_k \\rvert}\\sum_{i, i^\\prime \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i^\\prime j})^2 &amp;= \\frac{1}{\\lvert C_k \\rvert}\\sum_{i, i^\\prime \\in C_k} \\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj} + \\bar{x}_{kj} - x_{i^\\prime j})^2 \\\\ &amp;= \\frac{1}{\\lvert C_k \\rvert}\\sum_{i, i^\\prime \\in C_k} \\sum_{j=1}^p \\left((x_{ij} - \\bar{x}_{kj})^2 + (x_{i^\\prime j} - \\bar{x}_{kj})^2 - 2(x_{ij} - \\bar{x}_{kj})(x_{i^\\prime j} - \\bar{x}_{kj})\\right) \\\\ &amp;= \\frac{1}{\\lvert C_k \\rvert} \\sum_{j=1}^p \\left(2\\lvert C_k \\rvert \\sum_{i \\in C_k}(x_{ij} - \\bar{x}_{kj})^2 - 2\\sum_{i, i^\\prime \\in C_k}(x_{ij} - \\bar{x}_{kj})(x_{i^\\prime j} - \\bar{x}_{kj})\\right) \\\\ &amp;= 2 \\sum_{i \\in C_k}\\sum_{j = 1}^p(x_{ij} - \\bar{x}_{kj})^2 - \\frac{2}{\\lvert C_k \\rvert}\\sum_{j = 1}^p \\sum_{i, i^\\prime \\in C_k}(x_{ij} - \\bar{x}_{kj})(x_{i^\\prime j} - \\bar{x}_{kj}) \\end{align*}\\] Moreover, \\[\\sum_{j = 1}^p \\sum_{i, i^\\prime \\in C_k}(x_{ij} - \\bar{x}_{kj})(x_{i^\\prime j} - \\bar{x}_{kj}) = 0 \\quad\\text{because}\\quad \\sum_{i \\in C_k}(x_{ij} - \\bar{x}_{kj}) = 0\\] So, the equality is proved. Question (b) The previous equation show that minimizing the total within-cluster variation is equivalent to minimize the sum of the Euclideans square distance for each cluster. And thus, assigning each observation to the cluster whise centroid is closest will decrease the objective (10.11). 10.1.2 Exercise 2. Suppose that we have four observations, for which we compute a dissimilarity matrix given by \\[\\begin{pmatrix} &amp; 0.3 &amp; 0.4 &amp; 0.7 \\\\ 0.3 &amp; &amp; 0.5 &amp; 0.8 \\\\ 0.4 &amp; 0.5 &amp; &amp; 0.45 \\\\ 0.7 &amp; 0.8 &amp; 0.45 &amp; \\\\ \\end{pmatrix}.\\] D &lt;- as.dist(matrix(c(0, 0.3, 0.4, 0.7, 0.3, 0, 0.5, 0.8, 0.4, 0.5, 0, 0.45, 0.7, 0.8, 0.45, 0), nrow = 4)) Question (a) Figure 10.1: Dendrogram using complete linkage. Question (b) Figure 10.2: Dendrogram using single linkage. Question (c) If we cut the dendrogram obtained in question (a) such that two clusters results, we will have one cluster with \\(1\\) and \\(2\\) and the other one will have \\(3\\) and \\(4\\). Question (d) If we cut the dendrogram obtained in question (b) such that two clusters results, we will have one cluster with \\(1\\), \\(2\\) and \\(3\\) and the other one will only contain \\(4\\). Question (e) Figure 10.3: Dendrogram using complete linkage. 10.1.3 Exercise 3. In this problem, we will perform \\(K\\)-means clustering manually, with \\(K = 2\\), on a small example with \\(n = 6\\) observations and \\(p = 2\\) features. Obs \\(X_1\\) \\(X_2\\) 1 1 4 2 1 3 3 0 4 4 5 1 5 6 2 6 4 0 df &lt;- as_tibble(matrix(c(1, 4, 1, 3, 0, 4, 5, 1, 6, 2, 4, 0), ncol = 2, byrow = TRUE)) Question (a) Figure 10.4: Observations. Question (b) set.seed(42) labels &lt;- sample(c(0, 1), nrow(df), replace = TRUE) df &lt;- df %&gt;% add_column(label = as.factor(labels)) Figure 10.5: Observation with random cluster labels. Question (c) df_centroid &lt;- group_by(df, label) %&gt;% summarise(V1 = mean(V1), V2 = mean(V2)) Figure 10.6: Observation with random cluster labels and centroid. Question (d) We reassign the observations to their closest centroid. dist_mat &lt;- df %&gt;% bind_rows(df_centroid) %&gt;% dist() %&gt;% as.matrix() df$label2 &lt;- 1 * (dist_mat[7, 1:6] &gt; dist_mat[8, 1:6]) Figure 10.7: Observation with cluster labels. Question (e) and (f) Finally, we recompute the centroid. df_centroid &lt;- group_by(df, label2) %&gt;% summarise(V1 = mean(V1), V2 = mean(V2)) Figure 9.4: Observation with random cluster labels and centroid. 10.1.4 Exercise 4. Question (a) There is not enough information to tell because maximal intercluster dissimilarity could be equal to the mimimal intercluster dissimilarity. However, this case will be very unlikel to happened. Most of the time, the single linkage will fused at a lower height than the complete linkage. Question (b) They will fuse at the same height because there is only one observation in each of the cluster. 10.1.5 Exercise 5. First, let’s consider the case of number of item purchases. The number of computers sold is 0 or 1, and thus will have no impact on the results of \\(K\\)-means clustering with Euclidean distance. The two classes will result on the clustering of the number of socks sold. The orange seller will be alone in his cluster and the remaining ones are the other cluster. When we scales each variable by its standard deviation, the \\(K\\)-means clustering result in a clustering computer purchase / no computer purchase because the Euclidean distance in the computer dimension in greater than the one in the socks dimension. The last scalling give all the weight to the computer, so it will result in a clustering computer purchase / no computer purchase. 10.1.6 Exercise 6. Question (a) The first principal component explains \\(10\\%\\) of the variance means that by projecting the data onto the first principal components, the information contains within this projected data corresponds to \\(10\\%\\) of the total variation presents in the raw data. We can also say that \\(90\\%\\) of the variance is lost by projeting the data onto the first principal component. The first component mostly carries the information of the machine used to perform the test. Question (b) We can try to guess why the researcher decides to replace the \\((j,i)\\)th element of \\(X\\) with \\(x_{ji} - \\phi_{j1}z_{i1}\\). We would say that we want to remove the information of the machine used for the test from the data. However, \\({x_{ij}} - {z_{i1}}{\\phi _{j1}}\\) corresponds to the remaning information after projecting observations onto the first principal and the remaining information without the machine used. For the analysis of the PCA, it is suggested to firstly plot a scree plot w.r.t. number of principal components used, find the elbow point at which the number of principal components are preferred, say it is \\(T\\). Then, we can replace the \\(i\\) th original observation \\(x_i\\) in predictor space of dimension \\(p = 100\\) with \\(z_i\\) in reduced predictor space of dimention \\(T\\) which contains sufficient information (whether it is good approximating original observations using such an elbow point depends on the data set). Afterwards, instead of analyzing on a \\(1000 \\times 100\\) matrix, we are now able to reduce its size to \\(1000 \\times T\\) where \\(T \\ll 100\\). But, here, analyzing the components of the PCA with two samples \\(t\\)-test in not feasible because the information of conditions is lost. For the analysis, it may be better to split the dataframe considering the machine used (maybe, perform a \\(K\\)-means clustering to retrieve the group). And, then perform the two samples \\(t\\)-test on the two group separately. Question (c) We will the data Ch10Ex11.csv from the book website with some small modification in order to show our method. This dataset consists of \\(40\\) tissue samples with measurements on \\(1000\\) genes. The first \\(20\\) samples are from healthy patients, while the second \\(20\\) are from a diseased group. So, we can consider the first \\(20\\) as the control group and the other \\(20\\) as the treatment group. df &lt;- read_csv(&#39;data/Ch10Ex11.csv&#39;, col_names = FALSE) df &lt;- t(df) %&gt;% as_tibble() df &lt;- df %&gt;% add_column(group = c(rep(&#39;C&#39;, 20), rep(&#39;T&#39;, 20)), .before = 1) %&gt;% # Add group sample_frac(1, replace = FALSE) %&gt;% # Random the group add_column(machine = c(sample(c(0, 10), 20, replace = TRUE, prob = c(0.8, 0.2)), sample(c(0, 10), 20, replace = TRUE, prob = c(0.2, 0.8))), .before = 1) %&gt;% # Add the machine used mutate_at(vars(starts_with(&#39;V&#39;)), # Shift the mean for one machine funs(case_when(machine == 10 ~ . + 1, machine == 0 ~ .))) Method from the researcher pca &lt;- prcomp(select(df, starts_with(&#39;V&#39;))) # Perform the PCA X &lt;- select(df, starts_with(&#39;V&#39;)) - matrix(pca$x[,1], nrow = 40) %*% pca$rotation[,1] X &lt;- X %&gt;% add_column(group = df$group, .before = 1) test &lt;- lapply(select(X, starts_with(&#39;V&#39;)), function(x) t.test(x ~ X$group)) The variance explained by the first components is 21.89%. Over the \\(1000\\) genes, we found that 244 have a significative difference between the mean in the Control group and the Treatment group. Our method pca_our &lt;- prcomp(select(df, -group)) test_A &lt;- lapply(select(filter(df, machine == 0), starts_with(&#39;V&#39;)), function(x) t.test(x ~ filter(df, machine == 0)$group)) test_B &lt;- lapply(select(filter(df, machine == 10), starts_with(&#39;V&#39;)), function(x) t.test(x ~ filter(df, machine == 10)$group)) The variance explained by the first components is 23.09% which is an improvement over the PCA without the feature Machine. Over the \\(1000\\) genes, we found that 154 have a significative difference between the mean in the Control group and the Treatment group for the machine A and 142 for the machine B. And they have 113 significative genes in common. This much less than the one wihtout the splitting in two groups, and probably more accurate. 10.2 Applied exercises 10.2.1 Exercise 7. df &lt;- as_tibble(USArrests) euclid_dist &lt;- dist(scale(df))^2 # Compute Euclidean distance cor_dist &lt;- as.dist(1 - cor(t(scale(df)))) # Compute the correlation distance Figure 8.4: Boxplot of Euclidean / Correlation 10.2.2 Exercise 8. df &lt;- as_tibble(USArrests) df_scaled &lt;- scale(df, center = TRUE, scale = TRUE) Question (a) pca &lt;- prcomp(df_scaled, center = FALSE, scale. = FALSE) PVE_a &lt;- pca$sdev**2 / sum(pca$sdev**2); PVE_a ## [1] 0.62006039 0.24744129 0.08914080 0.04335752 Question (b) PVE_b &lt;- apply((df_scaled %*% pca$rotation)**2, 2, sum) / sum(df_scaled**2); PVE_b ## PC1 PC2 PC3 PC4 ## 0.62006039 0.24744129 0.08914080 0.04335752 10.2.3 Exercise 9. df &lt;- USArrests Question (a) hclust_complete &lt;- hclust(dist(df), method = &#39;complete&#39;) Figure 10.8: Dendrogram using complete linkage. Question (b) cut &lt;- cutree(hclust_complete, 3) The state in the first group: Alabama, Alaska, Arizona, California, Delaware, Florida, Illinois, Louisiana, Maryland, Michigan, Mississippi, Nevada, New Mexico, New York, North Carolina, South Carolina. The state in the second group: Arkansas, Colorado, Georgia, Massachusetts, Missouri, New Jersey, Oklahoma, Oregon, Rhode Island, Tennessee, Texas, Virginia, Washington, Wyoming. And the state in the third group: Connecticut, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Montana, Nebraska, New Hampshire, North Dakota, Ohio, Pennsylvania, South Dakota, Utah, Vermont, West Virginia, Wisconsin. Question (c) hclust_complete &lt;- hclust(dist(scale(df)), method = &#39;complete&#39;) Figure 7.14: Dendrogram using complete linkage. Question (d) After scaling the variables, measure of dissimilarities at which the fusion occured decreased. Moreover, it is clear that the data should be split in two or four groups, but not in three. The variables should probably be scaled before the inter-observation dissimilarities are computed because the variance is quite different between the variables (different units). 10.2.4 Exercise 10. Here, we will generate simulated data, and then perform PCA and \\(K\\)-means clustering on the data. Question (a) df_A &lt;- matrix(rnorm(20 * 50, 0, 1.5), ncol = 50) df_B &lt;- matrix(rnorm(20 * 50, 1, 0.8), ncol = 50) df_C &lt;- matrix(rnorm(20 * 50, 2, 1.2), ncol = 50) df &lt;- rbind(df_A, df_B, df_C) Question (b) pca &lt;- prcomp(df) Figure 10.9: Observations into the first principal plan Question (c) clus &lt;- kmeans(df, 3) kable(table(df_plot$group, clus$cluster)) 1 2 3 A 20 0 0 B 0 0 20 C 0 20 0 The results are perfect. Question (d) clus &lt;- kmeans(df, 2) kable(table(df_plot$group, clus$cluster)) 1 2 A 0 20 B 20 0 C 20 0 Two of the classes almost fuse. Question (e) clus &lt;- kmeans(df, 4) kable(table(df_plot$group, clus$cluster)) 1 2 3 4 A 0 0 0 20 B 0 0 20 0 C 14 6 0 0 One of the true classe is splitted in two. Question (f) clus &lt;- kmeans(pca$x[,1:2], 3) kable(table(df_plot$group, clus$cluster)) 1 2 3 A 0 20 0 B 0 0 20 C 20 0 0 Once again, we found a perfect match! Question (g) clus &lt;- kmeans(scale(df), 3) kable(table(df_plot$group, clus$cluster)) 1 2 3 A 0 0 20 B 0 20 0 C 20 0 0 The results can be good of not depending on the initialization scheme. 10.2.5 Exercise 11. We will the data Ch10Ex11.csv from the book website. This dataset consists of \\(40\\) tissue samples with measurements on \\(1000\\) genes. The first \\(20\\) samples are from healthy patients, while the second \\(20\\) are from a diseased group. Question (a) df &lt;- read_csv(&#39;data/Ch10Ex11.csv&#39;, col_names = FALSE) Question (b) cor_dist &lt;- as.dist(1 - cor(df)) hclust_complete &lt;- hclust(cor_dist, method = &#39;complete&#39;) Figure 8.9: Dendrogram using complete linkage. cor_dist &lt;- as.dist(1 - cor(df)) hclust_single &lt;- hclust(cor_dist, method = &#39;single&#39;) Figure 10.10: Dendrogram using single linkage. cor_dist &lt;- as.dist(1 - cor(df)) hclust_average &lt;- hclust(cor_dist, method = &#39;average&#39;) Figure 10.11: Dendrogram using average linkage. Depending on the linkage, the genes are splitted between two or three groups. Question (c) In order to find the genes that differ the most across the two groups, we can run a \\(K\\)-means (with \\(K = 2\\)) in order to split the two groups and then perform a two samples \\(t\\)-test to find the genes that have significative difference between the groups. clus &lt;- kmeans(t(df), 2) df_clus &lt;- as_tibble(t(df)) %&gt;% add_column(group = clus$cluster, .before = 1) test &lt;- lapply(select(df_clus, starts_with(&#39;V&#39;)), function(x) t.test(x ~ df_clus$group)) Over the \\(1000\\) genes, we found that 158 have a significative difference between the mean in the healthy and the diseased group. These genes are the following: V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V135, V156, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V514, V515, V516, V517, V518, V519, V520, V521, V522, V523, V524, V525, V526, V527, V528, V529, V530, V531, V532, V533, V534, V535, V536, V537, V538, V539, V540, V541, V542, V543, V544, V545, V546, V547, V548, V549, V550, V551, V552, V553, V554, V555, V556, V557, V558, V559, V560, V561, V562, V563, V564, V565, V566, V567, V568, V569, V570, V571, V572, V573, V574, V575, V576, V577, V578, V579, V580, V581, V582, V583, V584, V585, V586, V587, V588, V589, V590, V591, V592, V593, V594, V595, V596, V597, V598, V599, V600. "],
["references.html", "References", " References James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in R. Springer. https://faculty.marshall.usc.edu/gareth-james/ISL/. "]
]
