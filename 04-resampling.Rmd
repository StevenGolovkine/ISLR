# Resampling Methods

```{r setup, include=FALSE}
fig.process <- function(x) {
  if (!grepl('[.]pdf$', x)) return(x) 
  x2 = sub('pdf$', 'png', x)
  magick::image_write(magick::image_read(x, density = 300), x2, format = 'png')
  x2
}

knitr::opts_chunk$set(echo = TRUE, fig.process = fig.process)
options(knitr.graphics.auto_pdf = TRUE)
```

```{r load_package, include=FALSE}
library(boot)
library(caret)
library(class)
library(GGally)
library(ggfortify)
library(gridExtra)
library(ISLR)
library(kableExtra)
library(knitr)
library(latex2exp)
library(MASS)
library(plotly)
library(reshape2)
library(tidyverse)

source('https://gist.githubusercontent.com/StevenGolovkine/55b9a2b6c849deadf86e051ed78ae149/raw/1452f33204a44c47b71bd22536594cce7e1e74e3/ggcustom.R')
source('https://gist.githubusercontent.com/StevenGolovkine/632632f470375390853529be54b9ebeb/raw/c07317788844d3d904aab7908e9cfe3d9df29931/summary_functions.R')
source('https://gist.githubusercontent.com/StevenGolovkine/c1124f4f55ef1edbba51ed619a087ce4/raw/72950af203fc017beb4698455b2edd47701ed002/print_df.R')
source('https://gist.githubusercontent.com/StevenGolovkine/c1124f4f55ef1edbba51ed619a087ce4/raw/72950af203fc017beb4698455b2edd47701ed002/print_summary_df.R')
source('https://gist.githubusercontent.com/StevenGolovkine/c1124f4f55ef1edbba51ed619a087ce4/raw/401f5c2edc8a04a294bfec38136adcb2f5f2e62d/print_summary_lm.R')
source('https://gist.githubusercontent.com/StevenGolovkine/c1124f4f55ef1edbba51ed619a087ce4/raw/c9a50e250666422da513db7da0fbb2eb007e9cc7/print_summary_glm.R')
```


## Conceptual Exercises

### Exercise 1.

Proof that $\alpha = \frac{\sigma_Y^2 - \sigma_{X,Y}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{X,Y}}$ minimize $\text{Var}(\alpha X + (1 - \alpha)Y)$.

Using properties of the variance, we found that:

$$\text{Var}(\alpha X + (1 - \alpha)Y) = \alpha^2\sigma_X^2 + (1 - \alpha)^2\sigma_Y^2 + 2\alpha(1 - \alpha)\sigma_{X,Y}.$$

As we seek to minimize this quantity with respect to $\alpha$, we have to set the derivative with respect to $\alpha$ equal to $0$. 

\begin{align}
\frac{d\text{Var}(\alpha X + (1 - \alpha)Y)}{d\alpha} = 0 &\Longleftrightarrow 2\alpha\sigma_X^2 + 2(1 - \alpha)\sigma_Y^2 + 2(1 - 2\alpha)\sigma_{X,Y} = 0 \\
&\Longleftrightarrow \left(\sigma_X^2 + \sigma_Y^2 - 2\sigma_{X,Y}\right)\alpha = \sigma_Y^2 - \sigma_{X,Y} \\
&\Longleftrightarrow \alpha = \frac{\sigma_Y^2 - \sigma_{X,Y}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{X,Y}}
\end{align}

### Exercise 2.

Suppose that we obtain a bootstrap sample from a set of $n$ observations.

* *Question (a)*

We draw one observation from a set of $n$ observations. So, the probability of getting a particular observation $j$ is $1/n$.

$$\mathbb{P}(X_j \neq S^{(1)}) = 1 - \mathbb{P}(X_j = S^{(1)}) = 1 - \frac{1}{n}, \quad\text{where}~ S^{(1)} ~\text{is the first element of the sample.}$$ 

* *Question (b)*

The bootstrap is performed **wih replacement**. So, the probability of getting a ârticular observation is $1/n$ at each drawing.

$$\mathbb{P}(X_j \neq S^{(2)}) = 1 - \mathbb{P}(X_j = S^{(2)}) = 1 - \frac{1}{n}, \quad\text{where}~ S^{(2)} ~\text{is the first element of the sample.}$$

* *Question (c)*

\begin{align}
\mathbb{P}(X_j \notin S) &= \mathbb{P}(X_j \neq S^{(1)}, \dots, X_j \neq S^{(n)}) \\
  &= \mathbb{P}(X_j \neq S^{(1)}) \times \dots \times \mathbb{P}(X_j \neq S^{(n)}) \\
  &= \left(1 -  \mathbb{P}(X_j = S^{(1)})\right) \times \dots \times \left(1 -  \mathbb{P}(X_j = S^{(n)})\right) \\
  &= \left(1 - \frac{1}{n}\right)^n
\end{align}

* *Question (d)*

\begin{align}
\mathbb{P}(X_j \in S) &= \mathbb{P}(X_j \notin S)\\
  &= 1 - \left(1 - \frac{1}{5}\right)^5 \\
  &= 0.672
\end{align}

* *Question (e)*

\begin{align}
\mathbb{P}(X_j \in S) &= \mathbb{P}(X_j \notin S)\\
  &= 1 - \left(1 - \frac{1}{100}\right)^{100} \\
  &= 0.634
\end{align}

* *Question (f)*

\begin{align}
\mathbb{P}(X_j \in S) &= \mathbb{P}(X_j \notin S)\\
  &= 1 - \left(1 - \frac{1}{10000}\right)^{10000} \\
  &= 0.632
\end{align}

* *Question (g)*

```{r}
prob <- function(n) return(1 - (1 - (1/n))**n)
df <- tibble(n = seq(1, 1000, by = 1), p = prob(n))
```

<center>
```{r ex2g, cache=TRUE, dev='tikz', echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, cache=TRUE, fig.cap='Probability that a particular observation belongs to the bootstrap sample.', fig.align='center'}
ggplot(df) +
  geom_point(aes(x = n, y = p)) +
  geom_hline(aes(yintercept = 1 - exp(-1)), col = 'red') +
  ylab('Probability') +
  ylim(0.5, 1) +
  theme_custom()
```
</center>

We observe that the probability that the $j$th observation is in the bootstrap samples seems to convergence until $0.63$.

* *Question (h)*
```{r}
store <- rep(NA, 1000)
for(i in 1:1000) store[i] <- sum(sample(1:100, replace = TRUE) == 4) > 0
```

This piece of code repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample. In mean, we found that the fourth observation is contain in `r 100*mean(store)`% of the samples. So, we retrieve the observation that gave at the previous question.

Mathematically, we can be prove like that:
\begin{align}
1 - \left(1 - \frac{1}{n}\right)^n &= 1 - \exp\left(n\ln\left(1 - \frac{1}{n}\right)\right) \\
&\underset{n \rightarrow \infty}{=} 1 - \exp\left(n\left(-\frac{1}{n} + o\left(\frac{1}{n}\right)\right)\right) \\ 
&\underset{n \rightarrow \infty}{\longrightarrow} 1 - \exp(-1)) \approx 0.63
\end{align}

### Exercise 3.

* *Question (a)*

From page 181 of *Introduction to Statistical learning with R*:

  * Step 1: Randomly divide the set of observation in $k$ groups;
  * Step 2: Fit the method on $k-1$ groups;
  * Step 3: Compute the MSE on the remaining group;
  * Step 4: Repeat the process $k$ times, considering another test group;
  * Step 5: Estimate the test error by averaging the test errors of each fold.

* *Question (b)*

On the validation set approach:

* Advantages: More accurate test error estimation (bias and variance).
* Disadvantages: Computationnaly less efficient.

On the LOOCV:

* Advantages: Computationnaly more efficient and more accurate test error estimation (variance).
* Disadvantages: Less accurate test error estimation (bias).

### Exercise 4.

We can estimate $\widehat{Y}$ by bootstrap samples. Assume $R$ bootstrap samples. So, we obtain $R$ values for $\widehat{Y}$: $\widehat{Y}_1, \dots, \widehat{Y}_R$.
And

$$ \bar{\widehat{Y}} = \frac{1}{R}\sum_{i = 1}^R \widehat{Y}_i \quad\text{and}\quad sd(\widehat{Y}) = \left(\frac{1}{R - 1}\sum_{i=1}^R\left(\widehat{Y}_i - \bar{\widehat{Y}}\right)^2\right)^{1/2}.$$


## Applied Exercises

### Exercise 5.

```{r ex5load, message=FALSE, warning=FALSE, paged.print=FALSE}
df <- as_tibble(Default)
```

* *Question (a)*

```{r ex5a, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
logit_model <- glm(default ~ income + balance, data = df, family = 'binomial')
logit_model %>% summary() %>% print_summary_glm()
```

* *Question (b)*

* *Question (b.i)* Split the sample.
```{r ex5bi, message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(42)
idx <- df$default %>% createDataPartition(p = 0.7, list = FALSE, times = 1)
train <- df[idx[,1],]
test <- df[-idx[,1],]
```

* *Question (b.ii)* Fit a logistic regression on the train set.

```{r ex5bii, message=FALSE, warning=FALSE, paged.print=FALSE}
logit_model <- glm(default ~ income + balance, data = train, family = 'binomial')
```

* *Question (b.iii)* Prediction on the test set.

```{r ex5biii, message=FALSE, warning=FALSE, paged.print=FALSE}
pred_prob <- predict(logit_model, test, type= 'response')
pred <- if_else(pred_prob > 0.5, 'Yes', 'No')
```

* *Question (b.iv)* Compute the validation set error.

```{r ex5biv, message=FALSE, warning=FALSE, paged.print=FALSE}
misclassif <- 1 - mean(test$default == pred)
```
So, there are `r round(100*misclassif, 2)`% of misclassified on the validation set, which correponds to the validation set error.

* *Question (c)*
```{r ex5c, message=FALSE, warning=FALSE, paged.print=FALSE, cache=TRUE}
misclassif <- vector('double', 3)
for(i in 1:3){
  set.seed(i)
  
  idx <- df$default %>% createDataPartition(p = 0.7, list = FALSE, times = 1)
  train <- df[idx[,1],]
  test <- df[-idx[,1],]
  
  logit_model <- glm(default ~ income + balance, data = train, family = 'binomial')
  
  pred_prob <- predict(logit_model, test, type= 'response')
  pred <- if_else(pred_prob > 0.5, 'Yes', 'No')
  misclassif[i] <- 1 - mean(test$default == pred)
}
```

The three misclassification error are respectively `r round(100*misclassif[1], 2)`%, `r round(100*misclassif[2], 2)`% and `r round(100*misclassif[3], 2)`% on each on the validation set. These results are quite close to each others.

* *Question (d)*
```{r ex5d, message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(42)
  
idx <- df$default %>% createDataPartition(p = 0.7, list = FALSE, times = 1)
train <- df[idx[,1],]
test <- df[-idx[,1],]
  
logit_model <- glm(default ~ income + balance + student, data = train, family = 'binomial')
  
pred_prob <- predict(logit_model, test, type= 'response')
pred <- if_else(pred_prob > 0.5, 'Yes', 'No')
misclassif <- 1 - mean(test$default == pred)
```

So, there are `r round(100*misclassif, 2)`% of misclassified on the validation set, which correponds to the validation set error. This result is almost the same as the one of the model without the `student` feature. So, the inclusion of this feature does not leads to a significant reduction in the test error rate. 

### Exercise 6.

```{r ex6load, message=FALSE, warning=FALSE, paged.print=FALSE}
df <- as_tibble(Default)
```

* *Question (a)*

```{r ex6a, message=FALSE, warning=FALSE, paged.print=FALSE}
logit_model <- glm(default ~ income + balance, data = df, family = 'binomial')
logit_model_summary <- logit_model %>% summary()
```

The estimation of the coefficient for the `ìncome` feature is $`r logit_model_summary[["coefficients"]]['income', 'Estimate']`$ with a standard error of $`r logit_model_summary[["coefficients"]]['income', 'Std. Error']`$. And the estimation of the coefficient for the `balance` feature is $`r logit_model_summary[["coefficients"]]['balance', 'Estimate']`$ with a standard error of $`r logit_model_summary[["coefficients"]]['balance', 'Std. Error']`$.

* *Question (b)*

```{r ex6b, message=FALSE, warning=FALSE, paged.print=FALSE}
boot.fn <- function(df, index){
  return(coef(glm(default ~ income + balance, data = df, subset = index, family = 'binomial')))
}
```

* *Question (c)*

```{r ex6c, message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
set.seed(42)
bootstrap_summary <- boot::boot(data = df, statistic = boot.fn, R = 1000)
```

Using the bootstrap method, the estimation of the coefficient for the `income` feature is $`r bootstrap_summary[['t0']]['income']`$ with a standard error of $`r sd(bootstrap_summary[['t']][,2])`$. And the estimation of the coefficient for the `balance` feature is $`r bootstrap_summary[['t0']]['balance']`$ with a standard error of $`r sd(bootstrap_summary[['t']][,3])`$.

* *Question (d)*

The estimated standard errors obtained with the `glm` function are slightly smaller than the ones obtained using the bootstrap method. This is due to the fact that the formula for the standard errors rely on some assumptions, and more particularly on an estimation of the noise variance. As we estimate $\sigma^2$ using the RSS, we use the linearaty assumption in our model. So, there is probably a non-linear relationship in the data, and so, the residuals from a linear fit will be inflated and so will $\widehat{\sigma}^2$. Secondly, the standard formulas assume that the $x_i$ are fixed, and all the variability comes from the variation in the errors $\epsilon_i$. However, the bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of the coefficients than the `summary` function.

### Exercise 7.

```{r ex7load, message=FALSE, warning=FALSE, paged.print=FALSE}
weekly <- as_tibble(Weekly)
```

* *Question (a)*

```{r ex7a, message=FALSE, warning=FALSE, paged.print=FALSE}
logit_model <- glm(Direction ~ Lag1 + Lag2, data = weekly, family = 'binomial')
```

* *Question (b)*

```{r ex7b, message=FALSE, warning=FALSE, paged.print=FALSE}
logit_model_1 <- glm(Direction ~ Lag1 + Lag2, data = weekly, subset = 2:nrow(weekly), family = 'binomial')
```

* *Question (c)*

```{r ex7c, message=FALSE, warning=FALSE, paged.print=FALSE}
pred <- predict(logit_model_1, newdata = weekly[1, c('Lag1', 'Lag2')], type = 'response')
```

So, the prediction of the direction of the first observation is **`r ifelse(pred > .5, 'Up', 'Down')`** (because $\mathbb{P}(Direction = UP | Lag1, Lag2) = `r pred`$). This observation is incorrectly classify.

* *Question (d)*

```{r ex7d, message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
errors <- vector('logical', nrow(weekly))
for(i in 1:nrow(weekly)){
  model <- glm(Direction ~ Lag1 + Lag2, data = weekly[-i,], family = 'binomial')
  pred_prob <- predict(model, newdata = weekly[i, c('Lag1', 'Lag2')], type = 'response')
  pred <- ifelse(pred_prob > .5, 'Up', 'Down')
  errors[i] <- (weekly[i, 'Direction'] != pred)
}
```

* *Question (e)*

```{r ex7e, message=FALSE, warning=FALSE, paged.print=FALSE}
loocv_error <- mean(errors)
```

The LOOCV estimate for the test error is `r loocv_error`. The value of the LOOCV estimate is quite small, a bit under $0.5$. It does indicate that the linear model is not very suitable for this dataset.

### Exercise 8.

* *Question (a)*

```{r ex8a, message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(42)
x <- rnorm(100)
y <- x - 2*x**2 + rnorm(100)
```

In this dataset, we have $n = 100$ and $p = 2$. The model used to generate the data is:
$$ Y = X - 2X^2 + \epsilon, \quad\text{where}\quad \epsilon \sim \mathcal{N}(0, 1)$$ 

* *Question (b)*

<center>
```{r ex8b, echo=FALSE, fig.height=5, fig.width=5, message=FALSE, warning=FALSE, dev='tikz', paged.print=FALSE, fig.align='center'}
df <- tibble(x, y)
ggplot(df) +
  geom_point(aes(x = x, y = y)) +
  xlab('$X$') + ylab('$Y$') +
  theme_custom()
```
</center>

We see that there is a clear relationship between $Y$ and $X$ which is not linear.

* *Question (c)*
```{r ex8c, cache=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(42)
cv_errors <- vector('numeric', 4)
for(i in 1:4){
  model <- glm(y ~ poly(x, i), data = df)
  cv_errors[i] <- cv.glm(df, model)$delta[1]
}
```

```{r ex8c2, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
c('Linear', 'Quadratic', 'Cubic', 'Quadric') %>% 
  tibble(cv_errors) %>% rename('Model' = '.', 'LOOCV Errors' = cv_errors) %>% t() %>% 
  kable(format = 'html', row.names = TRUE) %>%
  column_spec(1, bold = T) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), position = "center")
```

* *Question (d)*

```{r ex8d, cache=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(1)
cv_errors <- vector('numeric', 4)
for(i in 1:4){
  model <- glm(y ~ poly(x, i), data = df)
  cv_errors[i] <- cv.glm(df, model)$delta[1]
}
```

```{r ex8d2, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
c('Linear', 'Quadratic', 'Cubic', 'Quadric') %>% 
  tibble(cv_errors) %>% rename('Model' = '.', 'LOOCV Errors' = cv_errors) %>% t() %>% 
  kable(format = 'html', row.names = TRUE) %>%
  column_spec(1, bold = T) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), position = "center")
```

The LOOCV errors are exactly the same when we use another seed because we predict every observation using all the other ones which in fact involve no randomness.

* *Question (e)*

The model which the smallest LOOCV error is the quadratic model which makes sense because data are generated using this model.

* *Question (f)*

```{r ex8f, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
model <- lm(y ~ poly(x, 4), data = df)
model %>% summary() %>% print_summary_lm()
```

The coefficients that are statistically significant are the ones for $X$ and $X^2$ which correspond to the results given by the cross-validation.

### Exercise 9.

```{r ex9load}
boston <- as_tibble(Boston)
```

* *Question (a)*

```{r ex9a, message=FALSE, warning=FALSE, paged.print=FALSE}
mu_hat <- mean(boston$medv)
```

The estimate of the mean of `medv` is `r mu_hat`.

* *Question (b)*

```{r ex9b, message=FALSE, warning=FALSE, paged.print=FALSE}
se_mu_hat <- sd(boston$medv) / sqrt(nrow(boston))
```

So, a $95\%$ confidence interval fot the mean of `medv` is $`r mu_hat` \pm 2\times`r se_mu_hat`$. 

* *Question (c)*

```{r ex9c, message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
mean.fn <- function(data, index) return(mean(data[index]))
mu_boot <- boot(boston$medv, statistic = mean.fn, R = 1000)
```

The standard error of $\widehat{\mu}$ using the bootstrap is $`r sd(mu_boot[['t']])`$ which is very close to $`r se_mu_hat`$ (the one found in question (b)).

* *Question (d)*

Based on the bootstrap estimate, a $95\%$ confidence interval for the mean of `medv` is $[`r mu_boot[['t0']] - 2* sd(mu_boot[['t']])`, `r mu_boot[['t0']] + 2* sd(mu_boot[['t']])`]$. The one found with `t.test(boston$medv)` is $[`r t.test(boston[['medv']])[['conf.int']][1]`, `r t.test(boston[['medv']])[['conf.int']][2]`]$. Both of the intervals are very similar.

* *Question (e)*

```{r ex9e, message=FALSE, warning=FALSE, paged.print=FALSE}
med_hat <- median(boston$medv)
```

The estimate of the median of `medv` is `r med_hat`.

* *Question (f)*

```{r ex9f, message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
median.fn <- function(data, index) return(median(data[index]))
median_boot <- boot(boston$medv, statistic = median.fn, R = 1000)
```

The standard error of $\widehat{\mu}_{med}$ using the bootstrap is $`r sd(median_boot[['t']])`$ .

* *Question (g)*

```{r ex9g, message=FALSE, warning=FALSE, paged.print=FALSE}
quant_hat <- quantile(boston$medv, probs = 0.1)
```

The estimate of the tenth percentile of `medv` is `r quant_hat`.

* *Question (h)*

```{r ex9h, message=FALSE, warning=FALSE, cache=TRUE, paged.print=FALSE}
quant.fn <- function(data, index) return(quantile(data[index], probs = 0.1))
quant_boot <- boot(boston$medv, statistic = quant.fn, R = 1000)
```

The standard error of $\widehat{\mu}_{0.1}$ using the bootstrap is $`r sd(quant_boot[['t']])`$ .
